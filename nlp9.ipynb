{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "jp7jmfSbci61",
        "outputId": "7e5ad17c-d8ea-431f-c27a-fd73dc4f16e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 144154 sentences.\n",
            "Unique words in corpus: 91309\n",
            "\n",
            "Training Byte Pair Encoding...\n",
            "Merge step 1000: ('मै', 'च</w>')\n",
            "Merge step 2000: ('सा', 'ं')\n",
            "Merge step 3000: ('मुस्लि', 'म</w>')\n",
            "Merge step 4000: ('म', 'क')\n",
            "Merge step 5000: ('त', 'ें')\n",
            "Merge step 6000: ('एम', 'सी')\n",
            "Merge step 7000: ('आकर्', 'षित</w>')\n",
            "Merge step 8000: ('हा', 'ली</w>')\n",
            "Merge step 9000: ('स्ट्र', 'ा')\n",
            "Merge step 10000: ('शे', 'र')\n",
            "Merge step 11000: ('ना', 'नक</w>')\n",
            "Merge step 12000: ('पीपु', 'ल्स</w>')\n",
            "Merge step 13000: ('ढूंढ', '</w>')\n",
            "Merge step 14000: ('ग', 'पुर</w>')\n",
            "Merge step 15000: ('बी', 'जापुर</w>')\n",
            "Merge step 16000: ('किसा', 'नो</w>')\n",
            "Merge step 17000: ('व', 'ता</w>')\n",
            "Merge step 18000: ('क', 'म्प</w>')\n",
            "Merge step 19000: ('बि', 'खरा</w>')\n",
            "Merge step 20000: ('करो', 'ड़')\n",
            "Merge step 21000: ('एफ', 'एसएल</w>')\n",
            "Merge step 22000: ('सेलिब्रि', 'टी</w>')\n",
            "Merge step 23000: ('औचि', 'त्य</w>')\n",
            "Merge step 24000: ('सिसौ', 'दिया</w>')\n",
            "Merge step 25000: ('टू', 'ल')\n",
            "Merge step 26000: ('कल्याण', 'पुर</w>')\n",
            "Merge step 27000: ('को', 'त्सव</w>')\n",
            "Merge step 28000: ('7', '80</w>')\n",
            "Merge step 29000: ('इ', 'ग')\n",
            "Merge step 30000: ('लाउ', 'ड</w>')\n",
            "Merge step 31000: ('कैंस', 'ल</w>')\n",
            "Merge step 32000: ('अल्मो', 'ड़ा</w>')\n",
            "BPE training complete. Final vocab size ≈ 31436\n",
            "\n",
            "Training WordPiece...\n",
            "WordPiece vocab size: 1000\n",
            "WordPiece vocab size: 2000\n",
            "WordPiece vocab size: 3000\n",
            "WordPiece vocab size: 4000\n",
            "WordPiece vocab size: 5000\n",
            "WordPiece vocab size: 6000\n",
            "WordPiece vocab size: 7000\n"
          ]
        }
      ],
      "source": [
        "# ============================================\n",
        "# ASSIGNMENT — BYTE PAIR ENCODING & WORDPIECE\n",
        "# ============================================\n",
        "# Author: (Your Name)\n",
        "# Input file: sentence_tokens.txt\n",
        "# Requirements: None (pure Python implementation)\n",
        "# Target: 32,000 merge steps / vocab size\n",
        "\n",
        "import re\n",
        "from collections import defaultdict, Counter\n",
        "from math import log\n",
        "\n",
        "# --------------------------------------------------\n",
        "# STEP 1. Load Corpus\n",
        "# --------------------------------------------------\n",
        "with open(\"sentence_tokens.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    corpus = [line.strip() for line in f if line.strip()]\n",
        "\n",
        "print(f\"Loaded {len(corpus)} sentences.\")\n",
        "\n",
        "# --------------------------------------------------\n",
        "# STEP 2. Prepare data for subword learning\n",
        "# --------------------------------------------------\n",
        "def corpus_to_char_tokens(corpus):\n",
        "    \"\"\"Split each word into characters separated by spaces, append '</w>' to mark word end.\"\"\"\n",
        "    tokenized = []\n",
        "    for sent in corpus:\n",
        "        words = sent.strip().split()\n",
        "        tokenized.append([\" \".join(list(word)) + \" </w>\" for word in words])\n",
        "    return tokenized\n",
        "\n",
        "char_level_corpus = corpus_to_char_tokens(corpus)\n",
        "\n",
        "# Flatten into word list\n",
        "word_freq = Counter()\n",
        "for sent in char_level_corpus:\n",
        "    for word in sent:\n",
        "        word_freq[word] += 1\n",
        "\n",
        "print(f\"Unique words in corpus: {len(word_freq)}\")\n",
        "\n",
        "# --------------------------------------------------\n",
        "# STEP 3. Helper functions for BPE\n",
        "# --------------------------------------------------\n",
        "def get_pair_freq(word_freq):\n",
        "    \"\"\"Count frequency of symbol pairs in the corpus.\"\"\"\n",
        "    pairs = defaultdict(int)\n",
        "    for word, freq in word_freq.items():\n",
        "        symbols = word.split()\n",
        "        for i in range(len(symbols) - 1):\n",
        "            pairs[(symbols[i], symbols[i + 1])] += freq\n",
        "    return pairs\n",
        "\n",
        "\n",
        "def merge_vocab(pair, v_in):\n",
        "    \"\"\"Merge the most frequent pair in vocabulary.\"\"\"\n",
        "    v_out = {}\n",
        "    bigram = re.escape(\" \".join(pair))\n",
        "    pattern = re.compile(r\"(?<!\\S)\" + bigram + r\"(?!\\S)\")\n",
        "    for word in v_in:\n",
        "        w_out = pattern.sub(\"\".join(pair), word)\n",
        "        v_out[w_out] = v_in[word]\n",
        "    return v_out\n",
        "\n",
        "\n",
        "# --------------------------------------------------\n",
        "# STEP 4. BYTE PAIR ENCODING (BPE)\n",
        "# --------------------------------------------------\n",
        "def byte_pair_encoding(word_freq, num_merges=32000):\n",
        "    merges = []\n",
        "    vocab = word_freq.copy()\n",
        "\n",
        "    for i in range(num_merges):\n",
        "        pairs = get_pair_freq(vocab)\n",
        "        if not pairs:\n",
        "            break\n",
        "        best = max(pairs, key=pairs.get)\n",
        "        vocab = merge_vocab(best, vocab)\n",
        "        merges.append(best)\n",
        "\n",
        "        if (i + 1) % 1000 == 0:\n",
        "            print(f\"Merge step {i+1}: {best}\")\n",
        "\n",
        "    # Build final vocabulary\n",
        "    final_vocab = set()\n",
        "    for word in vocab:\n",
        "        for token in word.split():\n",
        "            final_vocab.add(token)\n",
        "    return merges, final_vocab\n",
        "\n",
        "\n",
        "print(\"\\nTraining Byte Pair Encoding...\")\n",
        "bpe_merges, bpe_vocab = byte_pair_encoding(word_freq, num_merges=32000)\n",
        "print(f\"BPE training complete. Final vocab size ≈ {len(bpe_vocab)}\")\n",
        "\n",
        "# --------------------------------------------------\n",
        "# STEP 5. WORDPIECE IMPLEMENTATION\n",
        "# --------------------------------------------------\n",
        "def train_wordpiece(corpus, vocab_size=32000):\n",
        "    \"\"\"\n",
        "    Simplified WordPiece training (similar to BERT).\n",
        "    Starts from character-level vocab and merges pairs based on likelihood.\n",
        "    \"\"\"\n",
        "    # Initialize vocab with characters\n",
        "    word_freq = Counter()\n",
        "    for sent in corpus:\n",
        "        for word in sent.split():\n",
        "            word_freq[word] += 1\n",
        "\n",
        "    # Character-level initialization\n",
        "    vocab = set()\n",
        "    for word in word_freq:\n",
        "        for ch in word:\n",
        "            vocab.add(ch)\n",
        "    vocab = {ch: 1 for ch in vocab}\n",
        "    vocab[\"##UNK\"] = 1\n",
        "\n",
        "    # Create words with start/end tokens\n",
        "    subwords = {\" \".join(list(w)) + \" </w>\": f for w, f in word_freq.items()}\n",
        "\n",
        "    merges = []\n",
        "    while len(vocab) < vocab_size:\n",
        "        pair_freq = get_pair_freq(subwords)\n",
        "        if not pair_freq:\n",
        "            break\n",
        "        best = max(pair_freq, key=pair_freq.get)\n",
        "        merges.append(best)\n",
        "        subwords = merge_vocab(best, subwords)\n",
        "        new_token = \"\".join(best)\n",
        "        vocab[new_token] = 1\n",
        "        if len(vocab) % 1000 == 0:\n",
        "            print(f\"WordPiece vocab size: {len(vocab)}\")\n",
        "\n",
        "    return merges, vocab\n",
        "\n",
        "\n",
        "print(\"\\nTraining WordPiece...\")\n",
        "wp_merges, wp_vocab = train_wordpiece(corpus, vocab_size=32000)\n",
        "print(f\"WordPiece training complete. Final vocab size ≈ {len(wp_vocab)}\")\n",
        "\n",
        "# --------------------------------------------------\n",
        "# STEP 6. Encode a sentence example\n",
        "# --------------------------------------------------\n",
        "def encode_with_merges(sentence, merges, end_token=\"</w>\"):\n",
        "    \"\"\"Tokenize a new sentence using learned BPE merges.\"\"\"\n",
        "    tokens = []\n",
        "    for word in sentence.split():\n",
        "        chars = list(word) + [end_token]\n",
        "        while True:\n",
        "            pairs = [(chars[i], chars[i + 1]) for i in range(len(chars) - 1)]\n",
        "            mergeable = [p for p in pairs if p in merges]\n",
        "            if not mergeable:\n",
        "                break\n",
        "            best = mergeable[0]\n",
        "            new_chars = []\n",
        "            i = 0\n",
        "            while i < len(chars):\n",
        "                if i < len(chars) - 1 and (chars[i], chars[i + 1]) == best:\n",
        "                    new_chars.append(\"\".join(best))\n",
        "                    i += 2\n",
        "                else:\n",
        "                    new_chars.append(chars[i])\n",
        "                    i += 1\n",
        "            chars = new_chars\n",
        "        tokens.extend(chars)\n",
        "    return tokens\n",
        "\n",
        "sample = \"Natural language processing\"\n",
        "print(\"\\nEncoding sample with BPE:\\n\", encode_with_merges(sample.lower(), bpe_merges[:500]))\n",
        "\n",
        "# --------------------------------------------------\n",
        "# STEP 7. Save outputs (optional)\n",
        "# --------------------------------------------------\n",
        "with open(\"bpe_vocab.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    for tok in sorted(bpe_vocab):\n",
        "        f.write(tok + \"\\n\")\n",
        "\n",
        "with open(\"wordpiece_vocab.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    for tok in sorted(wp_vocab):\n",
        "        f.write(tok + \"\\n\")\n",
        "\n",
        "print(\"\\nSaved bpe_vocab.txt and wordpiece_vocab.txt\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}