{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J-hNrCD9vM6A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "788c97a0-8306-43b8-9d27-7041aac07c98"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Created train/val/test files successfully!\n",
            "   Total sentences: 20077\n",
            "   Train: 16061 | Val: 2008 | Test: 2008\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "\n",
        "# Read all tokenized sentences\n",
        "with open(\"sentence_tokens.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    sentences = [line.strip() for line in f if line.strip()]\n",
        "\n",
        "# Shuffle\n",
        "random.seed(42)\n",
        "random.shuffle(sentences)\n",
        "\n",
        "# Split 80 / 10 / 10\n",
        "n = len(sentences)\n",
        "train_end = int(0.8 * n)\n",
        "val_end = int(0.9 * n)\n",
        "\n",
        "train_sentences = sentences[:train_end]\n",
        "val_sentences = sentences[train_end:val_end]\n",
        "test_sentences = sentences[val_end:]\n",
        "\n",
        "# Save splits\n",
        "with open(\"train.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(\"\\n\".join(train_sentences))\n",
        "with open(\"val.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(\"\\n\".join(val_sentences))\n",
        "with open(\"test.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(\"\\n\".join(test_sentences))\n",
        "\n",
        "print(f\"âœ… Created train/val/test files successfully!\")\n",
        "print(f\"   Total sentences: {n}\")\n",
        "print(f\"   Train: {len(train_sentences)} | Val: {len(val_sentences)} | Test: {len(test_sentences)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from collections import Counter\n",
        "from math import log\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# =====================================================\n",
        "# STEP 1: Read and Clean Sentences\n",
        "# =====================================================\n",
        "with open(\"sentence_tokens.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    sentences = [line.strip() for line in f if line.strip()]\n",
        "\n",
        "# ðŸ§¹ Remove duplicates\n",
        "sentences = list(dict.fromkeys(sentences))\n",
        "\n",
        "# ðŸ§¹ Remove very short or punctuation-only sentences\n",
        "sentences = [s for s in sentences if len(s.split()) > 2]\n",
        "\n",
        "# =====================================================\n",
        "# STEP 2: Shuffle and Split (80/10/10)\n",
        "# =====================================================\n",
        "random.seed(42)\n",
        "random.shuffle(sentences)\n",
        "\n",
        "n = len(sentences)\n",
        "train_end = int(0.8 * n)\n",
        "val_end = int(0.9 * n)\n",
        "\n",
        "train_sentences = sentences[:train_end]\n",
        "val_sentences = sentences[train_end:val_end]\n",
        "test_sentences = sentences[val_end:]\n",
        "\n",
        "# Save the splits\n",
        "for name, data in zip([\"train\", \"val\", \"test\"], [train_sentences, val_sentences, test_sentences]):\n",
        "    with open(f\"{name}.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(\"\\n\".join(data))\n",
        "\n",
        "print(f\"âœ… Data prepared successfully!\")\n",
        "print(f\"Total: {n} | Train: {len(train_sentences)} | Val: {len(val_sentences)} | Test: {len(test_sentences)}\")\n",
        "\n",
        "# =====================================================\n",
        "# STEP 3: Build Unigram and Bigram Models from Train\n",
        "# =====================================================\n",
        "def get_ngrams(sentences, n=1):\n",
        "    ngrams = []\n",
        "    for s in sentences:\n",
        "        tokens = s.split()\n",
        "        ngrams.extend([\" \".join(tokens[i:i+n]) for i in range(len(tokens)-n+1)])\n",
        "    return Counter(ngrams)\n",
        "\n",
        "train_unigrams = get_ngrams(train_sentences, 1)\n",
        "train_bigrams = get_ngrams(train_sentences, 2)\n",
        "total_unigrams = sum(train_unigrams.values())\n",
        "total_bigrams = sum(train_bigrams.values())\n",
        "\n",
        "print(f\"Train unigrams: {len(train_unigrams)} | bigrams: {len(train_bigrams)}\")\n",
        "\n",
        "# =====================================================\n",
        "# STEP 4: Compute PMI for Val/Test Bigrams\n",
        "# =====================================================\n",
        "def compute_pmi(sentences, min_freq=3):\n",
        "    pmi_scores = {}\n",
        "    for s in sentences:\n",
        "        tokens = s.split()\n",
        "        for i in range(len(tokens) - 1):\n",
        "            w1, w2 = tokens[i], tokens[i + 1]\n",
        "            bigram = f\"{w1} {w2}\"\n",
        "            # Skip unseen or rare bigrams\n",
        "            if train_bigrams[bigram] < min_freq:\n",
        "                continue\n",
        "            p_w1 = train_unigrams[w1] / total_unigrams\n",
        "            p_w2 = train_unigrams[w2] / total_unigrams\n",
        "            p_bigram = train_bigrams[bigram] / total_bigrams\n",
        "            pmi_scores[bigram] = log(p_bigram / (p_w1 * p_w2))\n",
        "    return sorted(pmi_scores.items(), key=lambda x: -x[1])[:10]\n",
        "\n",
        "top_pmi_val = compute_pmi(val_sentences)\n",
        "top_pmi_test = compute_pmi(test_sentences)\n",
        "\n",
        "print(\"\\nðŸ”¹ Top 10 PMI bigrams (Validation):\")\n",
        "for bigram, score in top_pmi_val:\n",
        "    print(f\"{bigram:<30} â†’ PMI = {score:.3f}\")\n",
        "\n",
        "print(\"\\nðŸ”¹ Top 10 PMI bigrams (Test):\")\n",
        "for bigram, score in top_pmi_test:\n",
        "    print(f\"{bigram:<30} â†’ PMI = {score:.3f}\")\n",
        "\n",
        "# =====================================================\n",
        "# STEP 5: TF-IDF Vectorization\n",
        "# =====================================================\n",
        "vectorizer = TfidfVectorizer()\n",
        "tfidf_train = vectorizer.fit_transform(train_sentences)\n",
        "tfidf_val = vectorizer.transform(val_sentences)\n",
        "tfidf_test = vectorizer.transform(test_sentences)\n",
        "\n",
        "print(\"\\nâœ… TF-IDF vectorization done.\")\n",
        "print(f\"Train: {tfidf_train.shape}, Val: {tfidf_val.shape}, Test: {tfidf_test.shape}\")\n",
        "\n",
        "# =====================================================\n",
        "# STEP 6: Nearest Neighbor Search (Within Set)\n",
        "# =====================================================\n",
        "def nearest_neighbors(tfidf_matrix, sentences, top_n=10):\n",
        "    sims = cosine_similarity(tfidf_matrix)\n",
        "    np.fill_diagonal(sims, 0)  # ignore self\n",
        "    pairs = []\n",
        "    for i in range(len(sentences)):\n",
        "        j = np.argmax(sims[i])\n",
        "        pairs.append((sentences[i], sentences[j], sims[i][j]))\n",
        "    pairs.sort(key=lambda x: -x[2])\n",
        "    return pairs[:top_n]\n",
        "\n",
        "print(\"\\nðŸ”¹ Top 10 Similar Sentence Pairs (Validation Set):\")\n",
        "for a, b, sim in nearest_neighbors(tfidf_val, val_sentences):\n",
        "    print(f\"{sim:.3f} | {a[:40]} â†” {b[:40]}\")\n",
        "\n",
        "# =====================================================\n",
        "# STEP 7: Nearest Neighbor (Val/Test â†’ Train)\n",
        "# =====================================================\n",
        "def nearest_in_training(tfidf_train, tfidf_other, other_sentences, train_sentences, top_n=10, batch_size=1000):\n",
        "    neighbors = []\n",
        "    for start in range(0, tfidf_other.shape[0], batch_size):\n",
        "        end = start + batch_size\n",
        "        sims = cosine_similarity(tfidf_other[start:end], tfidf_train)\n",
        "        nearest_idx = np.argmax(sims, axis=1)\n",
        "        max_sims = np.max(sims, axis=1)\n",
        "        for i, j, s in zip(range(start, end), nearest_idx, max_sims):\n",
        "            neighbors.append((other_sentences[i], train_sentences[j], s))\n",
        "    neighbors.sort(key=lambda x: -x[2])\n",
        "    return neighbors[:top_n]\n",
        "\n",
        "print(\"\\nðŸ”¹ Top 10 Nearest Validation â†’ Train Sentences:\")\n",
        "for a, b, sim in nearest_in_training(tfidf_train, tfidf_val, val_sentences, train_sentences):\n",
        "    print(f\"{sim:.3f} | {a[:40]} â†” {b[:40]}\")\n",
        "\n",
        "print(\"\\nâœ… All tasks completed successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lXBGGD5LXZKT",
        "outputId": "9c8b3644-7787-4826-814d-b84f438c4865"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Data prepared successfully!\n",
            "Total: 140439 | Train: 112351 | Val: 14044 | Test: 14044\n",
            "Train unigrams: 81181 | bigrams: 743177\n",
            "\n",
            "ðŸ”¹ Top 10 PMI bigrams (Validation):\n",
            "à¤¯à¥‚à¤•à¥à¤°à¥‡à¤¨à¤¿à¤¯à¤¨ à¤°à¤¿à¤µà¥à¤¨à¤¿à¤¯à¤¾            â†’ PMI = 13.652\n",
            "à¤à¤²à¥à¤µà¥€à¤°à¥‹ à¤ªà¥€à¤Ÿà¤°à¤¸à¤¨                 â†’ PMI = 13.141\n",
            "à¤®à¥‡à¤¹à¥à¤² à¤šà¥‹à¤•à¤¸à¥€                    â†’ PMI = 13.141\n",
            "à¤®à¥ˆà¤•à¥à¤¸à¤¿à¤•à¤¨ à¤ªà¥‡à¤¸à¥‹                  â†’ PMI = 13.141\n",
            "à¤°à¥‹à¤°à¥€ à¤¬à¤°à¥à¤¨à¥à¤¸                    â†’ PMI = 13.141\n",
            "à¤¬à¥à¤°à¥‰à¤¨ à¤¸à¥à¤Ÿà¥à¤°à¥‹à¤®à¥ˆà¤¨                â†’ PMI = 13.141\n",
            "à¤¹à¤°à¤•à¥€ à¤ªà¥ˆà¤¡à¤¼à¥€                     â†’ PMI = 12.959\n",
            "à¤¸à¥à¤ªà¥à¤°à¤¿à¤¯à¤¾ à¤¸à¥à¤²à¥‡                  â†’ PMI = 12.854\n",
            "à¤¡à¥€à¤µà¤¾à¤ˆ à¤šà¤‚à¤¦à¥à¤°à¤šà¥‚à¤¡à¤¼                â†’ PMI = 12.854\n",
            "à¤ªà¤²à¥à¤²à¤¾ à¤à¤¾à¥œ                      â†’ PMI = 12.805\n",
            "\n",
            "ðŸ”¹ Top 10 PMI bigrams (Test):\n",
            "à¤—à¥à¤°à¥‡à¤Ÿà¤¾ à¤¥à¤¨à¤¬à¤°à¥à¤—                  â†’ PMI = 13.364\n",
            "à¤®à¥ˆà¤•à¥à¤¸à¤¿à¤•à¤¨ à¤ªà¥‡à¤¸à¥‹                  â†’ PMI = 13.141\n",
            "à¤¨à¤®à¤¾à¤®à¤¿ à¤—à¤‚à¤—à¥‡                     â†’ PMI = 13.141\n",
            "à¤œà¥‡à¤« à¤¬à¥‡à¤œà¥‹à¤¸                      â†’ PMI = 13.141\n",
            "à¤¬à¥à¤°à¥‰à¤¨ à¤¸à¥à¤Ÿà¥à¤°à¥‹à¤®à¥ˆà¤¨                â†’ PMI = 13.141\n",
            "àµ† àµ                            â†’ PMI = 12.959\n",
            "à¤à¤²à¥à¤«à¥à¤°à¥‡à¤¡ à¤¬à¤°à¥à¤¨à¤¾à¤°à¥à¤¡              â†’ PMI = 12.854\n",
            "à¤¸à¥à¤ªà¥à¤°à¤¿à¤¯à¤¾ à¤¸à¥à¤²à¥‡                  â†’ PMI = 12.854\n",
            "à¤®à¤²à¤¯ à¤ªà¥à¤°à¤¾à¤¯à¤¦à¥à¤µà¥€à¤ª                 â†’ PMI = 12.805\n",
            "à« àª¾                            â†’ PMI = 12.736\n",
            "\n",
            "âœ… TF-IDF vectorization done.\n",
            "Train: (112351, 11090), Val: (14044, 11090), Test: (14044, 11090)\n",
            "\n",
            "ðŸ”¹ Top 10 Similar Sentence Pairs (Validation Set):\n",
            "1.000 | à¤…à¤¬ à¤¦à¥‹ à¤®à¤¾à¤¹ à¤¹à¥‹ à¤—à¤¯à¥‡ à¤¥à¥‡ â†” à¤…à¤¬ à¤•à¤¾à¤® à¤ªà¥‚à¤°à¤¾ à¤¹à¥‹ à¤—à¤¯à¤¾ à¤¹à¥ˆ\n",
            "1.000 | à¤‡à¤¸à¤•à¥€ à¤¶à¥à¤°à¥à¤†à¤¤ à¤œà¥à¤•à¤¾à¤® à¤µ à¤–à¤¾à¤‚à¤¸à¥€ à¤®à¤¾à¤¤à¥à¤° à¤¸à¥‡ à¤¹à¥‹à¤¤à¥€  â†” à¤‡à¤¸à¤•à¥‡ à¤¬à¤¾à¤¦ à¤†à¤¤à¤‚à¤•à¥€ à¤¨à¥‡ à¤ªà¤¾à¤¨à¥€ à¤®à¤¾à¤‚à¤—à¤¾\n",
            "1.000 | à¤‡à¤¸à¤•à¥‡ à¤¬à¤¾à¤¦ à¤†à¤¤à¤‚à¤•à¥€ à¤¨à¥‡ à¤ªà¤¾à¤¨à¥€ à¤®à¤¾à¤‚à¤—à¤¾ â†” à¤‡à¤¸à¤•à¥€ à¤¶à¥à¤°à¥à¤†à¤¤ à¤œà¥à¤•à¤¾à¤® à¤µ à¤–à¤¾à¤‚à¤¸à¥€ à¤®à¤¾à¤¤à¥à¤° à¤¸à¥‡ à¤¹à¥‹à¤¤à¥€ \n",
            "1.000 | à¤²à¥‡à¤•à¤¿à¤¨ à¤¦à¥‹à¤¨à¥‹à¤‚ à¤«à¥‡à¤œ à¤®à¥‡à¤‚ à¤¹à¥€ à¤ªà¥à¤°à¥‹à¤œà¥‡à¤•à¥à¤Ÿ à¤ªà¥‚à¤°à¤¾ à¤¨à¤¹ â†” à¤•à¥à¤› à¤Ÿà¥à¤°à¥‡à¤¨à¥‹à¤‚ à¤®à¥‡à¤‚ à¤šà¥‡à¤•à¤¿à¤‚à¤— à¤­à¥€ à¤¨à¤¹à¥€à¤‚ à¤¹à¥‹ à¤¸à¤•à¥€\n",
            "1.000 | à¤‡à¤¸à¤•à¤¾ à¤•à¤¾à¤® à¤‡à¤‚à¤¦à¤¿à¤°à¤¾ à¤šà¥Œà¤• à¤¸à¥‡ à¤¶à¥à¤°à¥‚ à¤•à¤° à¤¦à¤¿à¤¯à¤¾ à¤¹à¥ˆ â†” à¤µà¤¿à¤µà¤¿ à¤‡à¤¸à¤•à¥‡ à¤²à¤¿à¤ à¤•à¥‰à¤²à¥‡à¤œ à¤ªà¥à¤°à¤¾à¤šà¤¾à¤°à¥à¤¯à¥‹à¤‚ à¤•à¥‹ à¤¨à¤¿à¤°à¥à¤¦\n",
            "1.000 | à¤®à¥‡à¤¡à¤¿à¤•à¤² à¤•à¤¾à¤²à¥‡à¤œ à¤•à¥€ à¤®à¥‡à¤¡à¤¿à¤¸à¤¿à¤¨ à¤•à¥‹ à¤¶à¤¿à¤«à¥à¤Ÿ à¤•à¤¿à¤¯à¤¾ à¤œà¤¾ â†” à¤•à¤² à¤¬à¤‚à¤¦ à¤°à¤¹à¥‡à¤‚à¤—à¥‡ à¤ªà¥‡à¤Ÿà¥à¤°à¥‹à¤² à¤ªà¤‚à¤ª\n",
            "1.000 | à¤µà¤¹ à¤°à¤¾à¤¤ à¤•à¥‹ à¤¦à¥à¤•à¤¾à¤¨ à¤¸à¥‡ à¤µà¤¾à¤ªà¤¸ à¤²à¥Œà¤Ÿ à¤°à¤¹à¤¾ à¤¥à¤¾ â†” à¤µà¤¹à¤¾à¤‚ à¤¸à¥‡ à¤°à¤¾à¤¤ à¤®à¥‡à¤‚ à¤µà¤¾à¤ªà¤¸ à¤²à¥Œà¤Ÿ à¤°à¤¹à¥‡ à¤¥à¥‡\n",
            "1.000 | à¤•à¥‹à¤ˆ à¤•à¤¾à¤°à¥à¤°à¤µà¤¾à¤ˆ à¤¨à¤¹à¥€à¤‚ à¤¹à¥‹ à¤°à¤¹à¥€ à¤¹à¥ˆ â†” à¤²à¥‡à¤•à¤¿à¤¨ à¤•à¤¾à¤°à¥à¤°à¤µà¤¾à¤ˆ à¤¨à¤¹à¥€à¤‚ à¤¹à¥‹ à¤°à¤¹à¥€ à¤¹à¥ˆ\n",
            "1.000 | à¤²à¥‡à¤•à¤¿à¤¨ à¤•à¤¾à¤°à¥à¤°à¤µà¤¾à¤ˆ à¤¨à¤¹à¥€à¤‚ à¤¹à¥‹ à¤°à¤¹à¥€ à¤¹à¥ˆ â†” à¤•à¥‹à¤ˆ à¤•à¤¾à¤°à¥à¤°à¤µà¤¾à¤ˆ à¤¨à¤¹à¥€à¤‚ à¤¹à¥‹ à¤°à¤¹à¥€ à¤¹à¥ˆ\n",
            "1.000 | à¤µà¤¹à¤¾à¤‚ à¤¸à¥‡ à¤°à¤¾à¤¤ à¤®à¥‡à¤‚ à¤µà¤¾à¤ªà¤¸ à¤²à¥Œà¤Ÿ à¤°à¤¹à¥‡ à¤¥à¥‡ â†” à¤µà¤¹ à¤°à¤¾à¤¤ à¤•à¥‹ à¤¦à¥à¤•à¤¾à¤¨ à¤¸à¥‡ à¤µà¤¾à¤ªà¤¸ à¤²à¥Œà¤Ÿ à¤°à¤¹à¤¾ à¤¥à¤¾\n",
            "\n",
            "ðŸ”¹ Top 10 Nearest Validation â†’ Train Sentences:\n",
            "1.000 | à¤®à¥ˆà¤š à¤•à¤¾ à¤ªà¤¹à¤²à¤¾ à¤—à¥‹à¤² à¤œà¤°à¥à¤®à¤¨à¥€ à¤•à¥‡ à¤¨à¤¾à¤® à¤°à¤¹à¤¾ â†” à¤ªà¤¹à¤²à¤¾ à¤¹à¤¾à¤« 1 - 0 à¤¸à¥‡ à¤œà¤°à¥à¤®à¤¨à¥€ à¤•à¥‡ à¤¨à¤¾à¤® à¤°à¤¹à¤¾\n",
            "1.000 | à¤…à¤–à¤¿à¤²à¥‡à¤¶ à¤¯à¤¾à¤¦à¤µ à¤¨à¥‡ à¤•à¤¹à¤¾ à¤•à¤¿ à¤¸à¤¿à¤‚à¤šà¤¾à¤ˆ à¤®à¥à¤«à¥à¤¤ à¤•à¤°à¥‡à¤‚à¤— â†” à¤…à¤–à¤¿à¤²à¥‡à¤¶ à¤¯à¤¾à¤¦à¤µ à¤¨à¥‡ à¤Ÿà¥à¤µà¥€à¤Ÿ à¤•à¤° à¤•à¤¹à¤¾ à¤•à¤¿\n",
            "1.000 | à¤ªà¤¹à¤²à¥‡ à¤®à¤‚à¤œà¤¿à¤² à¤ªà¤° à¤ªà¥ˆà¤¥à¥‹à¤²à¥‰à¤œà¥€ à¤•à¥€ à¤œà¤¾à¤‚à¤šà¥‡à¤‚ à¤¹à¥‹à¤¤à¥€ à¤¹à¥ˆ â†” à¤ªà¤¹à¤²à¥‡ à¤¦à¤¿à¤¨ à¤…à¤‚à¤—à¥à¤°à¥‡à¤œà¥€ à¤•à¤¾ à¤ªà¥‡à¤ªà¤° à¤¹à¥à¤†\n",
            "1.000 | à¤œà¤¿à¤¸à¤¸à¥‡ à¤µà¤¹ à¤˜à¤¾à¤¯à¤² à¤¹à¥‹ à¤—à¤¯à¤¾ â†” à¤œà¤¿à¤¸à¤¸à¥‡ à¤µà¤¹ à¤—à¤‚à¤­à¥€à¤° à¤°à¥‚à¤ª à¤¸à¥‡ à¤˜à¤¾à¤¯à¤² à¤¹à¥‹ à¤—à¤¯à¤¾\n",
            "1.000 | à¤‡à¤¸à¥‡ à¤°à¥‡à¤¯à¤° à¤•à¥‡à¤¸ à¤®à¥‡à¤‚ à¤¶à¤¾à¤®à¤¿à¤² à¤•à¤¿à¤¯à¤¾ à¤—à¤¯à¤¾ à¤¹à¥ˆ â†” à¤œà¤¾à¤¨à¥‡à¤‚ à¤•à¤¿à¤¸ à¤ªà¥‡à¤œ à¤¸à¥‡ à¤‡à¤¸ à¤µà¥€à¤¡à¤¿à¤¯à¥‹ à¤•à¥‹ à¤•à¤¿à¤¯à¤¾ à¤—à¤¯à¤¾ à¤¶\n",
            "1.000 | à¤®à¤¾à¤®à¤²à¥‡ à¤®à¥‡à¤‚ à¤œà¤¾à¤‚à¤š à¤•à¤°à¤¾à¤ˆ à¤œà¤¾ à¤°à¤¹à¥€ à¤¹à¥ˆ â†” à¤ªà¥à¤²à¤¿à¤¸ à¤¤à¥€à¤¨à¥‹à¤‚ à¤®à¤¾à¤®à¤²à¥‹à¤‚ à¤®à¥‡à¤‚ à¤œà¤¾à¤‚à¤š à¤•à¤° à¤°à¤¹à¥€ à¤¹à¥ˆ\n",
            "1.000 | à¤²à¥‡à¤•à¤¿à¤¨ à¤†à¤ª à¤¹à¥ˆà¤¶ à¤”à¤° à¤ à¤‚à¤¡à¥‡ à¤°à¥‚à¤ª à¤®à¥‡à¤‚ à¤–à¤¾ à¤¸à¤•à¤¤à¥‡ à¤¹à¥ˆà¤‚ â†” à¤²à¥‡à¤•à¤¿à¤¨ à¤†à¤ª à¤ªà¤¾à¤¨à¥€ , à¤šà¤¾à¤¯ à¤”à¤° à¤•à¥‰à¥žà¥€ à¤²à¥‡ à¤¸à¤•à¤¤à¥‡ à¤¹à¥ˆ\n",
            "1.000 | à¤¡à¥€ à¤°à¤¾à¤œà¤¾ à¤¨à¥‡ à¤•à¤¹à¤¾ à¤²à¥‹à¤—à¥‹à¤‚ à¤•à¤¾ à¤§à¥à¤¯à¤¾à¤¨ à¤¬à¤‚à¤Ÿà¤¾ à¤°à¤¹à¥€ à¤¬ â†” à¤®à¥ˆà¤‚ à¤¯à¥‡ à¤¬à¤¾à¤¤à¥‡à¤‚ à¤œà¤¿à¤®à¥à¤®à¥‡à¤¦à¤¾à¤°à¥€ à¤•à¥‡ à¤¸à¤¾à¤¥ à¤•à¤¹ à¤°à¤¹à¤¾ à¤¹à¥‚\n",
            "1.000 | à¤‡à¤¸ à¤ªà¥ˆà¤• à¤®à¥‡à¤‚ à¤­à¥€ 28 à¤¦à¤¿à¤¨ à¤•à¥€ à¤¹à¥€ à¤µà¥ˆà¤²à¤¿à¤¡à¤¿à¤Ÿà¥€ à¤¹à¥ˆ â†” à¤‡à¤¸ à¤ªà¥à¤²à¤¾à¤¨ à¤•à¥€ à¤µà¥ˆà¤²à¤¿à¤¡à¤¿à¤Ÿà¥€ 28 à¤¦à¤¿à¤¨à¥‹à¤‚ à¤•à¥€ à¤¹à¥ˆ\n",
            "1.000 | 40 à¤•à¤°à¥‹à¤¡à¤¼ à¤²à¥‹à¤—à¥‹à¤‚ à¤•à¥‡ à¤–à¤¾à¤¤à¥‡ à¤–à¥‹à¤² à¤¦à¤¿à¤ â†” 40 à¤¸à¤¾à¤² à¤¨à¥Œà¤•à¤°à¥€ à¤•à¥‡ à¤¬à¤¾à¤¦ à¤¸à¥‡à¤µà¤¾à¤¨à¤¿à¤µà¥ƒà¤¤à¥à¤¤\n",
            "\n",
            "âœ… All tasks completed successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FsUmETxabZsQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}