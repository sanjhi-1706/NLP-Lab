{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "7b41696d",
      "metadata": {
        "id": "7b41696d"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "95c2d5c4",
      "metadata": {
        "id": "95c2d5c4"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Directly stream the Hindi-Devanagari split\n",
        "hindi_stream = load_dataset(\n",
        "    \"ai4bharat/IndicCorpV2\",\n",
        "    \"indiccorp_v2\",\n",
        "    streaming=True,\n",
        "    split=\"hin_Deva\"\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "3863ba5f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3863ba5f",
        "outputId": "e2f29e8e-1ac8-4017-f9d3-532112af2bca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (4.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.34.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.14)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.7.14)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_tokenizer_hindi_with_matras(text):\n",
        "    # Handle special items\n",
        "    url_pattern = r'https?://[^\\s]+|www\\.[^\\s]+'\n",
        "    email_pattern = r'\\b[\\w\\.-]+@[\\w\\.-]+\\.\\w{2,4}\\b'\n",
        "\n",
        "    urls = re.findall(url_pattern, text)\n",
        "    text = re.sub(url_pattern, '<URL>', text)\n",
        "\n",
        "    emails = re.findall(email_pattern, text)\n",
        "    text = re.sub(email_pattern, '<EMAIL>', text)\n",
        "\n",
        "    tokens = []\n",
        "    for word in text.split():\n",
        "        if word == '<URL>':\n",
        "            tokens.append(urls.pop(0))\n",
        "        elif word == '<EMAIL>':\n",
        "            tokens.append(emails.pop(0))\n",
        "        else:\n",
        "            # Tokenize Devanagari characters while separating matras\n",
        "            word_tokens = re.findall(rf'[\\u0915-\\u0939][\\u093e-\\u094c\\u0902\\u0903]?|\\d+|[^\\s\\w]', word)\n",
        "            tokens.extend(word_tokens)\n",
        "\n",
        "    return tokens\n"
      ],
      "metadata": {
        "id": "HHZDJb_sVy9P"
      },
      "id": "HHZDJb_sVy9P",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "0d43c61e",
      "metadata": {
        "id": "0d43c61e"
      },
      "outputs": [],
      "source": [
        "from itertools import islice\n",
        "\n",
        "samples = list(islice(hindi_stream, 10))  # change 5 to any number\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "947bf6a1",
      "metadata": {
        "id": "947bf6a1"
      },
      "outputs": [],
      "source": [
        "text = samples[0]['text']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "cf95337c",
      "metadata": {
        "id": "cf95337c"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def hindi_tokenizer(text):\n",
        "    url_pattern = r'https?://[^\\s]+|www\\.[^\\s]+'\n",
        "    email_pattern = r'\\b[\\w\\.-]+@[\\w\\.-]+\\.\\w{2,4}\\b'\n",
        "\n",
        "    # Save and replace URLs and emails\n",
        "    urls = re.findall(url_pattern, text)\n",
        "    text = re.sub(url_pattern, '<URL>', text)\n",
        "\n",
        "    emails = re.findall(email_pattern, text)\n",
        "    text = re.sub(email_pattern, '<EMAIL>', text)\n",
        "\n",
        "    tokens = []\n",
        "    for token in text.split():\n",
        "        if token == '<URL>':\n",
        "            tokens.append(urls.pop(0))\n",
        "        elif token == '<EMAIL>':\n",
        "            tokens.append(emails.pop(0))\n",
        "        else:\n",
        "            split_tokens = re.findall(\n",
        "                r'[\\u0900-\\u097F]+|[a-zA-Z0-9]+|[ред.,!?;:()\\\"\\'\\-]|[^\\s]',\n",
        "                token\n",
        "            )\n",
        "            tokens.extend(split_tokens)\n",
        "\n",
        "    return tokens\n",
        "\n",
        "def hindi_sentence_tokenizer(text):\n",
        "    sentence_end_pattern = r'(?<=[ред!?\\.])\\s+'\n",
        "    sentences = re.split(sentence_end_pattern, text.strip())\n",
        "    return [s.strip() for s in sentences if s.strip()]\n",
        "\n",
        "def detokenize(tokens):\n",
        "    # Rebuild sentence with proper spacing logic\n",
        "    sentence = ''\n",
        "    for i, token in enumerate(tokens):\n",
        "        if i > 0 and not re.match(r'[ред.,!?;:)\\]\\'\\\"]', token):\n",
        "            sentence += ' '\n",
        "        sentence += token\n",
        "    return sentence.strip()\n",
        "\n",
        "def hindi_corpus_statistics(text):\n",
        "    sentences = hindi_sentence_tokenizer(text)\n",
        "    all_tokens = []\n",
        "    reconstructed_sentences = []\n",
        "\n",
        "    for sentence in sentences:\n",
        "        tokens = hindi_tokenizer(sentence)\n",
        "        all_tokens.extend(tokens)\n",
        "\n",
        "        # For checking reformation\n",
        "        reconstructed = detokenize(tokens)\n",
        "        reconstructed_sentences.append(reconstructed)\n",
        "\n",
        "    num_tokens = len(all_tokens)\n",
        "    unique_tokens = set(all_tokens)\n",
        "    total_chars = sum(len(token) for token in all_tokens)\n",
        "\n",
        "    word_tokens = [t for t in all_tokens if re.match(r'^[\\u0900-\\u097F\\w]+$', t)]\n",
        "    avg_word_length = sum(len(t) for t in word_tokens) / len(word_tokens) if word_tokens else 0\n",
        "    type_token_ratio = len(unique_tokens) / num_tokens if num_tokens else 0\n",
        "\n",
        "    return {\n",
        "        'sentences': sentences,\n",
        "        'tokens': all_tokens,\n",
        "        'num_sentences':len(sentences),\n",
        "\n",
        "        're_num_sentences':len(reconstructed_sentences),\n",
        "        'num_tokens': num_tokens,\n",
        "        'total_characters': total_chars,\n",
        "        'average_word_length': round(avg_word_length, 2),\n",
        "        'type_token_ratio': round(type_token_ratio, 3),\n",
        "        'reconstructed_sentences': reconstructed_sentences\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "0c0f3048",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0c0f3048",
        "outputId": "691ebbe7-693a-44d4-c5d8-f1e307510388"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Sentences:\n",
            "рд▓реЛрдЧреЛрдВ рдХреЛ рдмрд┐рд▓реЛрдВ рд╕рдВрдмрдВрдзреА рд╕реБрд╡рд┐рдзрд╛ рджреЗрдирд╛ рд╣реА рдЙрдирдХрд╛ рдХрд╛рдо  рдЗрдиреЗрд▓реЛ 1987 рдореЗрдВ рдЙрд╕ рд╡рдХреНрдд рдРрд╕реЗ рд╣реА рджреЛрд░рд╛рд╣реЗ рдкрд░ рдЦрдбрд╝реА рдереА, рдЬрдм рдкреВрд░реНрд╡ рдЙрдкрдкреНрд░рдзрд╛рдирдордВрддреНрд░реА рджреЗрд╡реАрд▓рд╛рд▓ рдиреЗ рдЕрдкрдиреЗ рдкреБрддреНрд░ рдУрдордкреНрд░рдХрд╛рд╢ рдЪреМрдЯрд╛рд▓рд╛ рдХреЛ рдЕрдкрдирд╛ рд░рд╛рдЬрдиреАрддрд┐рдХ рдЙрддреНрддрд░рд╛рдзрд┐рдХрд╛рд░реА рдШреЛрд╖рд┐рдд рдХрд┐рдпрд╛ рдерд╛ред\n",
            "рд╣рд╛рд▓рд╛рдВрдХрд┐ рддрдм рдкрд╛рд░реНрдЯреА рдкрд░ рджреЗрд╡реАрд▓рд╛рд▓ рдХреА рдордЬрдмреВрдд рдкрдХрдбрд╝ рдХреЗ рдЪрд▓рддреЗ рдкрд╛рд░реНрдЯреА рдЯреВрдЯрдиреЗ рд╕реЗ рдмрдЪ рдЧрдИ рдереАред\n",
            "1989 рдореЗрдВ рджреЗрд╡реАрд▓рд╛рд▓ рдХреЗрдиреНрджреНрд░ рдХреА рд░рд╛рдЬрдиреАрддрд┐ рдореЗрдВ рд╕рдХреНрд░рд┐рдп рд╣реЛ рдЧрдП рдереЗ рдФрд░ рдЙрдирдХреЗ рдЙрдкрдкреНрд░рдзрд╛рдирдордВрддреНрд░реА рдмрдирдиреЗ рдХреЗ рдкрд╢реНрдЪрд╛рддреН рдЙрдирдХреЗ рддреАрди рдмреЗрдЯреЛрдВ рдЬрдЧрджреАрд╢ рд╕рд┐рдВрд╣, рд░рдгрдЬреАрдд рд╕рд┐рдВрд╣ рдФрд░ рдУрдордкреНрд░рдХрд╛рд╢ рдЪреМрдЯрд╛рд▓рд╛ рдореЗрдВ рд╕реЗ рд░рдгрдЬреАрдд рдФрд░ рдУрдордкреНрд░рдХрд╛рд╢ рдХреЗ рдмреАрдЪ рд╣рд░рд┐рдпрд╛рдгрд╛ рдореЗрдВ рдЙрдирдХреА рд░рд╛рдЬрдиреАрддрд┐рдХ рд╡рд┐рд░рд╛рд╕рдд рдХреЛ рд▓реЗрдХрд░ рдЬрдВрдЧ рд╢реБрд░реВ рд╣реЛ рдЧрдИ рдереАред\n",
            "рдЙрди рдкрд░рд┐рд╕реНрдерд┐рддрд┐рдпреЛрдВ рдореЗрдВ рджреЗрд╡реАрд▓рд╛рд▓ рдиреЗ рдХрдбрд╝рд╛ рдирд┐рд░реНрдгрдп рд▓реЗрддреЗ рд╣реБрдП рдкрд╛рд░реНрдЯреА рдХреА рдмрд╛рдЧрдбреЛрд░ рдУрдордкреНрд░рдХрд╛рд╢ рдЪреМрдЯрд╛рд▓рд╛ рдХреЗ рд╣рд╡рд╛рд▓реЗ рдХрд░ рджреА рдереА, рдЬрд┐рд╕рдХреЗ рдмрд╛рдж рд░рдгрдЬреАрдд рдХреА рдмрдЧрд╛рд╡рдд рдХрд╛ рдЕрд╕рд░ рдкрд╛рд░реНрдЯреА, рд╕рдВрдЧрдарди рдФрд░ рдЙрдирдХреА рд╕рд░рдХрд╛рд░ рдкрд░ рднреА рдкрдбрд╝рд╛ рдерд╛ред\n",
            "рдЙрд╕ рд╕рдордп рд░рдгрдЬреАрдд рдХреА рдирд╛рд░рд╛рдЬрдЧреА рдХреЗ рдЪрд▓рддреЗ рдЙрдирдХреЗ рд╕рдорд░реНрдерди рдореЗрдВ рдХрдИ рдХреИрдмрд┐рдиреЗрдЯ рдордВрддреНрд░рд┐рдпреЛрдВ рдиреЗ рдЗрд╕реНрддреАрдлреЗ рджреЗ рджрд┐рдП рдереЗ рдХрд┐рдиреНрддреБ рддрдм рдкрд╛рд░реНрдЯреА рд╕реБрдкреНрд░реАрдореЛ рдЪреМ.\n",
            "рджреЗрд╡реАрд▓рд╛рд▓ рдХреА рд╣рд░рд┐рдпрд╛рдгрд╛ рдХреА рдЬрдирддрд╛ рдкрд░ рдЗрддрдиреА рдордЬрдмреВрдд рдкрдХрдбрд╝ рдереА рдХрд┐ рдУрдордкреНрд░рдХрд╛рд╢ рдЪреМрдЯрд╛рд▓рд╛ рдХреЛ рдЙрддреНрддрд░рд╛рдзрд┐рдХрд╛рд░реА рдмрдирд╛рдиреЗ рдХреЗ рдЙрдирдХреЗ рдлреИрд╕рд▓реЗ рдХрд╛ рдЬрдирддрд╛ рдХреЗ рдмреАрдЪ рдХреЛрдИ рдЦрд╛рд╕ рд╡рд┐рд░реЛрдз рдирд╣реАрдВ рд╣реБрдЖ рдерд╛ рд▓реЗрдХрд┐рди рдЖрдЬ рд╕реНрдерд┐рддрд┐ рдмрд┐рд▓реНрдХреБрд▓ рд╡рд┐рдкрд░реАрдд рд╣реИред\n",
            "рдУрдордкреНрд░рдХрд╛рд╢ рдЪреМрдЯрд╛рд▓рд╛ рдкрд┐рдЫрд▓реЗ рдХрд╛рдлреА рд╕рдордп рд╕реЗ рдЬреЗрд▓ рдореЗрдВ рд╣реИрдВ рдФрд░ рдЬреЗрд▓ рдореЗрдВ рд░рд╣рддреЗ рдкрд╛рд░реНрдЯреА рдХреЗ рд╕рд╛рде-рд╕рд╛рде рдкрд░рд┐рд╡рд╛рд░ рдкрд░ рднреА рдЙрдирдХреА рдкрдХрдбрд╝ рдХрд╛рдлреА рдврд╝реАрд▓реА рд╣реЛ рдЧрдИ рд╣реИ, рдЗрд╕реА рдХрд╛рд░рдг рдЙрдирдореЗрдВ рдЕрдм рджреЗрд╡реАрд▓рд╛рд▓ рдЬреИрд╕рд╛ рд╡реЛ рд╕рд╛рдорд░реНрдереНрдп рдирдЬрд░ рдирд╣реАрдВ рдЖрддрд╛ рдХрд┐ рд╡реЗ рдЕрдкрдиреЗ рдлреИрд╕рд▓реЛрдВ рдХреЛ рдмрдЧреИрд░ рдХрд┐рд╕реА рдкреНрд░рддрд┐рд░реЛрдз рдХреЗ рд▓рд╛рдЧреВ рдХрд░рд╛ рд╕рдХреЗрдВред\n",
            "рдЬрд╣рд╛рдВ рдЖрдИ рдереА рддрдмрд╛рд╣реА рдЙрд╕ рдШрд╛рдЯреА рдХреНрд╖реЗрддреНрд░ рдореЗрдВ рдЦрддрд░рд╛ рдЬреНрдпрд╛рджрд╛  рдЗрд╕рдХреЗ рдмрд╛рдж рдХреЗрдВрджреНрд░ рдХреА рдУрд░ рд╕реЗ рдкреНрд░рджреЗрд╢ рд╕рд░рдХрд╛рд░ рдХреЛ рдкреАрдПрдордЬреАрдПрд╕рд╡рд╛рдИ рдореЗрдВ 200 рдХрд░реЛрдбрд╝ рд░реБрдкрдпреЗ рдХреА рд░рд╛рд╢рд┐ рдЙрдкрд▓рдмреНрдз рдХрд░рд╛ рджреА рдЧрдИред\n",
            "рднрд╛рдЬрдкрд╛ рдХреЗ рдореАрдбрд┐рдпрд╛ рдкреНрд░рднрд╛рд░реА рджрд┐рд╡рд╛рдХрд░ рд╕рд┐рдВрд╣ рдиреЗ рд╢рдирд┐рд╡рд╛рд░ рдХреЛ рдмрддрд╛рдпрд╛ рдХрд┐ рдХреЗрдВрджреНрд░ рдиреЗ рдкреНрд░рджреЗрд╢ рд╕рд░рдХрд╛рд░ рдХреЛ 200 рдХрд░реЛрдбрд╝ рд░реБрдкрдпреЗ рднреЗрдЬрд╛ рд╣реИред\n",
            "рдпрд╣ рдкреВрдЫрдиреЗ рдкрд░ рдХрд┐ рдЗрд╕ рдмрдбрд╝реЗ рдореИрдЪ рд╕реЗ рдкрд╣рд▓реЗ рдЙрдирдХреА рдиреАрдВрдж рдЧрд╛рдпрдм рд╣реБрдИ рддреЛ рдмрд╛рдмрд░ рдиреЗ рдХрд╣рд╛, \"рд╣рдо рдХрд╛рдлреА рдЯреВрд░реНрдирд╛рдореЗрдВрдЯ рдЦреЗрд▓ рдЪреБрдХреЗ рд╣реИрдВ, рд╣рдордиреЗ рдЪреИрдореНрдкрд┐рдпрдВрд╕ рдЯреНрд░рд╛рдлреА рдореЗрдВ рднреА рдЕрдЪреНрдЫрд╛ рдХрд┐рдпрд╛ рдерд╛.\n",
            "рд╣рдо рдЗрд╕реЗ рдЬрд┐рддрдирд╛ рд╕рд░рд▓ рд░рдЦреЗрдВрдЧреЗ, рдЙрддрдирд╛ рд╣реА рдмреЗрд╣рддрд░ рд╣реЛрдЧрд╛.\n",
            "рдЗрд╕рдореЗрдВ рд╕рд┐рд░реНрдл рдмреЗрд╕рд┐рдХреНрд╕ рдкрд░ рдЕрдбрд┐рдЧ рд░рд╣рдирд╛ рд╣реЛрдЧрд╛ рдФрд░ рд╕рд╛рде рд╣реА рд╢рд╛рдВрдд рдЪрд┐рддреНрдд рдмрдиреЗ рд░рд╣рдирд╛ рд╣реЛрдЧрд╛.\n",
            "рд╣рдорд╛рд░реА рддреИрдпрд╛рд░реА рд╣рдорд╛рд░реЗ рд╣рд╛рдереЛрдВ рдореЗрдВ рд╣реИрдВ рдФрд░ рд╣рдордиреЗ рдЕрдкрдирд╛ рд╢рдд рдкреНрд░рддрд┐рд╢рдд рджрд┐рдпрд╛ рд╣реИ.\n",
            "рд╣рдореЗрдВ рдореИрдЪ рдХреЗ рджрд┐рди рдЕрдЪреНрдЫреА рдХреНрд░рд┐рдХреЗрдЯ рдЦреЗрд▓рдиреЗ рдХреА рдЙрдореНрдореАрдж рд╣реИ.\"\n",
            "\n",
            "Reconstructed Sentences:\n",
            "рд▓реЛрдЧреЛрдВ рдХреЛ рдмрд┐рд▓реЛрдВ рд╕рдВрдмрдВрдзреА рд╕реБрд╡рд┐рдзрд╛ рджреЗрдирд╛ рд╣реА рдЙрдирдХрд╛ рдХрд╛рдо рдЗрдиреЗрд▓реЛ 1987 рдореЗрдВ рдЙрд╕ рд╡рдХреНрдд рдРрд╕реЗ рд╣реА рджреЛрд░рд╛рд╣реЗ рдкрд░ рдЦрдбрд╝реА рдереА, рдЬрдм рдкреВрд░реНрд╡ рдЙрдкрдкреНрд░рдзрд╛рдирдордВрддреНрд░реА рджреЗрд╡реАрд▓рд╛рд▓ рдиреЗ рдЕрдкрдиреЗ рдкреБрддреНрд░ рдУрдордкреНрд░рдХрд╛рд╢ рдЪреМрдЯрд╛рд▓рд╛ рдХреЛ рдЕрдкрдирд╛ рд░рд╛рдЬрдиреАрддрд┐рдХ рдЙрддреНрддрд░рд╛рдзрд┐рдХрд╛рд░реА рдШреЛрд╖рд┐рдд рдХрд┐рдпрд╛ рдерд╛ред\n",
            "рд╣рд╛рд▓рд╛рдВрдХрд┐ рддрдм рдкрд╛рд░реНрдЯреА рдкрд░ рджреЗрд╡реАрд▓рд╛рд▓ рдХреА рдордЬрдмреВрдд рдкрдХрдбрд╝ рдХреЗ рдЪрд▓рддреЗ рдкрд╛рд░реНрдЯреА рдЯреВрдЯрдиреЗ рд╕реЗ рдмрдЪ рдЧрдИ рдереАред\n",
            "1989 рдореЗрдВ рджреЗрд╡реАрд▓рд╛рд▓ рдХреЗрдиреНрджреНрд░ рдХреА рд░рд╛рдЬрдиреАрддрд┐ рдореЗрдВ рд╕рдХреНрд░рд┐рдп рд╣реЛ рдЧрдП рдереЗ рдФрд░ рдЙрдирдХреЗ рдЙрдкрдкреНрд░рдзрд╛рдирдордВрддреНрд░реА рдмрдирдиреЗ рдХреЗ рдкрд╢реНрдЪрд╛рддреН рдЙрдирдХреЗ рддреАрди рдмреЗрдЯреЛрдВ рдЬрдЧрджреАрд╢ рд╕рд┐рдВрд╣, рд░рдгрдЬреАрдд рд╕рд┐рдВрд╣ рдФрд░ рдУрдордкреНрд░рдХрд╛рд╢ рдЪреМрдЯрд╛рд▓рд╛ рдореЗрдВ рд╕реЗ рд░рдгрдЬреАрдд рдФрд░ рдУрдордкреНрд░рдХрд╛рд╢ рдХреЗ рдмреАрдЪ рд╣рд░рд┐рдпрд╛рдгрд╛ рдореЗрдВ рдЙрдирдХреА рд░рд╛рдЬрдиреАрддрд┐рдХ рд╡рд┐рд░рд╛рд╕рдд рдХреЛ рд▓реЗрдХрд░ рдЬрдВрдЧ рд╢реБрд░реВ рд╣реЛ рдЧрдИ рдереАред\n",
            "рдЙрди рдкрд░рд┐рд╕реНрдерд┐рддрд┐рдпреЛрдВ рдореЗрдВ рджреЗрд╡реАрд▓рд╛рд▓ рдиреЗ рдХрдбрд╝рд╛ рдирд┐рд░реНрдгрдп рд▓реЗрддреЗ рд╣реБрдП рдкрд╛рд░реНрдЯреА рдХреА рдмрд╛рдЧрдбреЛрд░ рдУрдордкреНрд░рдХрд╛рд╢ рдЪреМрдЯрд╛рд▓рд╛ рдХреЗ рд╣рд╡рд╛рд▓реЗ рдХрд░ рджреА рдереА, рдЬрд┐рд╕рдХреЗ рдмрд╛рдж рд░рдгрдЬреАрдд рдХреА рдмрдЧрд╛рд╡рдд рдХрд╛ рдЕрд╕рд░ рдкрд╛рд░реНрдЯреА, рд╕рдВрдЧрдарди рдФрд░ рдЙрдирдХреА рд╕рд░рдХрд╛рд░ рдкрд░ рднреА рдкрдбрд╝рд╛ рдерд╛ред\n",
            "рдЙрд╕ рд╕рдордп рд░рдгрдЬреАрдд рдХреА рдирд╛рд░рд╛рдЬрдЧреА рдХреЗ рдЪрд▓рддреЗ рдЙрдирдХреЗ рд╕рдорд░реНрдерди рдореЗрдВ рдХрдИ рдХреИрдмрд┐рдиреЗрдЯ рдордВрддреНрд░рд┐рдпреЛрдВ рдиреЗ рдЗрд╕реНрддреАрдлреЗ рджреЗ рджрд┐рдП рдереЗ рдХрд┐рдиреНрддреБ рддрдм рдкрд╛рд░реНрдЯреА рд╕реБрдкреНрд░реАрдореЛ рдЪреМ.\n",
            "рджреЗрд╡реАрд▓рд╛рд▓ рдХреА рд╣рд░рд┐рдпрд╛рдгрд╛ рдХреА рдЬрдирддрд╛ рдкрд░ рдЗрддрдиреА рдордЬрдмреВрдд рдкрдХрдбрд╝ рдереА рдХрд┐ рдУрдордкреНрд░рдХрд╛рд╢ рдЪреМрдЯрд╛рд▓рд╛ рдХреЛ рдЙрддреНрддрд░рд╛рдзрд┐рдХрд╛рд░реА рдмрдирд╛рдиреЗ рдХреЗ рдЙрдирдХреЗ рдлреИрд╕рд▓реЗ рдХрд╛ рдЬрдирддрд╛ рдХреЗ рдмреАрдЪ рдХреЛрдИ рдЦрд╛рд╕ рд╡рд┐рд░реЛрдз рдирд╣реАрдВ рд╣реБрдЖ рдерд╛ рд▓реЗрдХрд┐рди рдЖрдЬ рд╕реНрдерд┐рддрд┐ рдмрд┐рд▓реНрдХреБрд▓ рд╡рд┐рдкрд░реАрдд рд╣реИред\n",
            "рдУрдордкреНрд░рдХрд╛рд╢ рдЪреМрдЯрд╛рд▓рд╛ рдкрд┐рдЫрд▓реЗ рдХрд╛рдлреА рд╕рдордп рд╕реЗ рдЬреЗрд▓ рдореЗрдВ рд╣реИрдВ рдФрд░ рдЬреЗрд▓ рдореЗрдВ рд░рд╣рддреЗ рдкрд╛рд░реНрдЯреА рдХреЗ рд╕рд╛рде - рд╕рд╛рде рдкрд░рд┐рд╡рд╛рд░ рдкрд░ рднреА рдЙрдирдХреА рдкрдХрдбрд╝ рдХрд╛рдлреА рдврд╝реАрд▓реА рд╣реЛ рдЧрдИ рд╣реИ, рдЗрд╕реА рдХрд╛рд░рдг рдЙрдирдореЗрдВ рдЕрдм рджреЗрд╡реАрд▓рд╛рд▓ рдЬреИрд╕рд╛ рд╡реЛ рд╕рд╛рдорд░реНрдереНрдп рдирдЬрд░ рдирд╣реАрдВ рдЖрддрд╛ рдХрд┐ рд╡реЗ рдЕрдкрдиреЗ рдлреИрд╕рд▓реЛрдВ рдХреЛ рдмрдЧреИрд░ рдХрд┐рд╕реА рдкреНрд░рддрд┐рд░реЛрдз рдХреЗ рд▓рд╛рдЧреВ рдХрд░рд╛ рд╕рдХреЗрдВред\n",
            "рдЬрд╣рд╛рдВ рдЖрдИ рдереА рддрдмрд╛рд╣реА рдЙрд╕ рдШрд╛рдЯреА рдХреНрд╖реЗрддреНрд░ рдореЗрдВ рдЦрддрд░рд╛ рдЬреНрдпрд╛рджрд╛ рдЗрд╕рдХреЗ рдмрд╛рдж рдХреЗрдВрджреНрд░ рдХреА рдУрд░ рд╕реЗ рдкреНрд░рджреЗрд╢ рд╕рд░рдХрд╛рд░ рдХреЛ рдкреАрдПрдордЬреАрдПрд╕рд╡рд╛рдИ рдореЗрдВ 200 рдХрд░реЛрдбрд╝ рд░реБрдкрдпреЗ рдХреА рд░рд╛рд╢рд┐ рдЙрдкрд▓рдмреНрдз рдХрд░рд╛ рджреА рдЧрдИред\n",
            "рднрд╛рдЬрдкрд╛ рдХреЗ рдореАрдбрд┐рдпрд╛ рдкреНрд░рднрд╛рд░реА рджрд┐рд╡рд╛рдХрд░ рд╕рд┐рдВрд╣ рдиреЗ рд╢рдирд┐рд╡рд╛рд░ рдХреЛ рдмрддрд╛рдпрд╛ рдХрд┐ рдХреЗрдВрджреНрд░ рдиреЗ рдкреНрд░рджреЗрд╢ рд╕рд░рдХрд╛рд░ рдХреЛ 200 рдХрд░реЛрдбрд╝ рд░реБрдкрдпреЗ рднреЗрдЬрд╛ рд╣реИред\n",
            "рдпрд╣ рдкреВрдЫрдиреЗ рдкрд░ рдХрд┐ рдЗрд╕ рдмрдбрд╝реЗ рдореИрдЪ рд╕реЗ рдкрд╣рд▓реЗ рдЙрдирдХреА рдиреАрдВрдж рдЧрд╛рдпрдм рд╣реБрдИ рддреЛ рдмрд╛рдмрд░ рдиреЗ рдХрд╣рд╛,\" рд╣рдо рдХрд╛рдлреА рдЯреВрд░реНрдирд╛рдореЗрдВрдЯ рдЦреЗрд▓ рдЪреБрдХреЗ рд╣реИрдВ, рд╣рдордиреЗ рдЪреИрдореНрдкрд┐рдпрдВрд╕ рдЯреНрд░рд╛рдлреА рдореЗрдВ рднреА рдЕрдЪреНрдЫрд╛ рдХрд┐рдпрд╛ рдерд╛.\n",
            "рд╣рдо рдЗрд╕реЗ рдЬрд┐рддрдирд╛ рд╕рд░рд▓ рд░рдЦреЗрдВрдЧреЗ, рдЙрддрдирд╛ рд╣реА рдмреЗрд╣рддрд░ рд╣реЛрдЧрд╛.\n",
            "рдЗрд╕рдореЗрдВ рд╕рд┐рд░реНрдл рдмреЗрд╕рд┐рдХреНрд╕ рдкрд░ рдЕрдбрд┐рдЧ рд░рд╣рдирд╛ рд╣реЛрдЧрд╛ рдФрд░ рд╕рд╛рде рд╣реА рд╢рд╛рдВрдд рдЪрд┐рддреНрдд рдмрдиреЗ рд░рд╣рдирд╛ рд╣реЛрдЧрд╛.\n",
            "рд╣рдорд╛рд░реА рддреИрдпрд╛рд░реА рд╣рдорд╛рд░реЗ рд╣рд╛рдереЛрдВ рдореЗрдВ рд╣реИрдВ рдФрд░ рд╣рдордиреЗ рдЕрдкрдирд╛ рд╢рдд рдкреНрд░рддрд┐рд╢рдд рджрд┐рдпрд╛ рд╣реИ.\n",
            "рд╣рдореЗрдВ рдореИрдЪ рдХреЗ рджрд┐рди рдЕрдЪреНрдЫреА рдХреНрд░рд┐рдХреЗрдЯ рдЦреЗрд▓рдиреЗ рдХреА рдЙрдореНрдореАрдж рд╣реИ.\"\n",
            "\n",
            "Tokens: ['рд▓реЛрдЧреЛрдВ', 'рдХреЛ', 'рдмрд┐рд▓реЛрдВ', 'рд╕рдВрдмрдВрдзреА', 'рд╕реБрд╡рд┐рдзрд╛', 'рджреЗрдирд╛', 'рд╣реА', 'рдЙрдирдХрд╛', 'рдХрд╛рдо', 'рдЗрдиреЗрд▓реЛ', '1987', 'рдореЗрдВ', 'рдЙрд╕', 'рд╡рдХреНрдд', 'рдРрд╕реЗ', 'рд╣реА', 'рджреЛрд░рд╛рд╣реЗ', 'рдкрд░', 'рдЦрдбрд╝реА', 'рдереА', ',', 'рдЬрдм', 'рдкреВрд░реНрд╡', 'рдЙрдкрдкреНрд░рдзрд╛рдирдордВрддреНрд░реА', 'рджреЗрд╡реАрд▓рд╛рд▓', 'рдиреЗ', 'рдЕрдкрдиреЗ', 'рдкреБрддреНрд░', 'рдУрдордкреНрд░рдХрд╛рд╢', 'рдЪреМрдЯрд╛рд▓рд╛', 'рдХреЛ', 'рдЕрдкрдирд╛', 'рд░рд╛рдЬрдиреАрддрд┐рдХ', 'рдЙрддреНрддрд░рд╛рдзрд┐рдХрд╛рд░реА', 'рдШреЛрд╖рд┐рдд', 'рдХрд┐рдпрд╛', 'рдерд╛ред', 'рд╣рд╛рд▓рд╛рдВрдХрд┐', 'рддрдм', 'рдкрд╛рд░реНрдЯреА', 'рдкрд░', 'рджреЗрд╡реАрд▓рд╛рд▓', 'рдХреА', 'рдордЬрдмреВрдд', 'рдкрдХрдбрд╝', 'рдХреЗ', 'рдЪрд▓рддреЗ', 'рдкрд╛рд░реНрдЯреА', 'рдЯреВрдЯрдиреЗ', 'рд╕реЗ', 'рдмрдЪ', 'рдЧрдИ', 'рдереАред', '1989', 'рдореЗрдВ', 'рджреЗрд╡реАрд▓рд╛рд▓', 'рдХреЗрдиреНрджреНрд░', 'рдХреА', 'рд░рд╛рдЬрдиреАрддрд┐', 'рдореЗрдВ', 'рд╕рдХреНрд░рд┐рдп', 'рд╣реЛ', 'рдЧрдП', 'рдереЗ', 'рдФрд░', 'рдЙрдирдХреЗ', 'рдЙрдкрдкреНрд░рдзрд╛рдирдордВрддреНрд░реА', 'рдмрдирдиреЗ', 'рдХреЗ', 'рдкрд╢реНрдЪрд╛рддреН', 'рдЙрдирдХреЗ', 'рддреАрди', 'рдмреЗрдЯреЛрдВ', 'рдЬрдЧрджреАрд╢', 'рд╕рд┐рдВрд╣', ',', 'рд░рдгрдЬреАрдд', 'рд╕рд┐рдВрд╣', 'рдФрд░', 'рдУрдордкреНрд░рдХрд╛рд╢', 'рдЪреМрдЯрд╛рд▓рд╛', 'рдореЗрдВ', 'рд╕реЗ', 'рд░рдгрдЬреАрдд', 'рдФрд░', 'рдУрдордкреНрд░рдХрд╛рд╢', 'рдХреЗ', 'рдмреАрдЪ', 'рд╣рд░рд┐рдпрд╛рдгрд╛', 'рдореЗрдВ', 'рдЙрдирдХреА', 'рд░рд╛рдЬрдиреАрддрд┐рдХ', 'рд╡рд┐рд░рд╛рд╕рдд', 'рдХреЛ', 'рд▓реЗрдХрд░', 'рдЬрдВрдЧ', 'рд╢реБрд░реВ', 'рд╣реЛ', 'рдЧрдИ', 'рдереАред', 'рдЙрди', 'рдкрд░рд┐рд╕реНрдерд┐рддрд┐рдпреЛрдВ', 'рдореЗрдВ', 'рджреЗрд╡реАрд▓рд╛рд▓', 'рдиреЗ', 'рдХрдбрд╝рд╛', 'рдирд┐рд░реНрдгрдп', 'рд▓реЗрддреЗ', 'рд╣реБрдП', 'рдкрд╛рд░реНрдЯреА', 'рдХреА', 'рдмрд╛рдЧрдбреЛрд░', 'рдУрдордкреНрд░рдХрд╛рд╢', 'рдЪреМрдЯрд╛рд▓рд╛', 'рдХреЗ', 'рд╣рд╡рд╛рд▓реЗ', 'рдХрд░', 'рджреА', 'рдереА', ',', 'рдЬрд┐рд╕рдХреЗ', 'рдмрд╛рдж', 'рд░рдгрдЬреАрдд', 'рдХреА', 'рдмрдЧрд╛рд╡рдд', 'рдХрд╛', 'рдЕрд╕рд░', 'рдкрд╛рд░реНрдЯреА', ',', 'рд╕рдВрдЧрдарди', 'рдФрд░', 'рдЙрдирдХреА', 'рд╕рд░рдХрд╛рд░', 'рдкрд░', 'рднреА', 'рдкрдбрд╝рд╛', 'рдерд╛ред', 'рдЙрд╕', 'рд╕рдордп', 'рд░рдгрдЬреАрдд', 'рдХреА', 'рдирд╛рд░рд╛рдЬрдЧреА', 'рдХреЗ', 'рдЪрд▓рддреЗ', 'рдЙрдирдХреЗ', 'рд╕рдорд░реНрдерди', 'рдореЗрдВ', 'рдХрдИ', 'рдХреИрдмрд┐рдиреЗрдЯ', 'рдордВрддреНрд░рд┐рдпреЛрдВ', 'рдиреЗ', 'рдЗрд╕реНрддреАрдлреЗ', 'рджреЗ', 'рджрд┐рдП', 'рдереЗ', 'рдХрд┐рдиреНрддреБ', 'рддрдм', 'рдкрд╛рд░реНрдЯреА', 'рд╕реБрдкреНрд░реАрдореЛ', 'рдЪреМ', '.', 'рджреЗрд╡реАрд▓рд╛рд▓', 'рдХреА', 'рд╣рд░рд┐рдпрд╛рдгрд╛', 'рдХреА', 'рдЬрдирддрд╛', 'рдкрд░', 'рдЗрддрдиреА', 'рдордЬрдмреВрдд', 'рдкрдХрдбрд╝', 'рдереА', 'рдХрд┐', 'рдУрдордкреНрд░рдХрд╛рд╢', 'рдЪреМрдЯрд╛рд▓рд╛', 'рдХреЛ', 'рдЙрддреНрддрд░рд╛рдзрд┐рдХрд╛рд░реА', 'рдмрдирд╛рдиреЗ', 'рдХреЗ', 'рдЙрдирдХреЗ', 'рдлреИрд╕рд▓реЗ', 'рдХрд╛', 'рдЬрдирддрд╛', 'рдХреЗ', 'рдмреАрдЪ', 'рдХреЛрдИ', 'рдЦрд╛рд╕', 'рд╡рд┐рд░реЛрдз', 'рдирд╣реАрдВ', 'рд╣реБрдЖ', 'рдерд╛', 'рд▓реЗрдХрд┐рди', 'рдЖрдЬ', 'рд╕реНрдерд┐рддрд┐', 'рдмрд┐рд▓реНрдХреБрд▓', 'рд╡рд┐рдкрд░реАрдд', 'рд╣реИред', 'рдУрдордкреНрд░рдХрд╛рд╢', 'рдЪреМрдЯрд╛рд▓рд╛', 'рдкрд┐рдЫрд▓реЗ', 'рдХрд╛рдлреА', 'рд╕рдордп', 'рд╕реЗ', 'рдЬреЗрд▓', 'рдореЗрдВ', 'рд╣реИрдВ', 'рдФрд░', 'рдЬреЗрд▓', 'рдореЗрдВ', 'рд░рд╣рддреЗ', 'рдкрд╛рд░реНрдЯреА', 'рдХреЗ', 'рд╕рд╛рде', '-', 'рд╕рд╛рде', 'рдкрд░рд┐рд╡рд╛рд░', 'рдкрд░', 'рднреА', 'рдЙрдирдХреА', 'рдкрдХрдбрд╝', 'рдХрд╛рдлреА', 'рдврд╝реАрд▓реА', 'рд╣реЛ', 'рдЧрдИ', 'рд╣реИ', ',', 'рдЗрд╕реА', 'рдХрд╛рд░рдг', 'рдЙрдирдореЗрдВ', 'рдЕрдм', 'рджреЗрд╡реАрд▓рд╛рд▓', 'рдЬреИрд╕рд╛', 'рд╡реЛ', 'рд╕рд╛рдорд░реНрдереНрдп', 'рдирдЬрд░', 'рдирд╣реАрдВ', 'рдЖрддрд╛', 'рдХрд┐', 'рд╡реЗ', 'рдЕрдкрдиреЗ', 'рдлреИрд╕рд▓реЛрдВ', 'рдХреЛ', 'рдмрдЧреИрд░', 'рдХрд┐рд╕реА', 'рдкреНрд░рддрд┐рд░реЛрдз', 'рдХреЗ', 'рд▓рд╛рдЧреВ', 'рдХрд░рд╛', 'рд╕рдХреЗрдВред', 'рдЬрд╣рд╛рдВ', 'рдЖрдИ', 'рдереА', 'рддрдмрд╛рд╣реА', 'рдЙрд╕', 'рдШрд╛рдЯреА', 'рдХреНрд╖реЗрддреНрд░', 'рдореЗрдВ', 'рдЦрддрд░рд╛', 'рдЬреНрдпрд╛рджрд╛', 'рдЗрд╕рдХреЗ', 'рдмрд╛рдж', 'рдХреЗрдВрджреНрд░', 'рдХреА', 'рдУрд░', 'рд╕реЗ', 'рдкреНрд░рджреЗрд╢', 'рд╕рд░рдХрд╛рд░', 'рдХреЛ', 'рдкреАрдПрдордЬреАрдПрд╕рд╡рд╛рдИ', 'рдореЗрдВ', '200', 'рдХрд░реЛрдбрд╝', 'рд░реБрдкрдпреЗ', 'рдХреА', 'рд░рд╛рд╢рд┐', 'рдЙрдкрд▓рдмреНрдз', 'рдХрд░рд╛', 'рджреА', 'рдЧрдИред', 'рднрд╛рдЬрдкрд╛', 'рдХреЗ', 'рдореАрдбрд┐рдпрд╛', 'рдкреНрд░рднрд╛рд░реА', 'рджрд┐рд╡рд╛рдХрд░', 'рд╕рд┐рдВрд╣', 'рдиреЗ', 'рд╢рдирд┐рд╡рд╛рд░', 'рдХреЛ', 'рдмрддрд╛рдпрд╛', 'рдХрд┐', 'рдХреЗрдВрджреНрд░', 'рдиреЗ', 'рдкреНрд░рджреЗрд╢', 'рд╕рд░рдХрд╛рд░', 'рдХреЛ', '200', 'рдХрд░реЛрдбрд╝', 'рд░реБрдкрдпреЗ', 'рднреЗрдЬрд╛', 'рд╣реИред', 'рдпрд╣', 'рдкреВрдЫрдиреЗ', 'рдкрд░', 'рдХрд┐', 'рдЗрд╕', 'рдмрдбрд╝реЗ', 'рдореИрдЪ', 'рд╕реЗ', 'рдкрд╣рд▓реЗ', 'рдЙрдирдХреА', 'рдиреАрдВрдж', 'рдЧрд╛рдпрдм', 'рд╣реБрдИ', 'рддреЛ', 'рдмрд╛рдмрд░', 'рдиреЗ', 'рдХрд╣рд╛', ',', '\"', 'рд╣рдо', 'рдХрд╛рдлреА', 'рдЯреВрд░реНрдирд╛рдореЗрдВрдЯ', 'рдЦреЗрд▓', 'рдЪреБрдХреЗ', 'рд╣реИрдВ', ',', 'рд╣рдордиреЗ', 'рдЪреИрдореНрдкрд┐рдпрдВрд╕', 'рдЯреНрд░рд╛рдлреА', 'рдореЗрдВ', 'рднреА', 'рдЕрдЪреНрдЫрд╛', 'рдХрд┐рдпрд╛', 'рдерд╛', '.', 'рд╣рдо', 'рдЗрд╕реЗ', 'рдЬрд┐рддрдирд╛', 'рд╕рд░рд▓', 'рд░рдЦреЗрдВрдЧреЗ', ',', 'рдЙрддрдирд╛', 'рд╣реА', 'рдмреЗрд╣рддрд░', 'рд╣реЛрдЧрд╛', '.', 'рдЗрд╕рдореЗрдВ', 'рд╕рд┐рд░реНрдл', 'рдмреЗрд╕рд┐рдХреНрд╕', 'рдкрд░', 'рдЕрдбрд┐рдЧ', 'рд░рд╣рдирд╛', 'рд╣реЛрдЧрд╛', 'рдФрд░', 'рд╕рд╛рде', 'рд╣реА', 'рд╢рд╛рдВрдд', 'рдЪрд┐рддреНрдд', 'рдмрдиреЗ', 'рд░рд╣рдирд╛', 'рд╣реЛрдЧрд╛', '.', 'рд╣рдорд╛рд░реА', 'рддреИрдпрд╛рд░реА', 'рд╣рдорд╛рд░реЗ', 'рд╣рд╛рдереЛрдВ', 'рдореЗрдВ', 'рд╣реИрдВ', 'рдФрд░', 'рд╣рдордиреЗ', 'рдЕрдкрдирд╛', 'рд╢рдд', 'рдкреНрд░рддрд┐рд╢рдд', 'рджрд┐рдпрд╛', 'рд╣реИ', '.', 'рд╣рдореЗрдВ', 'рдореИрдЪ', 'рдХреЗ', 'рджрд┐рди', 'рдЕрдЪреНрдЫреА', 'рдХреНрд░рд┐рдХреЗрдЯ', 'рдЦреЗрд▓рдиреЗ', 'рдХреА', 'рдЙрдореНрдореАрдж', 'рд╣реИ', '.', '\"']\n",
            "Number of sentences: 14\n",
            "Number of sentences: 14\n",
            "Number of Tokens: 387\n",
            "Total Characters: 1512\n",
            "Average Word Length: 4.04\n",
            "Type-Token Ratio: 0.571\n"
          ]
        }
      ],
      "source": [
        "text = ' '.join(sample['text'] for sample in samples)\n",
        "stats = hindi_corpus_statistics(text)\n",
        "\n",
        "print(\"Original Sentences:\")\n",
        "for sentence in stats['sentences']:\n",
        "    print(sentence)\n",
        "\n",
        "print(\"\\nReconstructed Sentences:\")\n",
        "for sent in stats['reconstructed_sentences']:\n",
        "    print(sent)\n",
        "\n",
        "print(\"\\nTokens:\", stats['tokens'])\n",
        "print(\"Number of sentences:\",stats['num_sentences'])\n",
        "print(\"Number of sentences:\",stats['re_num_sentences'])\n",
        "print(\"Number of Tokens:\", stats['num_tokens'])\n",
        "print(\"Total Characters:\", stats['total_characters'])\n",
        "print(\"Average Word Length:\", stats['average_word_length'])\n",
        "print(\"Type-Token Ratio:\", stats['type_token_ratio'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "f0e5b1c2",
      "metadata": {
        "id": "f0e5b1c2"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def english_tokenizer(text):\n",
        "    # Define patterns\n",
        "    url_pattern = r'https?://[^\\s]+|www\\.[^\\s]+'\n",
        "    email_pattern = r'\\b[\\w\\.-]+@[\\w\\.-]+\\.\\w{2,}\\b'\n",
        "\n",
        "    # Save and replace URLs/emails\n",
        "    urls = re.findall(url_pattern, text)\n",
        "    text = re.sub(url_pattern, '<URL>', text)\n",
        "\n",
        "    emails = re.findall(email_pattern, text)\n",
        "    text = re.sub(email_pattern, '<EMAIL>', text)\n",
        "\n",
        "    tokens = []\n",
        "    for token in text.split():\n",
        "        if token == '<URL>':\n",
        "            tokens.append(urls.pop(0))\n",
        "        elif token == '<EMAIL>':\n",
        "            tokens.append(emails.pop(0))\n",
        "        else:\n",
        "            # Tokenize: words, numbers, punctuation\n",
        "            split_tokens = re.findall(r\"[a-zA-Z0-9]+|[.,!?;:'\\\"()\\-]|[^\\s]\", token)\n",
        "            tokens.extend(split_tokens)\n",
        "\n",
        "    return tokens\n",
        "\n",
        "def english_sentence_tokenizer(text):\n",
        "    # Split on sentence-ending punctuation followed by space\n",
        "    sentence_end_pattern = r'(?<=[.!?])\\s+'\n",
        "    sentences = re.split(sentence_end_pattern, text.strip())\n",
        "    return [s.strip() for s in sentences if s.strip()]\n",
        "\n",
        "def detokenize(tokens):\n",
        "    sentence = ''\n",
        "    for i, token in enumerate(tokens):\n",
        "        if i > 0 and not re.match(r'[.,!?;:)\\]\\'\\\"]', token):\n",
        "            sentence += ' '\n",
        "        sentence += token\n",
        "    return sentence.strip()\n",
        "\n",
        "def english_corpus_statistics(text):\n",
        "    sentences = english_sentence_tokenizer(text)\n",
        "    all_tokens = []\n",
        "    reconstructed_sentences = []\n",
        "\n",
        "    for sentence in sentences:\n",
        "        tokens = english_tokenizer(sentence)\n",
        "        all_tokens.extend(tokens)\n",
        "\n",
        "        reconstructed = detokenize(tokens)\n",
        "        reconstructed_sentences.append(reconstructed)\n",
        "\n",
        "    num_tokens = len(all_tokens)\n",
        "    unique_tokens = set(all_tokens)\n",
        "    total_chars = sum(len(token) for token in all_tokens)\n",
        "\n",
        "    word_tokens = [t for t in all_tokens if re.match(r'^[a-zA-Z0-9]+$', t)]\n",
        "    avg_word_length = sum(len(t) for t in word_tokens) / len(word_tokens) if word_tokens else 0\n",
        "    type_token_ratio = len(unique_tokens) / num_tokens if num_tokens else 0\n",
        "\n",
        "    return {\n",
        "        'sentences': sentences,\n",
        "        'tokens': all_tokens,\n",
        "        'num_tokens': num_tokens,\n",
        "        'total_characters': total_chars,\n",
        "        'average_word_length': round(avg_word_length, 2),\n",
        "        'type_token_ratio': round(type_token_ratio, 3),\n",
        "        'reconstructed_sentences': reconstructed_sentences\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "fe7ae645",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fe7ae645",
        "outputId": "3a6d29c5-6017-4ba7-b4d3-6a7b5f163d2b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Sentences:\n",
            "Hello!\n",
            "I'm Alex.\n",
            "Feel free to drop a message at alex.jordan@gmail.com or visit my blog at www.alexwrites.com.\n",
            "Hope you find it interesting!\n",
            "\n",
            "Reconstructed Sentences:\n",
            "Hello!\n",
            "I' m Alex.\n",
            "Feel free to drop a message at alex.jordan@gmail.com or visit my blog at www.alexwrites.com.\n",
            "Hope you find it interesting!\n",
            "\n",
            "Tokens: ['Hello', '!', 'I', \"'\", 'm', 'Alex', '.', 'Feel', 'free', 'to', 'drop', 'a', 'message', 'at', 'alex.jordan@gmail.com', 'or', 'visit', 'my', 'blog', 'at', 'www.alexwrites.com.', 'Hope', 'you', 'find', 'it', 'interesting', '!']\n",
            "Number of Tokens: 27\n",
            "Total Characters: 118\n",
            "Average Word Length: 3.52\n",
            "Type-Token Ratio: 0.926\n"
          ]
        }
      ],
      "source": [
        "text = \"\"\"Hello! I'm Alex. Feel free to drop a message at alex.jordan@gmail.com or visit my blog at www.alexwrites.com. Hope you find it interesting!\"\"\"\n",
        "\n",
        "stats = english_corpus_statistics(text)\n",
        "\n",
        "print(\"Original Sentences:\")\n",
        "for s in stats['sentences']:\n",
        "    print(s)\n",
        "\n",
        "print(\"\\nReconstructed Sentences:\")\n",
        "for s in stats['reconstructed_sentences']:\n",
        "    print(s)\n",
        "\n",
        "print(\"\\nTokens:\", stats['tokens'])\n",
        "print(\"Number of Tokens:\", stats['num_tokens'])\n",
        "print(\"Total Characters:\", stats['total_characters'])\n",
        "print(\"Average Word Length:\", stats['average_word_length'])\n",
        "print(\"Type-Token Ratio:\", stats['type_token_ratio'])\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}