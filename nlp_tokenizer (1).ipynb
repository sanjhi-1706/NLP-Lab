{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "7b41696d",
      "metadata": {
        "id": "7b41696d"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "95c2d5c4",
      "metadata": {
        "id": "95c2d5c4"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Directly stream the Hindi-Devanagari split\n",
        "hindi_stream = load_dataset(\n",
        "    \"ai4bharat/IndicCorpV2\",\n",
        "    \"indiccorp_v2\",\n",
        "    streaming=True,\n",
        "    split=\"hin_Deva\"\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "3863ba5f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3863ba5f",
        "outputId": "e2f29e8e-1ac8-4017-f9d3-532112af2bca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (4.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.34.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.14)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.7.14)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_tokenizer_hindi_with_matras(text):\n",
        "    # Handle special items\n",
        "    url_pattern = r'https?://[^\\s]+|www\\.[^\\s]+'\n",
        "    email_pattern = r'\\b[\\w\\.-]+@[\\w\\.-]+\\.\\w{2,4}\\b'\n",
        "\n",
        "    urls = re.findall(url_pattern, text)\n",
        "    text = re.sub(url_pattern, '<URL>', text)\n",
        "\n",
        "    emails = re.findall(email_pattern, text)\n",
        "    text = re.sub(email_pattern, '<EMAIL>', text)\n",
        "\n",
        "    tokens = []\n",
        "    for word in text.split():\n",
        "        if word == '<URL>':\n",
        "            tokens.append(urls.pop(0))\n",
        "        elif word == '<EMAIL>':\n",
        "            tokens.append(emails.pop(0))\n",
        "        else:\n",
        "            # Tokenize Devanagari characters while separating matras\n",
        "            word_tokens = re.findall(rf'[\\u0915-\\u0939][\\u093e-\\u094c\\u0902\\u0903]?|\\d+|[^\\s\\w]', word)\n",
        "            tokens.extend(word_tokens)\n",
        "\n",
        "    return tokens\n"
      ],
      "metadata": {
        "id": "HHZDJb_sVy9P"
      },
      "id": "HHZDJb_sVy9P",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "0d43c61e",
      "metadata": {
        "id": "0d43c61e"
      },
      "outputs": [],
      "source": [
        "from itertools import islice\n",
        "\n",
        "samples = list(islice(hindi_stream, 10))  # change 5 to any number\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "947bf6a1",
      "metadata": {
        "id": "947bf6a1"
      },
      "outputs": [],
      "source": [
        "text = samples[0]['text']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "cf95337c",
      "metadata": {
        "id": "cf95337c"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def hindi_tokenizer(text):\n",
        "    url_pattern = r'https?://[^\\s]+|www\\.[^\\s]+'\n",
        "    email_pattern = r'\\b[\\w\\.-]+@[\\w\\.-]+\\.\\w{2,4}\\b'\n",
        "\n",
        "    # Save and replace URLs and emails\n",
        "    urls = re.findall(url_pattern, text)\n",
        "    text = re.sub(url_pattern, '<URL>', text)\n",
        "\n",
        "    emails = re.findall(email_pattern, text)\n",
        "    text = re.sub(email_pattern, '<EMAIL>', text)\n",
        "\n",
        "    tokens = []\n",
        "    for token in text.split():\n",
        "        if token == '<URL>':\n",
        "            tokens.append(urls.pop(0))\n",
        "        elif token == '<EMAIL>':\n",
        "            tokens.append(emails.pop(0))\n",
        "        else:\n",
        "            split_tokens = re.findall(\n",
        "                r'[\\u0900-\\u097F]+|[a-zA-Z0-9]+|[।.,!?;:()\\\"\\'\\-]|[^\\s]',\n",
        "                token\n",
        "            )\n",
        "            tokens.extend(split_tokens)\n",
        "\n",
        "    return tokens\n",
        "\n",
        "def hindi_sentence_tokenizer(text):\n",
        "    sentence_end_pattern = r'(?<=[।!?\\.])\\s+'\n",
        "    sentences = re.split(sentence_end_pattern, text.strip())\n",
        "    return [s.strip() for s in sentences if s.strip()]\n",
        "\n",
        "def detokenize(tokens):\n",
        "    # Rebuild sentence with proper spacing logic\n",
        "    sentence = ''\n",
        "    for i, token in enumerate(tokens):\n",
        "        if i > 0 and not re.match(r'[।.,!?;:)\\]\\'\\\"]', token):\n",
        "            sentence += ' '\n",
        "        sentence += token\n",
        "    return sentence.strip()\n",
        "\n",
        "def hindi_corpus_statistics(text):\n",
        "    sentences = hindi_sentence_tokenizer(text)\n",
        "    all_tokens = []\n",
        "    reconstructed_sentences = []\n",
        "\n",
        "    for sentence in sentences:\n",
        "        tokens = hindi_tokenizer(sentence)\n",
        "        all_tokens.extend(tokens)\n",
        "\n",
        "        # For checking reformation\n",
        "        reconstructed = detokenize(tokens)\n",
        "        reconstructed_sentences.append(reconstructed)\n",
        "\n",
        "    num_tokens = len(all_tokens)\n",
        "    unique_tokens = set(all_tokens)\n",
        "    total_chars = sum(len(token) for token in all_tokens)\n",
        "\n",
        "    word_tokens = [t for t in all_tokens if re.match(r'^[\\u0900-\\u097F\\w]+$', t)]\n",
        "    avg_word_length = sum(len(t) for t in word_tokens) / len(word_tokens) if word_tokens else 0\n",
        "    type_token_ratio = len(unique_tokens) / num_tokens if num_tokens else 0\n",
        "\n",
        "    return {\n",
        "        'sentences': sentences,\n",
        "        'tokens': all_tokens,\n",
        "        'num_sentences':len(sentences),\n",
        "\n",
        "        're_num_sentences':len(reconstructed_sentences),\n",
        "        'num_tokens': num_tokens,\n",
        "        'total_characters': total_chars,\n",
        "        'average_word_length': round(avg_word_length, 2),\n",
        "        'type_token_ratio': round(type_token_ratio, 3),\n",
        "        'reconstructed_sentences': reconstructed_sentences\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "0c0f3048",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0c0f3048",
        "outputId": "691ebbe7-693a-44d4-c5d8-f1e307510388"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Sentences:\n",
            "लोगों को बिलों संबंधी सुविधा देना ही उनका काम  इनेलो 1987 में उस वक्त ऐसे ही दोराहे पर खड़ी थी, जब पूर्व उपप्रधानमंत्री देवीलाल ने अपने पुत्र ओमप्रकाश चौटाला को अपना राजनीतिक उत्तराधिकारी घोषित किया था।\n",
            "हालांकि तब पार्टी पर देवीलाल की मजबूत पकड़ के चलते पार्टी टूटने से बच गई थी।\n",
            "1989 में देवीलाल केन्द्र की राजनीति में सक्रिय हो गए थे और उनके उपप्रधानमंत्री बनने के पश्चात् उनके तीन बेटों जगदीश सिंह, रणजीत सिंह और ओमप्रकाश चौटाला में से रणजीत और ओमप्रकाश के बीच हरियाणा में उनकी राजनीतिक विरासत को लेकर जंग शुरू हो गई थी।\n",
            "उन परिस्थितियों में देवीलाल ने कड़ा निर्णय लेते हुए पार्टी की बागडोर ओमप्रकाश चौटाला के हवाले कर दी थी, जिसके बाद रणजीत की बगावत का असर पार्टी, संगठन और उनकी सरकार पर भी पड़ा था।\n",
            "उस समय रणजीत की नाराजगी के चलते उनके समर्थन में कई कैबिनेट मंत्रियों ने इस्तीफे दे दिए थे किन्तु तब पार्टी सुप्रीमो चौ.\n",
            "देवीलाल की हरियाणा की जनता पर इतनी मजबूत पकड़ थी कि ओमप्रकाश चौटाला को उत्तराधिकारी बनाने के उनके फैसले का जनता के बीच कोई खास विरोध नहीं हुआ था लेकिन आज स्थिति बिल्कुल विपरीत है।\n",
            "ओमप्रकाश चौटाला पिछले काफी समय से जेल में हैं और जेल में रहते पार्टी के साथ-साथ परिवार पर भी उनकी पकड़ काफी ढ़ीली हो गई है, इसी कारण उनमें अब देवीलाल जैसा वो सामर्थ्य नजर नहीं आता कि वे अपने फैसलों को बगैर किसी प्रतिरोध के लागू करा सकें।\n",
            "जहां आई थी तबाही उस घाटी क्षेत्र में खतरा ज्यादा  इसके बाद केंद्र की ओर से प्रदेश सरकार को पीएमजीएसवाई में 200 करोड़ रुपये की राशि उपलब्ध करा दी गई।\n",
            "भाजपा के मीडिया प्रभारी दिवाकर सिंह ने शनिवार को बताया कि केंद्र ने प्रदेश सरकार को 200 करोड़ रुपये भेजा है।\n",
            "यह पूछने पर कि इस बड़े मैच से पहले उनकी नींद गायब हुई तो बाबर ने कहा, \"हम काफी टूर्नामेंट खेल चुके हैं, हमने चैम्पियंस ट्राफी में भी अच्छा किया था.\n",
            "हम इसे जितना सरल रखेंगे, उतना ही बेहतर होगा.\n",
            "इसमें सिर्फ बेसिक्स पर अडिग रहना होगा और साथ ही शांत चित्त बने रहना होगा.\n",
            "हमारी तैयारी हमारे हाथों में हैं और हमने अपना शत प्रतिशत दिया है.\n",
            "हमें मैच के दिन अच्छी क्रिकेट खेलने की उम्मीद है.\"\n",
            "\n",
            "Reconstructed Sentences:\n",
            "लोगों को बिलों संबंधी सुविधा देना ही उनका काम इनेलो 1987 में उस वक्त ऐसे ही दोराहे पर खड़ी थी, जब पूर्व उपप्रधानमंत्री देवीलाल ने अपने पुत्र ओमप्रकाश चौटाला को अपना राजनीतिक उत्तराधिकारी घोषित किया था।\n",
            "हालांकि तब पार्टी पर देवीलाल की मजबूत पकड़ के चलते पार्टी टूटने से बच गई थी।\n",
            "1989 में देवीलाल केन्द्र की राजनीति में सक्रिय हो गए थे और उनके उपप्रधानमंत्री बनने के पश्चात् उनके तीन बेटों जगदीश सिंह, रणजीत सिंह और ओमप्रकाश चौटाला में से रणजीत और ओमप्रकाश के बीच हरियाणा में उनकी राजनीतिक विरासत को लेकर जंग शुरू हो गई थी।\n",
            "उन परिस्थितियों में देवीलाल ने कड़ा निर्णय लेते हुए पार्टी की बागडोर ओमप्रकाश चौटाला के हवाले कर दी थी, जिसके बाद रणजीत की बगावत का असर पार्टी, संगठन और उनकी सरकार पर भी पड़ा था।\n",
            "उस समय रणजीत की नाराजगी के चलते उनके समर्थन में कई कैबिनेट मंत्रियों ने इस्तीफे दे दिए थे किन्तु तब पार्टी सुप्रीमो चौ.\n",
            "देवीलाल की हरियाणा की जनता पर इतनी मजबूत पकड़ थी कि ओमप्रकाश चौटाला को उत्तराधिकारी बनाने के उनके फैसले का जनता के बीच कोई खास विरोध नहीं हुआ था लेकिन आज स्थिति बिल्कुल विपरीत है।\n",
            "ओमप्रकाश चौटाला पिछले काफी समय से जेल में हैं और जेल में रहते पार्टी के साथ - साथ परिवार पर भी उनकी पकड़ काफी ढ़ीली हो गई है, इसी कारण उनमें अब देवीलाल जैसा वो सामर्थ्य नजर नहीं आता कि वे अपने फैसलों को बगैर किसी प्रतिरोध के लागू करा सकें।\n",
            "जहां आई थी तबाही उस घाटी क्षेत्र में खतरा ज्यादा इसके बाद केंद्र की ओर से प्रदेश सरकार को पीएमजीएसवाई में 200 करोड़ रुपये की राशि उपलब्ध करा दी गई।\n",
            "भाजपा के मीडिया प्रभारी दिवाकर सिंह ने शनिवार को बताया कि केंद्र ने प्रदेश सरकार को 200 करोड़ रुपये भेजा है।\n",
            "यह पूछने पर कि इस बड़े मैच से पहले उनकी नींद गायब हुई तो बाबर ने कहा,\" हम काफी टूर्नामेंट खेल चुके हैं, हमने चैम्पियंस ट्राफी में भी अच्छा किया था.\n",
            "हम इसे जितना सरल रखेंगे, उतना ही बेहतर होगा.\n",
            "इसमें सिर्फ बेसिक्स पर अडिग रहना होगा और साथ ही शांत चित्त बने रहना होगा.\n",
            "हमारी तैयारी हमारे हाथों में हैं और हमने अपना शत प्रतिशत दिया है.\n",
            "हमें मैच के दिन अच्छी क्रिकेट खेलने की उम्मीद है.\"\n",
            "\n",
            "Tokens: ['लोगों', 'को', 'बिलों', 'संबंधी', 'सुविधा', 'देना', 'ही', 'उनका', 'काम', 'इनेलो', '1987', 'में', 'उस', 'वक्त', 'ऐसे', 'ही', 'दोराहे', 'पर', 'खड़ी', 'थी', ',', 'जब', 'पूर्व', 'उपप्रधानमंत्री', 'देवीलाल', 'ने', 'अपने', 'पुत्र', 'ओमप्रकाश', 'चौटाला', 'को', 'अपना', 'राजनीतिक', 'उत्तराधिकारी', 'घोषित', 'किया', 'था।', 'हालांकि', 'तब', 'पार्टी', 'पर', 'देवीलाल', 'की', 'मजबूत', 'पकड़', 'के', 'चलते', 'पार्टी', 'टूटने', 'से', 'बच', 'गई', 'थी।', '1989', 'में', 'देवीलाल', 'केन्द्र', 'की', 'राजनीति', 'में', 'सक्रिय', 'हो', 'गए', 'थे', 'और', 'उनके', 'उपप्रधानमंत्री', 'बनने', 'के', 'पश्चात्', 'उनके', 'तीन', 'बेटों', 'जगदीश', 'सिंह', ',', 'रणजीत', 'सिंह', 'और', 'ओमप्रकाश', 'चौटाला', 'में', 'से', 'रणजीत', 'और', 'ओमप्रकाश', 'के', 'बीच', 'हरियाणा', 'में', 'उनकी', 'राजनीतिक', 'विरासत', 'को', 'लेकर', 'जंग', 'शुरू', 'हो', 'गई', 'थी।', 'उन', 'परिस्थितियों', 'में', 'देवीलाल', 'ने', 'कड़ा', 'निर्णय', 'लेते', 'हुए', 'पार्टी', 'की', 'बागडोर', 'ओमप्रकाश', 'चौटाला', 'के', 'हवाले', 'कर', 'दी', 'थी', ',', 'जिसके', 'बाद', 'रणजीत', 'की', 'बगावत', 'का', 'असर', 'पार्टी', ',', 'संगठन', 'और', 'उनकी', 'सरकार', 'पर', 'भी', 'पड़ा', 'था।', 'उस', 'समय', 'रणजीत', 'की', 'नाराजगी', 'के', 'चलते', 'उनके', 'समर्थन', 'में', 'कई', 'कैबिनेट', 'मंत्रियों', 'ने', 'इस्तीफे', 'दे', 'दिए', 'थे', 'किन्तु', 'तब', 'पार्टी', 'सुप्रीमो', 'चौ', '.', 'देवीलाल', 'की', 'हरियाणा', 'की', 'जनता', 'पर', 'इतनी', 'मजबूत', 'पकड़', 'थी', 'कि', 'ओमप्रकाश', 'चौटाला', 'को', 'उत्तराधिकारी', 'बनाने', 'के', 'उनके', 'फैसले', 'का', 'जनता', 'के', 'बीच', 'कोई', 'खास', 'विरोध', 'नहीं', 'हुआ', 'था', 'लेकिन', 'आज', 'स्थिति', 'बिल्कुल', 'विपरीत', 'है।', 'ओमप्रकाश', 'चौटाला', 'पिछले', 'काफी', 'समय', 'से', 'जेल', 'में', 'हैं', 'और', 'जेल', 'में', 'रहते', 'पार्टी', 'के', 'साथ', '-', 'साथ', 'परिवार', 'पर', 'भी', 'उनकी', 'पकड़', 'काफी', 'ढ़ीली', 'हो', 'गई', 'है', ',', 'इसी', 'कारण', 'उनमें', 'अब', 'देवीलाल', 'जैसा', 'वो', 'सामर्थ्य', 'नजर', 'नहीं', 'आता', 'कि', 'वे', 'अपने', 'फैसलों', 'को', 'बगैर', 'किसी', 'प्रतिरोध', 'के', 'लागू', 'करा', 'सकें।', 'जहां', 'आई', 'थी', 'तबाही', 'उस', 'घाटी', 'क्षेत्र', 'में', 'खतरा', 'ज्यादा', 'इसके', 'बाद', 'केंद्र', 'की', 'ओर', 'से', 'प्रदेश', 'सरकार', 'को', 'पीएमजीएसवाई', 'में', '200', 'करोड़', 'रुपये', 'की', 'राशि', 'उपलब्ध', 'करा', 'दी', 'गई।', 'भाजपा', 'के', 'मीडिया', 'प्रभारी', 'दिवाकर', 'सिंह', 'ने', 'शनिवार', 'को', 'बताया', 'कि', 'केंद्र', 'ने', 'प्रदेश', 'सरकार', 'को', '200', 'करोड़', 'रुपये', 'भेजा', 'है।', 'यह', 'पूछने', 'पर', 'कि', 'इस', 'बड़े', 'मैच', 'से', 'पहले', 'उनकी', 'नींद', 'गायब', 'हुई', 'तो', 'बाबर', 'ने', 'कहा', ',', '\"', 'हम', 'काफी', 'टूर्नामेंट', 'खेल', 'चुके', 'हैं', ',', 'हमने', 'चैम्पियंस', 'ट्राफी', 'में', 'भी', 'अच्छा', 'किया', 'था', '.', 'हम', 'इसे', 'जितना', 'सरल', 'रखेंगे', ',', 'उतना', 'ही', 'बेहतर', 'होगा', '.', 'इसमें', 'सिर्फ', 'बेसिक्स', 'पर', 'अडिग', 'रहना', 'होगा', 'और', 'साथ', 'ही', 'शांत', 'चित्त', 'बने', 'रहना', 'होगा', '.', 'हमारी', 'तैयारी', 'हमारे', 'हाथों', 'में', 'हैं', 'और', 'हमने', 'अपना', 'शत', 'प्रतिशत', 'दिया', 'है', '.', 'हमें', 'मैच', 'के', 'दिन', 'अच्छी', 'क्रिकेट', 'खेलने', 'की', 'उम्मीद', 'है', '.', '\"']\n",
            "Number of sentences: 14\n",
            "Number of sentences: 14\n",
            "Number of Tokens: 387\n",
            "Total Characters: 1512\n",
            "Average Word Length: 4.04\n",
            "Type-Token Ratio: 0.571\n"
          ]
        }
      ],
      "source": [
        "text = ' '.join(sample['text'] for sample in samples)\n",
        "stats = hindi_corpus_statistics(text)\n",
        "\n",
        "print(\"Original Sentences:\")\n",
        "for sentence in stats['sentences']:\n",
        "    print(sentence)\n",
        "\n",
        "print(\"\\nReconstructed Sentences:\")\n",
        "for sent in stats['reconstructed_sentences']:\n",
        "    print(sent)\n",
        "\n",
        "print(\"\\nTokens:\", stats['tokens'])\n",
        "print(\"Number of sentences:\",stats['num_sentences'])\n",
        "print(\"Number of sentences:\",stats['re_num_sentences'])\n",
        "print(\"Number of Tokens:\", stats['num_tokens'])\n",
        "print(\"Total Characters:\", stats['total_characters'])\n",
        "print(\"Average Word Length:\", stats['average_word_length'])\n",
        "print(\"Type-Token Ratio:\", stats['type_token_ratio'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "f0e5b1c2",
      "metadata": {
        "id": "f0e5b1c2"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def english_tokenizer(text):\n",
        "    # Define patterns\n",
        "    url_pattern = r'https?://[^\\s]+|www\\.[^\\s]+'\n",
        "    email_pattern = r'\\b[\\w\\.-]+@[\\w\\.-]+\\.\\w{2,}\\b'\n",
        "\n",
        "    # Save and replace URLs/emails\n",
        "    urls = re.findall(url_pattern, text)\n",
        "    text = re.sub(url_pattern, '<URL>', text)\n",
        "\n",
        "    emails = re.findall(email_pattern, text)\n",
        "    text = re.sub(email_pattern, '<EMAIL>', text)\n",
        "\n",
        "    tokens = []\n",
        "    for token in text.split():\n",
        "        if token == '<URL>':\n",
        "            tokens.append(urls.pop(0))\n",
        "        elif token == '<EMAIL>':\n",
        "            tokens.append(emails.pop(0))\n",
        "        else:\n",
        "            # Tokenize: words, numbers, punctuation\n",
        "            split_tokens = re.findall(r\"[a-zA-Z0-9]+|[.,!?;:'\\\"()\\-]|[^\\s]\", token)\n",
        "            tokens.extend(split_tokens)\n",
        "\n",
        "    return tokens\n",
        "\n",
        "def english_sentence_tokenizer(text):\n",
        "    # Split on sentence-ending punctuation followed by space\n",
        "    sentence_end_pattern = r'(?<=[.!?])\\s+'\n",
        "    sentences = re.split(sentence_end_pattern, text.strip())\n",
        "    return [s.strip() for s in sentences if s.strip()]\n",
        "\n",
        "def detokenize(tokens):\n",
        "    sentence = ''\n",
        "    for i, token in enumerate(tokens):\n",
        "        if i > 0 and not re.match(r'[.,!?;:)\\]\\'\\\"]', token):\n",
        "            sentence += ' '\n",
        "        sentence += token\n",
        "    return sentence.strip()\n",
        "\n",
        "def english_corpus_statistics(text):\n",
        "    sentences = english_sentence_tokenizer(text)\n",
        "    all_tokens = []\n",
        "    reconstructed_sentences = []\n",
        "\n",
        "    for sentence in sentences:\n",
        "        tokens = english_tokenizer(sentence)\n",
        "        all_tokens.extend(tokens)\n",
        "\n",
        "        reconstructed = detokenize(tokens)\n",
        "        reconstructed_sentences.append(reconstructed)\n",
        "\n",
        "    num_tokens = len(all_tokens)\n",
        "    unique_tokens = set(all_tokens)\n",
        "    total_chars = sum(len(token) for token in all_tokens)\n",
        "\n",
        "    word_tokens = [t for t in all_tokens if re.match(r'^[a-zA-Z0-9]+$', t)]\n",
        "    avg_word_length = sum(len(t) for t in word_tokens) / len(word_tokens) if word_tokens else 0\n",
        "    type_token_ratio = len(unique_tokens) / num_tokens if num_tokens else 0\n",
        "\n",
        "    return {\n",
        "        'sentences': sentences,\n",
        "        'tokens': all_tokens,\n",
        "        'num_tokens': num_tokens,\n",
        "        'total_characters': total_chars,\n",
        "        'average_word_length': round(avg_word_length, 2),\n",
        "        'type_token_ratio': round(type_token_ratio, 3),\n",
        "        'reconstructed_sentences': reconstructed_sentences\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "fe7ae645",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fe7ae645",
        "outputId": "3a6d29c5-6017-4ba7-b4d3-6a7b5f163d2b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Sentences:\n",
            "Hello!\n",
            "I'm Alex.\n",
            "Feel free to drop a message at alex.jordan@gmail.com or visit my blog at www.alexwrites.com.\n",
            "Hope you find it interesting!\n",
            "\n",
            "Reconstructed Sentences:\n",
            "Hello!\n",
            "I' m Alex.\n",
            "Feel free to drop a message at alex.jordan@gmail.com or visit my blog at www.alexwrites.com.\n",
            "Hope you find it interesting!\n",
            "\n",
            "Tokens: ['Hello', '!', 'I', \"'\", 'm', 'Alex', '.', 'Feel', 'free', 'to', 'drop', 'a', 'message', 'at', 'alex.jordan@gmail.com', 'or', 'visit', 'my', 'blog', 'at', 'www.alexwrites.com.', 'Hope', 'you', 'find', 'it', 'interesting', '!']\n",
            "Number of Tokens: 27\n",
            "Total Characters: 118\n",
            "Average Word Length: 3.52\n",
            "Type-Token Ratio: 0.926\n"
          ]
        }
      ],
      "source": [
        "text = \"\"\"Hello! I'm Alex. Feel free to drop a message at alex.jordan@gmail.com or visit my blog at www.alexwrites.com. Hope you find it interesting!\"\"\"\n",
        "\n",
        "stats = english_corpus_statistics(text)\n",
        "\n",
        "print(\"Original Sentences:\")\n",
        "for s in stats['sentences']:\n",
        "    print(s)\n",
        "\n",
        "print(\"\\nReconstructed Sentences:\")\n",
        "for s in stats['reconstructed_sentences']:\n",
        "    print(s)\n",
        "\n",
        "print(\"\\nTokens:\", stats['tokens'])\n",
        "print(\"Number of Tokens:\", stats['num_tokens'])\n",
        "print(\"Total Characters:\", stats['total_characters'])\n",
        "print(\"Average Word Length:\", stats['average_word_length'])\n",
        "print(\"Type-Token Ratio:\", stats['type_token_ratio'])\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}