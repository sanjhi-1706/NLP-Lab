{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Load JSON file\n",
        "with open(\"tokenized_data_final.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# Extract all tokens\n",
        "all_tokens = []\n",
        "for document in data:\n",
        "    for sentence in document.get(\"sentences\", []):\n",
        "        tokens = sentence.get(\"tokens\", [])\n",
        "        all_tokens.extend(tokens)\n",
        "\n",
        "print(f\"Total tokens extracted: {len(all_tokens)}\")\n",
        "print(f\"Sample tokens: {all_tokens[:20]}\")\n",
        "\n",
        "# Save all tokens to a file (space-separated for n-gram modeling)\n",
        "with open(\"all_tokens.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(\" \".join(all_tokens))\n",
        "\n",
        "print(\"Tokens saved to 'all_tokens.txt'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FKwMouwm30gm",
        "outputId": "c1512ad8-ecd4-4c58-c4c6-7c81428da51f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total tokens extracted: 617616\n",
            "Sample tokens: ['‡§≤‡•ã‡§ó‡•ã‡§Ç', '‡§ï‡•ã', '‡§¨‡§ø‡§≤‡•ã‡§Ç', '‡§∏‡§Ç‡§¨‡§Ç‡§ß‡•Ä', '‡§∏‡•Å‡§µ‡§ø‡§ß‡§æ', '‡§¶‡•á‡§®‡§æ', '‡§π‡•Ä', '‡§â‡§®‡§ï‡§æ', '‡§ï‡§æ‡§Æ', '‡§á‡§®‡•á‡§≤‡•ã', '1987', '‡§Æ‡•á‡§Ç', '‡§â‡§∏', '‡§µ‡§ï‡•ç‡§§', '‡§ê‡§∏‡•á', '‡§π‡•Ä', '‡§¶‡•ã‡§∞‡§æ‡§π‡•á', '‡§™‡§∞', '‡§ñ‡§°‡§º‡•Ä', '‡§•‡•Ä']\n",
            "Tokens saved to 'all_tokens.txt'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict, Counter\n",
        "import math\n",
        "\n",
        "# -------------------------------\n",
        "# Step 1: Read tokens from file\n",
        "# -------------------------------\n",
        "with open(\"all_tokens.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    # Assuming each token is separated by whitespace\n",
        "    tokens = f.read().split()\n",
        "\n",
        "print(f\"Total tokens: {len(tokens)}\")\n",
        "print(f\"Sample tokens: {tokens[:20]}\")\n",
        "\n",
        "# -------------------------------\n",
        "# Step 2: Function to build n-gram counts\n",
        "# -------------------------------\n",
        "def build_ngram(tokens, n):\n",
        "    \"\"\"\n",
        "    Build n-gram counts\n",
        "    Returns: dict with n-gram tuple as key and count as value\n",
        "    \"\"\"\n",
        "    ngrams = defaultdict(int)\n",
        "    for i in range(len(tokens) - n + 1):\n",
        "        ngram = tuple(tokens[i:i+n])\n",
        "        ngrams[ngram] += 1\n",
        "    return ngrams\n",
        "\n",
        "# -------------------------------\n",
        "# Step 3: Build Unigram, Bigram, Trigram, Quadrigram\n",
        "# -------------------------------\n",
        "unigrams = build_ngram(tokens, 1)\n",
        "bigrams = build_ngram(tokens, 2)\n",
        "trigrams = build_ngram(tokens, 3)\n",
        "quadrigrams = build_ngram(tokens, 4)\n",
        "\n",
        "print(f\"Unique unigrams: {len(unigrams)}\")\n",
        "print(f\"Unique bigrams: {len(bigrams)}\")\n",
        "print(f\"Unique trigrams: {len(trigrams)}\")\n",
        "print(f\"Unique quadrigrams: {len(quadrigrams)}\")\n",
        "\n",
        "# -------------------------------\n",
        "# Step 4: Convert counts to probabilities\n",
        "# -------------------------------\n",
        "def ngram_probabilities(ngrams, lower_order_counts=None):\n",
        "    \"\"\"\n",
        "    Convert counts to probabilities.\n",
        "    For unigram: lower_order_counts=None\n",
        "    For n>1: use conditional probability P(w_n | w_1,...,w_{n-1})\n",
        "    \"\"\"\n",
        "    probs = {}\n",
        "    if lower_order_counts is None:  # Unigram\n",
        "        total_count = sum(ngrams.values())\n",
        "        for ngram, count in ngrams.items():\n",
        "            probs[ngram] = count / total_count\n",
        "    else:  # Higher order\n",
        "        for ngram, count in ngrams.items():\n",
        "            prefix = ngram[:-1]\n",
        "            probs[ngram] = count / lower_order_counts[prefix]\n",
        "    return probs\n",
        "\n",
        "# Unigram probabilities\n",
        "unigram_probs = ngram_probabilities(unigrams)\n",
        "\n",
        "# Bigram probabilities\n",
        "bigram_probs = ngram_probabilities(bigrams, unigrams)\n",
        "\n",
        "# Trigram probabilities\n",
        "trigram_probs = ngram_probabilities(trigrams, bigrams)\n",
        "\n",
        "# Quadrigram probabilities\n",
        "quadrigram_probs = ngram_probabilities(quadrigrams, trigrams)\n",
        "\n",
        "# -------------------------------\n",
        "# Step 5: Example usage: print top 10 ngrams\n",
        "# -------------------------------\n",
        "def print_top_ngrams(probs, n=10):\n",
        "    sorted_ngrams = sorted(probs.items(), key=lambda x: x[1], reverse=True)\n",
        "    for gram, p in sorted_ngrams[:n]:\n",
        "        print(f\"{' '.join(gram)} : {p:.4f}\")\n",
        "\n",
        "print(\"\\nTop 10 Unigrams:\")\n",
        "print_top_ngrams(unigram_probs)\n",
        "\n",
        "print(\"\\nTop 10 Bigrams:\")\n",
        "print_top_ngrams(bigram_probs)\n",
        "\n",
        "print(\"\\nTop 10 Trigrams:\")\n",
        "print_top_ngrams(trigram_probs)\n",
        "\n",
        "print(\"\\nTop 10 Quadrigrams:\")\n",
        "print_top_ngrams(quadrigram_probs)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lwh9wO8N5EZu",
        "outputId": "7237b77b-9cef-4073-9f9e-c720525ca947"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total tokens: 617616\n",
            "Sample tokens: ['‡§≤‡•ã‡§ó‡•ã‡§Ç', '‡§ï‡•ã', '‡§¨‡§ø‡§≤‡•ã‡§Ç', '‡§∏‡§Ç‡§¨‡§Ç‡§ß‡•Ä', '‡§∏‡•Å‡§µ‡§ø‡§ß‡§æ', '‡§¶‡•á‡§®‡§æ', '‡§π‡•Ä', '‡§â‡§®‡§ï‡§æ', '‡§ï‡§æ‡§Æ', '‡§á‡§®‡•á‡§≤‡•ã', '1987', '‡§Æ‡•á‡§Ç', '‡§â‡§∏', '‡§µ‡§ï‡•ç‡§§', '‡§ê‡§∏‡•á', '‡§π‡•Ä', '‡§¶‡•ã‡§∞‡§æ‡§π‡•á', '‡§™‡§∞', '‡§ñ‡§°‡§º‡•Ä', '‡§•‡•Ä']\n",
            "Unique unigrams: 41043\n",
            "Unique bigrams: 284097\n",
            "Unique trigrams: 503313\n",
            "Unique quadrigrams: 584957\n",
            "\n",
            "Top 10 Unigrams:\n",
            "‡§ï‡•á : 0.0388\n",
            "‡§Æ‡•á‡§Ç : 0.0300\n",
            "‡§ï‡•Ä : 0.0237\n",
            ", : 0.0215\n",
            "‡§ï‡•ã : 0.0179\n",
            "‡§∏‡•á : 0.0171\n",
            "‡§®‡•á : 0.0135\n",
            ". : 0.0131\n",
            "‡§π‡•à : 0.0127\n",
            "‡§ï‡§æ : 0.0127\n",
            "\n",
            "Top 10 Bigrams:\n",
            "‡§¶‡•ã‡§∞‡§æ‡§π‡•á ‡§™‡§∞ : 1.0000\n",
            "‡§ö‡•å . : 1.0000\n",
            "‡§¢‡§º‡•Ä‡§≤‡•Ä ‡§π‡•ã : 1.0000\n",
            "‡§ö‡•à‡§Æ‡•ç‡§™‡§ø‡§Ø‡§Ç‡§∏ ‡§ü‡•ç‡§∞‡§æ‡§´‡•Ä : 1.0000\n",
            "‡§Ö‡§°‡§ø‡§ó ‡§∞‡§π‡§®‡§æ : 1.0000\n",
            "‡§™‡•à‡§ü‡•ç‡§∞‡•Ä‡§ï‡§ø‡§Ø‡•ã ‡§∞‡•ã‡§∏‡•á‡§Ç‡§°‡•á : 1.0000\n",
            "‡§∞‡•ã‡§∏‡•á‡§Ç‡§°‡•á ‡§®‡•á : 1.0000\n",
            "‡§Æ‡•Å‡§∞‡•ç‡§¶‡§æ‡§¨‡§æ‡§¶ ‡§ï‡•á : 1.0000\n",
            "‡§¶‡•á‡§µ‡§æ‡§Ç‡§ó‡§® ‡§∏‡•á : 1.0000\n",
            "‡§™‡•à‡§°‡§≤‡§∞ ‡§¶‡•ç‡§µ‡§æ‡§∞‡§æ : 1.0000\n",
            "\n",
            "Top 10 Trigrams:\n",
            "‡§ï‡•ã ‡§¨‡§ø‡§≤‡•ã‡§Ç ‡§∏‡§Ç‡§¨‡§Ç‡§ß‡•Ä : 1.0000\n",
            "‡§¨‡§ø‡§≤‡•ã‡§Ç ‡§∏‡§Ç‡§¨‡§Ç‡§ß‡•Ä ‡§∏‡•Å‡§µ‡§ø‡§ß‡§æ : 1.0000\n",
            "‡§∏‡§Ç‡§¨‡§Ç‡§ß‡•Ä ‡§∏‡•Å‡§µ‡§ø‡§ß‡§æ ‡§¶‡•á‡§®‡§æ : 1.0000\n",
            "‡§∏‡•Å‡§µ‡§ø‡§ß‡§æ ‡§¶‡•á‡§®‡§æ ‡§π‡•Ä : 1.0000\n",
            "‡§¶‡•á‡§®‡§æ ‡§π‡•Ä ‡§â‡§®‡§ï‡§æ : 1.0000\n",
            "‡§ï‡§æ‡§Æ ‡§á‡§®‡•á‡§≤‡•ã 1987 : 1.0000\n",
            "‡§á‡§®‡•á‡§≤‡•ã 1987 ‡§Æ‡•á‡§Ç : 1.0000\n",
            "‡§µ‡§ï‡•ç‡§§ ‡§ê‡§∏‡•á ‡§π‡•Ä : 1.0000\n",
            "‡§π‡•Ä ‡§¶‡•ã‡§∞‡§æ‡§π‡•á ‡§™‡§∞ : 1.0000\n",
            "‡§¶‡•ã‡§∞‡§æ‡§π‡•á ‡§™‡§∞ ‡§ñ‡§°‡§º‡•Ä : 1.0000\n",
            "\n",
            "Top 10 Quadrigrams:\n",
            "‡§≤‡•ã‡§ó‡•ã‡§Ç ‡§ï‡•ã ‡§¨‡§ø‡§≤‡•ã‡§Ç ‡§∏‡§Ç‡§¨‡§Ç‡§ß‡•Ä : 1.0000\n",
            "‡§ï‡•ã ‡§¨‡§ø‡§≤‡•ã‡§Ç ‡§∏‡§Ç‡§¨‡§Ç‡§ß‡•Ä ‡§∏‡•Å‡§µ‡§ø‡§ß‡§æ : 1.0000\n",
            "‡§¨‡§ø‡§≤‡•ã‡§Ç ‡§∏‡§Ç‡§¨‡§Ç‡§ß‡•Ä ‡§∏‡•Å‡§µ‡§ø‡§ß‡§æ ‡§¶‡•á‡§®‡§æ : 1.0000\n",
            "‡§∏‡§Ç‡§¨‡§Ç‡§ß‡•Ä ‡§∏‡•Å‡§µ‡§ø‡§ß‡§æ ‡§¶‡•á‡§®‡§æ ‡§π‡•Ä : 1.0000\n",
            "‡§∏‡•Å‡§µ‡§ø‡§ß‡§æ ‡§¶‡•á‡§®‡§æ ‡§π‡•Ä ‡§â‡§®‡§ï‡§æ : 1.0000\n",
            "‡§¶‡•á‡§®‡§æ ‡§π‡•Ä ‡§â‡§®‡§ï‡§æ ‡§ï‡§æ‡§Æ : 1.0000\n",
            "‡§π‡•Ä ‡§â‡§®‡§ï‡§æ ‡§ï‡§æ‡§Æ ‡§á‡§®‡•á‡§≤‡•ã : 1.0000\n",
            "‡§â‡§®‡§ï‡§æ ‡§ï‡§æ‡§Æ ‡§á‡§®‡•á‡§≤‡•ã 1987 : 1.0000\n",
            "‡§ï‡§æ‡§Æ ‡§á‡§®‡•á‡§≤‡•ã 1987 ‡§Æ‡•á‡§Ç : 1.0000\n",
            "‡§á‡§®‡•á‡§≤‡•ã 1987 ‡§Æ‡•á‡§Ç ‡§â‡§∏ : 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "def build_ngram_counts(tokens, n):\n",
        "    counts = defaultdict(int)\n",
        "    for i in range(len(tokens)-n+1):\n",
        "        ngram = tuple(tokens[i:i+n])\n",
        "        counts[ngram] += 1\n",
        "    return counts\n",
        "\n",
        "# Example for bigrams\n",
        "tokens = open(\"all_tokens.txt\", \"r\", encoding=\"utf-8\").read().split()\n",
        "\n",
        "unigrams = build_ngram_counts(tokens, 1)\n",
        "bigrams = build_ngram_counts(tokens, 2)\n",
        "\n",
        "vocab_size = len(unigrams)  # number of unique tokens\n",
        "total_tokens = len(tokens)\n"
      ],
      "metadata": {
        "id": "kvMUvFns540C"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_one_smoothing(ngram_counts, lower_counts, vocab_size):\n",
        "    probs = {}\n",
        "    for ngram, count in ngram_counts.items():\n",
        "        prefix = ngram[:-1]\n",
        "        probs[ngram] = (count + 1) / (lower_counts[prefix] + vocab_size)\n",
        "    return probs\n",
        "\n",
        "# Bigram probabilities with Add-One smoothing\n",
        "bigram_probs_add1 = add_one_smoothing(bigrams, unigrams, vocab_size)\n"
      ],
      "metadata": {
        "id": "rKHoEaFKmItL"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_k_smoothing(ngram_counts, lower_counts, vocab_size, k=0.5):\n",
        "    probs = {}\n",
        "    for ngram, count in ngram_counts.items():\n",
        "        prefix = ngram[:-1]\n",
        "        probs[ngram] = (count + k) / (lower_counts[prefix] + k * vocab_size)\n",
        "    return probs\n",
        "\n",
        "# Example: bigram with Add-K smoothing, k=0.5\n",
        "bigram_probs_addk = add_k_smoothing(bigrams, unigrams, vocab_size, k=0.5)\n"
      ],
      "metadata": {
        "id": "HWoB-L5rmLAq"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_token_type_smoothing(ngram_counts, lower_counts):\n",
        "    # Get number of unique continuations for each prefix\n",
        "    prefix_types = defaultdict(set)\n",
        "    for ngram in ngram_counts:\n",
        "        prefix = ngram[:-1]\n",
        "        prefix_types[prefix].add(ngram[-1])\n",
        "\n",
        "    probs = {}\n",
        "    for ngram, count in ngram_counts.items():\n",
        "        prefix = ngram[:-1]\n",
        "        num_types = len(prefix_types[prefix])\n",
        "        probs[ngram] = (count + 1) / num_types\n",
        "    return probs\n",
        "\n",
        "# Example: bigram with token-type smoothing\n",
        "bigram_probs_token_type = add_token_type_smoothing(bigrams, unigrams)\n"
      ],
      "metadata": {
        "id": "EjBl6d3hmOEH"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_top_probs(probs, n=10):\n",
        "    sorted_probs = sorted(probs.items(), key=lambda x: x[1], reverse=True)\n",
        "    for gram, p in sorted_probs[:n]:\n",
        "        print(f\"{' '.join(gram)} : {p:.4f}\")\n",
        "\n",
        "print(\"Top 10 bigrams with Add-One smoothing:\")\n",
        "print_top_probs(bigram_probs_add1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-z7djjfs6Mhi",
        "outputId": "6ed0877d-3c3f-4588-9b04-f59f2ff7a595"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 10 bigrams with Add-One smoothing:\n",
            "‡§ï‡•á ‡§≤‡§ø‡§è : 0.0530\n",
            "‡§π‡•à . : 0.0397\n",
            "‡§π‡•à ‡§ï‡§ø : 0.0359\n",
            "‡§ï‡§π‡§æ ‡§ï‡§ø : 0.0296\n",
            "‡§π‡•à , : 0.0275\n",
            "‡§ï‡•á ‡§∏‡§æ‡§• : 0.0216\n",
            "‡§®‡•á ‡§ï‡§π‡§æ : 0.0197\n",
            "‡§ï‡•á ‡§¨‡§æ‡§¶ : 0.0184\n",
            ". . : 0.0173\n",
            "‡§π‡•à‡§Ç . : 0.0164\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json, random\n",
        "\n",
        "# Load tokenized JSON\n",
        "with open(\"tokenized_data_final.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# Extract all sentences (tokens)\n",
        "all_sentences = []\n",
        "for document in data:\n",
        "    for sent in document.get(\"sentences\", []):\n",
        "        tokens = sent.get(\"tokens\", [])\n",
        "        if tokens:  # only keep non-empty\n",
        "            all_sentences.append(tokens)\n",
        "\n",
        "print(f\"Total sentences: {len(all_sentences)}\")\n",
        "\n",
        "# Randomly select 1000 (or all if fewer than 1000)\n",
        "sample_sentences = random.sample(all_sentences, min(1000, len(all_sentences)))\n",
        "print(f\"Selected {len(sample_sentences)} sentences\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ScvjMD1D6l_U",
        "outputId": "b7521a7d-6c44-4d13-8a4d-3fd4c10e7e5e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total sentences: 35391\n",
            "Selected 1000 sentences\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json, random\n",
        "from collections import defaultdict, Counter\n",
        "import math\n",
        "\n",
        "# ----------------------------\n",
        "# Load tokenized sentences\n",
        "# ----------------------------\n",
        "with open(\"tokenized_data_final.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "all_sentences = []\n",
        "for document in data:\n",
        "    for sent in document.get(\"sentences\", []):\n",
        "        tokens = sent.get(\"tokens\", [])\n",
        "        if tokens:\n",
        "            all_sentences.append(tokens)\n",
        "\n",
        "sample_sentences = random.sample(all_sentences, min(1000, len(all_sentences)))\n",
        "print(f\"Selected {len(sample_sentences)} sentences\")\n",
        "\n",
        "# ----------------------------\n",
        "# Build n-gram models\n",
        "# ----------------------------\n",
        "def build_ngram_counts(sentences, n):\n",
        "    counts = defaultdict(Counter)\n",
        "    for sent in sentences:\n",
        "        sent = [\"<s>\"]*(n-1) + sent + [\"</s>\"]\n",
        "        for i in range(len(sent)-n+1):\n",
        "            context = tuple(sent[i:i+n-1])\n",
        "            token = sent[i+n-1]\n",
        "            counts[context][token] += 1\n",
        "    return counts\n",
        "\n",
        "unigram_counts = build_ngram_counts(all_sentences, 1)\n",
        "bigram_counts = build_ngram_counts(all_sentences, 2)\n",
        "trigram_counts = build_ngram_counts(all_sentences, 3)\n",
        "quadrigram_counts = build_ngram_counts(all_sentences, 4)\n",
        "\n",
        "# Vocabulary\n",
        "vocab = set()\n",
        "for sent in all_sentences:\n",
        "    vocab.update(sent)\n",
        "V = len(vocab)\n",
        "\n",
        "# ----------------------------\n",
        "# Smoothed probability functions\n",
        "# ----------------------------\n",
        "def unigram_prob(token, k=1):\n",
        "    total = sum(unigram_counts[()][t] for t in unigram_counts[()][t])\n",
        "    total = sum(unigram_counts[()][t] for t in unigram_counts[()])\n",
        "    count = unigram_counts[()][token]\n",
        "    return (count + k) / (total + k*V)\n",
        "\n",
        "def ngram_prob(context, token, ngram_counts, k=1):\n",
        "    context_count = sum(ngram_counts[context].values())\n",
        "    token_count = ngram_counts[context][token]\n",
        "    return (token_count + k) / (context_count + k*V)\n",
        "\n",
        "# ----------------------------\n",
        "# Compute sentence probabilities\n",
        "# ----------------------------\n",
        "def sentence_log_prob(sentence, n, ngram_counts, k=1):\n",
        "    sent = [\"<s>\"]*(n-1) + sentence + [\"</s>\"]\n",
        "    log_prob = 0.0\n",
        "    for i in range(n-1, len(sent)):\n",
        "        context = tuple(sent[i-n+1:i])\n",
        "        token = sent[i]\n",
        "        prob = ngram_prob(context, token, ngram_counts, k)\n",
        "        log_prob += math.log(prob)\n",
        "    return log_prob\n",
        "\n",
        "# ----------------------------\n",
        "# Compute for sample sentences\n",
        "# ----------------------------\n",
        "results = []\n",
        "\n",
        "for sent in sample_sentences:\n",
        "    unigram_lp = sentence_log_prob(sent, 1, unigram_counts, k=1)\n",
        "    bigram_lp = sentence_log_prob(sent, 2, bigram_counts, k=1)\n",
        "    trigram_lp = sentence_log_prob(sent, 3, trigram_counts, k=1)\n",
        "    quadrigram_lp = sentence_log_prob(sent, 4, quadrigram_counts, k=1)\n",
        "\n",
        "    results.append({\n",
        "        \"sentence\": \" \".join(sent),\n",
        "        \"Unigram_logP\": unigram_lp,\n",
        "        \"Bigram_logP\": bigram_lp,\n",
        "        \"Trigram_logP\": trigram_lp,\n",
        "        \"Quadrigram_logP\": quadrigram_lp\n",
        "    })\n",
        "\n",
        "# Optional: save results to JSON\n",
        "with open(\"sentence_probs.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(results, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(\"Probability computation done for all sampled sentences.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S0096-hU8WmQ",
        "outputId": "778d6b21-5900-4ccb-d248-c2cac70aedbf"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected 1000 sentences\n",
            "Probability computation done for all sampled sentences.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "def build_ngram_counts(tokens, n):\n",
        "    counts = defaultdict(int)\n",
        "    for i in range(len(tokens)-n+1):\n",
        "        ngram = tuple(tokens[i:i+n])\n",
        "        counts[ngram] += 1\n",
        "    return counts\n",
        "\n",
        "# Flatten all tokens to build models\n",
        "all_tokens = [tok for sent in all_sentences for tok in sent]\n",
        "\n",
        "unigrams = build_ngram_counts(all_tokens, 1)\n",
        "bigrams = build_ngram_counts(all_tokens, 2)\n",
        "trigrams = build_ngram_counts(all_tokens, 3)\n",
        "quadrigrams = build_ngram_counts(all_tokens, 4)\n",
        "\n",
        "vocab_size = len(unigrams)\n"
      ],
      "metadata": {
        "id": "gkOiy6wq6l82"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_one_prob(ngram, ngram_counts, lower_counts, vocab_size):\n",
        "    prefix = ngram[:-1]\n",
        "    return (ngram_counts.get(ngram, 0) + 1) / (lower_counts.get(prefix, 0) + vocab_size)\n",
        "\n",
        "def add_k_prob(ngram, ngram_counts, lower_counts, vocab_size, k=0.5):\n",
        "    prefix = ngram[:-1]\n",
        "    return (ngram_counts.get(ngram, 0) + k) / (lower_counts.get(prefix, 0) + k * vocab_size)\n",
        "\n",
        "def add_token_type_prob(ngram, ngram_counts):\n",
        "    prefix = ngram[:-1]\n",
        "    continuations = [g[-1] for g in ngram_counts if g[:-1] == prefix]\n",
        "    num_types = len(set(continuations)) if continuations else 1\n",
        "    return (ngram_counts.get(ngram, 0) + 1) / num_types\n"
      ],
      "metadata": {
        "id": "Ka6JUG536l6F"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "def sentence_probability(sentence, method=\"add_one\", k=0.5):\n",
        "    prob_log = 0.0\n",
        "    for i in range(len(sentence)-1):\n",
        "        bigram = (sentence[i], sentence[i+1])\n",
        "        unigram = (sentence[i],)\n",
        "\n",
        "        if method == \"add_one\":\n",
        "            p = add_one_prob(bigram, bigrams, unigrams, vocab_size)\n",
        "        elif method == \"add_k\":\n",
        "            p = add_k_prob(bigram, bigrams, unigrams, vocab_size, k)\n",
        "        elif method == \"token_type\":\n",
        "            p = add_token_type_prob(bigram, bigrams)\n",
        "        else:\n",
        "            raise ValueError(\"Unknown method\")\n",
        "\n",
        "        prob_log += math.log(p + 1e-12)  # avoid log(0)\n",
        "\n",
        "    return math.exp(prob_log)  # sentence probability\n"
      ],
      "metadata": {
        "id": "HVU79RI06l38"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame(results)\n",
        "df.to_csv(\"sentence_probabilities.csv\", index=False, encoding=\"utf-8\")\n",
        "print(\"Saved sentence probabilities to sentence_probabilities.csv\")\n"
      ],
      "metadata": {
        "id": "hnbTyvGN6yAX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "# Precompute continuations for token-type smoothing\n",
        "prefix_continuations = defaultdict(set)\n",
        "for (w1, w2), count in bigrams.items():\n",
        "    prefix_continuations[(w1,)].add(w2)\n",
        "\n",
        "def add_token_type_prob(ngram, ngram_counts, prefix_continuations):\n",
        "    prefix = ngram[:-1]\n",
        "    num_types = len(prefix_continuations.get(prefix, [])) or 1\n",
        "    return (ngram_counts.get(ngram, 0) + 1) / num_types\n"
      ],
      "metadata": {
        "id": "V6NDZTTV7S4I"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prob_cache = {}\n",
        "\n",
        "def sentence_probability(sentence, method=\"add_one\", k=0.5):\n",
        "    prob_log = 0.0\n",
        "    for i in range(len(sentence)-1):\n",
        "        bigram = (sentence[i], sentence[i+1])\n",
        "\n",
        "        if (bigram, method) in prob_cache:\n",
        "            p = prob_cache[(bigram, method)]\n",
        "        else:\n",
        "            if method == \"add_one\":\n",
        "                p = add_one_prob(bigram, bigrams, unigrams, vocab_size)\n",
        "            elif method == \"add_k\":\n",
        "                p = add_k_prob(bigram, bigrams, unigrams, vocab_size, k)\n",
        "            elif method == \"token_type\":\n",
        "                p = add_token_type_prob(bigram, bigrams, prefix_continuations)\n",
        "            prob_cache[(bigram, method)] = p\n",
        "\n",
        "        prob_log += math.log(p + 1e-12)\n",
        "\n",
        "    return math.exp(prob_log)\n"
      ],
      "metadata": {
        "id": "MYI0PSSh7S1m"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sentence_log_probability(sentence, method=\"add_one\", k=0.5):\n",
        "    prob_log = 0.0\n",
        "    for i in range(len(sentence)-1):\n",
        "        bigram = (sentence[i], sentence[i+1])\n",
        "\n",
        "        if (bigram, method) in prob_cache:\n",
        "            p = prob_cache[(bigram, method)]\n",
        "        else:\n",
        "            if method == \"add_one\":\n",
        "                p = add_one_prob(bigram, bigrams, unigrams, vocab_size)\n",
        "            elif method == \"add_k\":\n",
        "                p = add_k_prob(bigram, bigrams, unigrams, vocab_size, k)\n",
        "            elif method == \"token_type\":\n",
        "                p = add_token_type_prob(bigram, bigrams, prefix_continuations)\n",
        "            prob_cache[(bigram, method)] = p\n",
        "\n",
        "        prob_log += math.log(p + 1e-12)\n",
        "    return prob_log\n"
      ],
      "metadata": {
        "id": "7yxHdVk-7SzW"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = []\n",
        "for sent in sample_sentences[:20]:\n",
        "    length = len(sent) or 1\n",
        "\n",
        "    log_add1 = sentence_log_probability(sent, \"add_one\")\n",
        "    log_addk = sentence_log_probability(sent, \"add_k\", k=0.5)\n",
        "    log_tt = sentence_log_probability(sent, \"token_type\")\n",
        "\n",
        "    results.append({\n",
        "        \"Sentence\": \" \".join(sent),\n",
        "        \"Length\": length,\n",
        "        \"Add-One (avg log P)\": log_add1 / length,\n",
        "        \"Add-K=0.5 (avg log P)\": log_addk / length,\n",
        "        \"Token-Type (avg log P)\": log_tt / length\n",
        "    })\n"
      ],
      "metadata": {
        "id": "fOeK3PD37aAx"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for row in results[:5]:\n",
        "    print(\"üìå Sentence:\", row[\"Sentence\"])\n",
        "    print(f\"   üîπ Add-One log P: {row['Add-One (log P)']:.2f}\")\n",
        "    print(f\"   üîπ Add-K=0.5 log P: {row['Add-K=0.5 (log P)']:.2f}\")\n",
        "    print(f\"   üîπ Token-Type log P: {row['Token-Type (log P)']:.2f}\")\n",
        "\n",
        "    best = max(\n",
        "        [(\"Add-One\", row['Add-One (log P)']),\n",
        "         (\"Add-K\", row['Add-K=0.5 (log P)']),\n",
        "         (\"Token-Type\", row['Token-Type (log P)'])],\n",
        "        key=lambda x: x[1]\n",
        "    )\n",
        "    print(f\"   ‚úÖ Most likely under: {best[0]}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pX1Ylq1t7mFu",
        "outputId": "cb2189f3-7fdd-4213-8385-8dcfd151bc43"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìå Sentence: ‡§¨‡§æ‡§Ç‡§¶‡•á ‡§•‡§æ‡§®‡§æ ‡§Æ‡•á‡§Ç ‡§™‡§¶‡§∏‡•ç‡§• ‡§è‡§è‡§∏‡§Ü‡§à ‡§∂‡§ø‡§µ ‡§ï‡•Å‡§Æ‡§æ‡§∞ ‡§Æ‡§Ç‡§°‡§æ‡§µ‡•Ä ‡§ï‡•ã ‡§¶‡§ø‡§® ‡§¶‡§π‡§æ‡•ú‡•á ‡§¨‡§æ‡§ú‡§æ‡§∞ ‡§Æ‡•á‡§Ç ‡§ó‡•ã‡§≤‡•Ä ‡§Æ‡§æ‡§∞‡•Ä‡•§\n",
            "   üîπ Add-One log P: -130.64\n",
            "   üîπ Add-K=0.5 log P: -124.47\n",
            "   üîπ Token-Type log P: -35.82\n",
            "   ‚úÖ Most likely under: Token-Type\n",
            "\n",
            "üìå Sentence: ‡§°‡•â .\n",
            "   üîπ Add-One log P: -5.55\n",
            "   üîπ Add-K=0.5 log P: -4.86\n",
            "   üîπ Token-Type log P: 1.50\n",
            "   ‚úÖ Most likely under: Token-Type\n",
            "\n",
            "üìå Sentence: ‡§Ü‡•Ö‡§∏‡•ç‡§ü‡•ç‡§∞‡•á‡§≤‡§ø‡§Ø‡§æ‡§à ‡§ï‡•ç‡§∞‡§ø‡§ï‡•á‡§ü ‡§ü‡•Ä‡§Æ ‡§ï‡•á ‡§™‡•Ç‡§∞‡•ç‡§µ ‡§ï‡§™‡•ç‡§§‡§æ‡§® ‡§∏‡•ç‡§ü‡•Ä‡§µ ‡§∏‡•ç‡§Æ‡§ø‡§• ‡§®‡•á ‡§ó‡•á‡§Ç‡§¶ ‡§∏‡•á ‡§õ‡•á‡§°‡§º‡§õ‡§æ‡§°‡§º ‡§ï‡•á ‡§Æ‡§æ‡§Æ‡§≤‡•á ‡§Æ‡•á‡§Ç ‡§∏‡§æ‡§∞‡•ç‡§µ‡§ú‡§®‡§ø‡§ï ‡§§‡•å‡§∞ ‡§™‡§∞ ‡§Æ‡§æ‡§´‡•Ä ‡§Æ‡§æ‡§Ç‡§ó‡•Ä ‡§π‡•à .\n",
            "   üîπ Add-One log P: -171.54\n",
            "   üîπ Add-K=0.5 log P: -160.88\n",
            "   üîπ Token-Type log P: -43.43\n",
            "   ‚úÖ Most likely under: Token-Type\n",
            "\n",
            "üìå Sentence: ‡§π‡§ø‡§Ç‡§¶‡•Ä ‡§®‡•ç‡§Ø‡•Ç‡•õ ‡§¨‡§ø‡§π‡§æ‡§∞ ‡§∏‡§π‡§∞‡§∏‡§æ ‡§Ö‡§≠‡§ø‡§Ø‡§Ç‡§§‡§æ ‡§¶‡§ø‡§µ‡§∏ ‡§™‡§∞ ‡§Ø‡§æ‡§¶ ‡§ï‡§ø‡§è ‡§ó‡§è ‡§°‡•â .\n",
            "   üîπ Add-One log P: -89.36\n",
            "   üîπ Add-K=0.5 log P: -83.32\n",
            "   üîπ Token-Type log P: -15.07\n",
            "   ‚úÖ Most likely under: Token-Type\n",
            "\n",
            "üìå Sentence: ‡§Æ‡§°‡§º‡§ø‡§Ø‡§æ‡§π‡•Ç‡§Ç ‡§ï‡•ã‡§§‡§µ‡§æ‡§≤‡•Ä ‡§ï‡•ç‡§∑‡•á‡§§‡•ç‡§∞ ‡§ï‡•á ‡§ö‡•ã‡§∞‡§æ‡§∞‡•Ä ‡§ó‡§æ‡§Ç‡§µ ‡§®‡§ø‡§µ‡§æ‡§∏‡•Ä ‡§°‡•â .\n",
            "   üîπ Add-One log P: -65.69\n",
            "   üîπ Add-K=0.5 log P: -61.52\n",
            "   üîπ Token-Type log P: -9.08\n",
            "   ‚úÖ Most likely under: Token-Type\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EU5ETKXI7rDB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}