{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Load JSON file\n",
        "with open(\"tokenized_data_final.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# Extract all tokens\n",
        "all_tokens = []\n",
        "for document in data:\n",
        "    for sentence in document.get(\"sentences\", []):\n",
        "        tokens = sentence.get(\"tokens\", [])\n",
        "        all_tokens.extend(tokens)\n",
        "\n",
        "print(f\"Total tokens extracted: {len(all_tokens)}\")\n",
        "print(f\"Sample tokens: {all_tokens[:20]}\")\n",
        "\n",
        "# Save all tokens to a file (space-separated for n-gram modeling)\n",
        "with open(\"all_tokens.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(\" \".join(all_tokens))\n",
        "\n",
        "print(\"Tokens saved to 'all_tokens.txt'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FKwMouwm30gm",
        "outputId": "c1512ad8-ecd4-4c58-c4c6-7c81428da51f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total tokens extracted: 617616\n",
            "Sample tokens: ['लोगों', 'को', 'बिलों', 'संबंधी', 'सुविधा', 'देना', 'ही', 'उनका', 'काम', 'इनेलो', '1987', 'में', 'उस', 'वक्त', 'ऐसे', 'ही', 'दोराहे', 'पर', 'खड़ी', 'थी']\n",
            "Tokens saved to 'all_tokens.txt'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict, Counter\n",
        "import math\n",
        "\n",
        "# -------------------------------\n",
        "# Step 1: Read tokens from file\n",
        "# -------------------------------\n",
        "with open(\"all_tokens.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    # Assuming each token is separated by whitespace\n",
        "    tokens = f.read().split()\n",
        "\n",
        "print(f\"Total tokens: {len(tokens)}\")\n",
        "print(f\"Sample tokens: {tokens[:20]}\")\n",
        "\n",
        "# -------------------------------\n",
        "# Step 2: Function to build n-gram counts\n",
        "# -------------------------------\n",
        "def build_ngram(tokens, n):\n",
        "    \"\"\"\n",
        "    Build n-gram counts\n",
        "    Returns: dict with n-gram tuple as key and count as value\n",
        "    \"\"\"\n",
        "    ngrams = defaultdict(int)\n",
        "    for i in range(len(tokens) - n + 1):\n",
        "        ngram = tuple(tokens[i:i+n])\n",
        "        ngrams[ngram] += 1\n",
        "    return ngrams\n",
        "\n",
        "# -------------------------------\n",
        "# Step 3: Build Unigram, Bigram, Trigram, Quadrigram\n",
        "# -------------------------------\n",
        "unigrams = build_ngram(tokens, 1)\n",
        "bigrams = build_ngram(tokens, 2)\n",
        "trigrams = build_ngram(tokens, 3)\n",
        "quadrigrams = build_ngram(tokens, 4)\n",
        "\n",
        "print(f\"Unique unigrams: {len(unigrams)}\")\n",
        "print(f\"Unique bigrams: {len(bigrams)}\")\n",
        "print(f\"Unique trigrams: {len(trigrams)}\")\n",
        "print(f\"Unique quadrigrams: {len(quadrigrams)}\")\n",
        "\n",
        "# -------------------------------\n",
        "# Step 4: Convert counts to probabilities\n",
        "# -------------------------------\n",
        "def ngram_probabilities(ngrams, lower_order_counts=None):\n",
        "    \"\"\"\n",
        "    Convert counts to probabilities.\n",
        "    For unigram: lower_order_counts=None\n",
        "    For n>1: use conditional probability P(w_n | w_1,...,w_{n-1})\n",
        "    \"\"\"\n",
        "    probs = {}\n",
        "    if lower_order_counts is None:  # Unigram\n",
        "        total_count = sum(ngrams.values())\n",
        "        for ngram, count in ngrams.items():\n",
        "            probs[ngram] = count / total_count\n",
        "    else:  # Higher order\n",
        "        for ngram, count in ngrams.items():\n",
        "            prefix = ngram[:-1]\n",
        "            probs[ngram] = count / lower_order_counts[prefix]\n",
        "    return probs\n",
        "\n",
        "# Unigram probabilities\n",
        "unigram_probs = ngram_probabilities(unigrams)\n",
        "\n",
        "# Bigram probabilities\n",
        "bigram_probs = ngram_probabilities(bigrams, unigrams)\n",
        "\n",
        "# Trigram probabilities\n",
        "trigram_probs = ngram_probabilities(trigrams, bigrams)\n",
        "\n",
        "# Quadrigram probabilities\n",
        "quadrigram_probs = ngram_probabilities(quadrigrams, trigrams)\n",
        "\n",
        "# -------------------------------\n",
        "# Step 5: Example usage: print top 10 ngrams\n",
        "# -------------------------------\n",
        "def print_top_ngrams(probs, n=10):\n",
        "    sorted_ngrams = sorted(probs.items(), key=lambda x: x[1], reverse=True)\n",
        "    for gram, p in sorted_ngrams[:n]:\n",
        "        print(f\"{' '.join(gram)} : {p:.4f}\")\n",
        "\n",
        "print(\"\\nTop 10 Unigrams:\")\n",
        "print_top_ngrams(unigram_probs)\n",
        "\n",
        "print(\"\\nTop 10 Bigrams:\")\n",
        "print_top_ngrams(bigram_probs)\n",
        "\n",
        "print(\"\\nTop 10 Trigrams:\")\n",
        "print_top_ngrams(trigram_probs)\n",
        "\n",
        "print(\"\\nTop 10 Quadrigrams:\")\n",
        "print_top_ngrams(quadrigram_probs)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lwh9wO8N5EZu",
        "outputId": "7237b77b-9cef-4073-9f9e-c720525ca947"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total tokens: 617616\n",
            "Sample tokens: ['लोगों', 'को', 'बिलों', 'संबंधी', 'सुविधा', 'देना', 'ही', 'उनका', 'काम', 'इनेलो', '1987', 'में', 'उस', 'वक्त', 'ऐसे', 'ही', 'दोराहे', 'पर', 'खड़ी', 'थी']\n",
            "Unique unigrams: 41043\n",
            "Unique bigrams: 284097\n",
            "Unique trigrams: 503313\n",
            "Unique quadrigrams: 584957\n",
            "\n",
            "Top 10 Unigrams:\n",
            "के : 0.0388\n",
            "में : 0.0300\n",
            "की : 0.0237\n",
            ", : 0.0215\n",
            "को : 0.0179\n",
            "से : 0.0171\n",
            "ने : 0.0135\n",
            ". : 0.0131\n",
            "है : 0.0127\n",
            "का : 0.0127\n",
            "\n",
            "Top 10 Bigrams:\n",
            "दोराहे पर : 1.0000\n",
            "चौ . : 1.0000\n",
            "ढ़ीली हो : 1.0000\n",
            "चैम्पियंस ट्राफी : 1.0000\n",
            "अडिग रहना : 1.0000\n",
            "पैट्रीकियो रोसेंडे : 1.0000\n",
            "रोसेंडे ने : 1.0000\n",
            "मुर्दाबाद के : 1.0000\n",
            "देवांगन से : 1.0000\n",
            "पैडलर द्वारा : 1.0000\n",
            "\n",
            "Top 10 Trigrams:\n",
            "को बिलों संबंधी : 1.0000\n",
            "बिलों संबंधी सुविधा : 1.0000\n",
            "संबंधी सुविधा देना : 1.0000\n",
            "सुविधा देना ही : 1.0000\n",
            "देना ही उनका : 1.0000\n",
            "काम इनेलो 1987 : 1.0000\n",
            "इनेलो 1987 में : 1.0000\n",
            "वक्त ऐसे ही : 1.0000\n",
            "ही दोराहे पर : 1.0000\n",
            "दोराहे पर खड़ी : 1.0000\n",
            "\n",
            "Top 10 Quadrigrams:\n",
            "लोगों को बिलों संबंधी : 1.0000\n",
            "को बिलों संबंधी सुविधा : 1.0000\n",
            "बिलों संबंधी सुविधा देना : 1.0000\n",
            "संबंधी सुविधा देना ही : 1.0000\n",
            "सुविधा देना ही उनका : 1.0000\n",
            "देना ही उनका काम : 1.0000\n",
            "ही उनका काम इनेलो : 1.0000\n",
            "उनका काम इनेलो 1987 : 1.0000\n",
            "काम इनेलो 1987 में : 1.0000\n",
            "इनेलो 1987 में उस : 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "def build_ngram_counts(tokens, n):\n",
        "    counts = defaultdict(int)\n",
        "    for i in range(len(tokens)-n+1):\n",
        "        ngram = tuple(tokens[i:i+n])\n",
        "        counts[ngram] += 1\n",
        "    return counts\n",
        "\n",
        "# Example for bigrams\n",
        "tokens = open(\"all_tokens.txt\", \"r\", encoding=\"utf-8\").read().split()\n",
        "\n",
        "unigrams = build_ngram_counts(tokens, 1)\n",
        "bigrams = build_ngram_counts(tokens, 2)\n",
        "\n",
        "vocab_size = len(unigrams)  # number of unique tokens\n",
        "total_tokens = len(tokens)\n"
      ],
      "metadata": {
        "id": "kvMUvFns540C"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_one_smoothing(ngram_counts, lower_counts, vocab_size):\n",
        "    probs = {}\n",
        "    for ngram, count in ngram_counts.items():\n",
        "        prefix = ngram[:-1]\n",
        "        probs[ngram] = (count + 1) / (lower_counts[prefix] + vocab_size)\n",
        "    return probs\n",
        "\n",
        "# Bigram probabilities with Add-One smoothing\n",
        "bigram_probs_add1 = add_one_smoothing(bigrams, unigrams, vocab_size)\n"
      ],
      "metadata": {
        "id": "rKHoEaFKmItL"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_k_smoothing(ngram_counts, lower_counts, vocab_size, k=0.5):\n",
        "    probs = {}\n",
        "    for ngram, count in ngram_counts.items():\n",
        "        prefix = ngram[:-1]\n",
        "        probs[ngram] = (count + k) / (lower_counts[prefix] + k * vocab_size)\n",
        "    return probs\n",
        "\n",
        "# Example: bigram with Add-K smoothing, k=0.5\n",
        "bigram_probs_addk = add_k_smoothing(bigrams, unigrams, vocab_size, k=0.5)\n"
      ],
      "metadata": {
        "id": "HWoB-L5rmLAq"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_token_type_smoothing(ngram_counts, lower_counts):\n",
        "    # Get number of unique continuations for each prefix\n",
        "    prefix_types = defaultdict(set)\n",
        "    for ngram in ngram_counts:\n",
        "        prefix = ngram[:-1]\n",
        "        prefix_types[prefix].add(ngram[-1])\n",
        "\n",
        "    probs = {}\n",
        "    for ngram, count in ngram_counts.items():\n",
        "        prefix = ngram[:-1]\n",
        "        num_types = len(prefix_types[prefix])\n",
        "        probs[ngram] = (count + 1) / num_types\n",
        "    return probs\n",
        "\n",
        "# Example: bigram with token-type smoothing\n",
        "bigram_probs_token_type = add_token_type_smoothing(bigrams, unigrams)\n"
      ],
      "metadata": {
        "id": "EjBl6d3hmOEH"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_top_probs(probs, n=10):\n",
        "    sorted_probs = sorted(probs.items(), key=lambda x: x[1], reverse=True)\n",
        "    for gram, p in sorted_probs[:n]:\n",
        "        print(f\"{' '.join(gram)} : {p:.4f}\")\n",
        "\n",
        "print(\"Top 10 bigrams with Add-One smoothing:\")\n",
        "print_top_probs(bigram_probs_add1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-z7djjfs6Mhi",
        "outputId": "6ed0877d-3c3f-4588-9b04-f59f2ff7a595"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 10 bigrams with Add-One smoothing:\n",
            "के लिए : 0.0530\n",
            "है . : 0.0397\n",
            "है कि : 0.0359\n",
            "कहा कि : 0.0296\n",
            "है , : 0.0275\n",
            "के साथ : 0.0216\n",
            "ने कहा : 0.0197\n",
            "के बाद : 0.0184\n",
            ". . : 0.0173\n",
            "हैं . : 0.0164\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json, random\n",
        "\n",
        "# Load tokenized JSON\n",
        "with open(\"tokenized_data_final.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# Extract all sentences (tokens)\n",
        "all_sentences = []\n",
        "for document in data:\n",
        "    for sent in document.get(\"sentences\", []):\n",
        "        tokens = sent.get(\"tokens\", [])\n",
        "        if tokens:  # only keep non-empty\n",
        "            all_sentences.append(tokens)\n",
        "\n",
        "print(f\"Total sentences: {len(all_sentences)}\")\n",
        "\n",
        "# Randomly select 1000 (or all if fewer than 1000)\n",
        "sample_sentences = random.sample(all_sentences, min(1000, len(all_sentences)))\n",
        "print(f\"Selected {len(sample_sentences)} sentences\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ScvjMD1D6l_U",
        "outputId": "b7521a7d-6c44-4d13-8a4d-3fd4c10e7e5e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total sentences: 35391\n",
            "Selected 1000 sentences\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json, random\n",
        "from collections import defaultdict, Counter\n",
        "import math\n",
        "\n",
        "# ----------------------------\n",
        "# Load tokenized sentences\n",
        "# ----------------------------\n",
        "with open(\"tokenized_data_final.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "all_sentences = []\n",
        "for document in data:\n",
        "    for sent in document.get(\"sentences\", []):\n",
        "        tokens = sent.get(\"tokens\", [])\n",
        "        if tokens:\n",
        "            all_sentences.append(tokens)\n",
        "\n",
        "sample_sentences = random.sample(all_sentences, min(1000, len(all_sentences)))\n",
        "print(f\"Selected {len(sample_sentences)} sentences\")\n",
        "\n",
        "# ----------------------------\n",
        "# Build n-gram models\n",
        "# ----------------------------\n",
        "def build_ngram_counts(sentences, n):\n",
        "    counts = defaultdict(Counter)\n",
        "    for sent in sentences:\n",
        "        sent = [\"<s>\"]*(n-1) + sent + [\"</s>\"]\n",
        "        for i in range(len(sent)-n+1):\n",
        "            context = tuple(sent[i:i+n-1])\n",
        "            token = sent[i+n-1]\n",
        "            counts[context][token] += 1\n",
        "    return counts\n",
        "\n",
        "unigram_counts = build_ngram_counts(all_sentences, 1)\n",
        "bigram_counts = build_ngram_counts(all_sentences, 2)\n",
        "trigram_counts = build_ngram_counts(all_sentences, 3)\n",
        "quadrigram_counts = build_ngram_counts(all_sentences, 4)\n",
        "\n",
        "# Vocabulary\n",
        "vocab = set()\n",
        "for sent in all_sentences:\n",
        "    vocab.update(sent)\n",
        "V = len(vocab)\n",
        "\n",
        "# ----------------------------\n",
        "# Smoothed probability functions\n",
        "# ----------------------------\n",
        "def unigram_prob(token, k=1):\n",
        "    total = sum(unigram_counts[()][t] for t in unigram_counts[()][t])\n",
        "    total = sum(unigram_counts[()][t] for t in unigram_counts[()])\n",
        "    count = unigram_counts[()][token]\n",
        "    return (count + k) / (total + k*V)\n",
        "\n",
        "def ngram_prob(context, token, ngram_counts, k=1):\n",
        "    context_count = sum(ngram_counts[context].values())\n",
        "    token_count = ngram_counts[context][token]\n",
        "    return (token_count + k) / (context_count + k*V)\n",
        "\n",
        "# ----------------------------\n",
        "# Compute sentence probabilities\n",
        "# ----------------------------\n",
        "def sentence_log_prob(sentence, n, ngram_counts, k=1):\n",
        "    sent = [\"<s>\"]*(n-1) + sentence + [\"</s>\"]\n",
        "    log_prob = 0.0\n",
        "    for i in range(n-1, len(sent)):\n",
        "        context = tuple(sent[i-n+1:i])\n",
        "        token = sent[i]\n",
        "        prob = ngram_prob(context, token, ngram_counts, k)\n",
        "        log_prob += math.log(prob)\n",
        "    return log_prob\n",
        "\n",
        "# ----------------------------\n",
        "# Compute for sample sentences\n",
        "# ----------------------------\n",
        "results = []\n",
        "\n",
        "for sent in sample_sentences:\n",
        "    unigram_lp = sentence_log_prob(sent, 1, unigram_counts, k=1)\n",
        "    bigram_lp = sentence_log_prob(sent, 2, bigram_counts, k=1)\n",
        "    trigram_lp = sentence_log_prob(sent, 3, trigram_counts, k=1)\n",
        "    quadrigram_lp = sentence_log_prob(sent, 4, quadrigram_counts, k=1)\n",
        "\n",
        "    results.append({\n",
        "        \"sentence\": \" \".join(sent),\n",
        "        \"Unigram_logP\": unigram_lp,\n",
        "        \"Bigram_logP\": bigram_lp,\n",
        "        \"Trigram_logP\": trigram_lp,\n",
        "        \"Quadrigram_logP\": quadrigram_lp\n",
        "    })\n",
        "\n",
        "# Optional: save results to JSON\n",
        "with open(\"sentence_probs.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(results, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(\"Probability computation done for all sampled sentences.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S0096-hU8WmQ",
        "outputId": "778d6b21-5900-4ccb-d248-c2cac70aedbf"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected 1000 sentences\n",
            "Probability computation done for all sampled sentences.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "def build_ngram_counts(tokens, n):\n",
        "    counts = defaultdict(int)\n",
        "    for i in range(len(tokens)-n+1):\n",
        "        ngram = tuple(tokens[i:i+n])\n",
        "        counts[ngram] += 1\n",
        "    return counts\n",
        "\n",
        "# Flatten all tokens to build models\n",
        "all_tokens = [tok for sent in all_sentences for tok in sent]\n",
        "\n",
        "unigrams = build_ngram_counts(all_tokens, 1)\n",
        "bigrams = build_ngram_counts(all_tokens, 2)\n",
        "trigrams = build_ngram_counts(all_tokens, 3)\n",
        "quadrigrams = build_ngram_counts(all_tokens, 4)\n",
        "\n",
        "vocab_size = len(unigrams)\n"
      ],
      "metadata": {
        "id": "gkOiy6wq6l82"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_one_prob(ngram, ngram_counts, lower_counts, vocab_size):\n",
        "    prefix = ngram[:-1]\n",
        "    return (ngram_counts.get(ngram, 0) + 1) / (lower_counts.get(prefix, 0) + vocab_size)\n",
        "\n",
        "def add_k_prob(ngram, ngram_counts, lower_counts, vocab_size, k=0.5):\n",
        "    prefix = ngram[:-1]\n",
        "    return (ngram_counts.get(ngram, 0) + k) / (lower_counts.get(prefix, 0) + k * vocab_size)\n",
        "\n",
        "def add_token_type_prob(ngram, ngram_counts):\n",
        "    prefix = ngram[:-1]\n",
        "    continuations = [g[-1] for g in ngram_counts if g[:-1] == prefix]\n",
        "    num_types = len(set(continuations)) if continuations else 1\n",
        "    return (ngram_counts.get(ngram, 0) + 1) / num_types\n"
      ],
      "metadata": {
        "id": "Ka6JUG536l6F"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "def sentence_probability(sentence, method=\"add_one\", k=0.5):\n",
        "    prob_log = 0.0\n",
        "    for i in range(len(sentence)-1):\n",
        "        bigram = (sentence[i], sentence[i+1])\n",
        "        unigram = (sentence[i],)\n",
        "\n",
        "        if method == \"add_one\":\n",
        "            p = add_one_prob(bigram, bigrams, unigrams, vocab_size)\n",
        "        elif method == \"add_k\":\n",
        "            p = add_k_prob(bigram, bigrams, unigrams, vocab_size, k)\n",
        "        elif method == \"token_type\":\n",
        "            p = add_token_type_prob(bigram, bigrams)\n",
        "        else:\n",
        "            raise ValueError(\"Unknown method\")\n",
        "\n",
        "        prob_log += math.log(p + 1e-12)  # avoid log(0)\n",
        "\n",
        "    return math.exp(prob_log)  # sentence probability\n"
      ],
      "metadata": {
        "id": "HVU79RI06l38"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame(results)\n",
        "df.to_csv(\"sentence_probabilities.csv\", index=False, encoding=\"utf-8\")\n",
        "print(\"Saved sentence probabilities to sentence_probabilities.csv\")\n"
      ],
      "metadata": {
        "id": "hnbTyvGN6yAX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "# Precompute continuations for token-type smoothing\n",
        "prefix_continuations = defaultdict(set)\n",
        "for (w1, w2), count in bigrams.items():\n",
        "    prefix_continuations[(w1,)].add(w2)\n",
        "\n",
        "def add_token_type_prob(ngram, ngram_counts, prefix_continuations):\n",
        "    prefix = ngram[:-1]\n",
        "    num_types = len(prefix_continuations.get(prefix, [])) or 1\n",
        "    return (ngram_counts.get(ngram, 0) + 1) / num_types\n"
      ],
      "metadata": {
        "id": "V6NDZTTV7S4I"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prob_cache = {}\n",
        "\n",
        "def sentence_probability(sentence, method=\"add_one\", k=0.5):\n",
        "    prob_log = 0.0\n",
        "    for i in range(len(sentence)-1):\n",
        "        bigram = (sentence[i], sentence[i+1])\n",
        "\n",
        "        if (bigram, method) in prob_cache:\n",
        "            p = prob_cache[(bigram, method)]\n",
        "        else:\n",
        "            if method == \"add_one\":\n",
        "                p = add_one_prob(bigram, bigrams, unigrams, vocab_size)\n",
        "            elif method == \"add_k\":\n",
        "                p = add_k_prob(bigram, bigrams, unigrams, vocab_size, k)\n",
        "            elif method == \"token_type\":\n",
        "                p = add_token_type_prob(bigram, bigrams, prefix_continuations)\n",
        "            prob_cache[(bigram, method)] = p\n",
        "\n",
        "        prob_log += math.log(p + 1e-12)\n",
        "\n",
        "    return math.exp(prob_log)\n"
      ],
      "metadata": {
        "id": "MYI0PSSh7S1m"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sentence_log_probability(sentence, method=\"add_one\", k=0.5):\n",
        "    prob_log = 0.0\n",
        "    for i in range(len(sentence)-1):\n",
        "        bigram = (sentence[i], sentence[i+1])\n",
        "\n",
        "        if (bigram, method) in prob_cache:\n",
        "            p = prob_cache[(bigram, method)]\n",
        "        else:\n",
        "            if method == \"add_one\":\n",
        "                p = add_one_prob(bigram, bigrams, unigrams, vocab_size)\n",
        "            elif method == \"add_k\":\n",
        "                p = add_k_prob(bigram, bigrams, unigrams, vocab_size, k)\n",
        "            elif method == \"token_type\":\n",
        "                p = add_token_type_prob(bigram, bigrams, prefix_continuations)\n",
        "            prob_cache[(bigram, method)] = p\n",
        "\n",
        "        prob_log += math.log(p + 1e-12)\n",
        "    return prob_log\n"
      ],
      "metadata": {
        "id": "7yxHdVk-7SzW"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = []\n",
        "for sent in sample_sentences[:20]:\n",
        "    length = len(sent) or 1\n",
        "\n",
        "    log_add1 = sentence_log_probability(sent, \"add_one\")\n",
        "    log_addk = sentence_log_probability(sent, \"add_k\", k=0.5)\n",
        "    log_tt = sentence_log_probability(sent, \"token_type\")\n",
        "\n",
        "    results.append({\n",
        "        \"Sentence\": \" \".join(sent),\n",
        "        \"Length\": length,\n",
        "        \"Add-One (avg log P)\": log_add1 / length,\n",
        "        \"Add-K=0.5 (avg log P)\": log_addk / length,\n",
        "        \"Token-Type (avg log P)\": log_tt / length\n",
        "    })\n"
      ],
      "metadata": {
        "id": "fOeK3PD37aAx"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for row in results[:5]:\n",
        "    print(\"📌 Sentence:\", row[\"Sentence\"])\n",
        "    print(f\"   🔹 Add-One log P: {row['Add-One (log P)']:.2f}\")\n",
        "    print(f\"   🔹 Add-K=0.5 log P: {row['Add-K=0.5 (log P)']:.2f}\")\n",
        "    print(f\"   🔹 Token-Type log P: {row['Token-Type (log P)']:.2f}\")\n",
        "\n",
        "    best = max(\n",
        "        [(\"Add-One\", row['Add-One (log P)']),\n",
        "         (\"Add-K\", row['Add-K=0.5 (log P)']),\n",
        "         (\"Token-Type\", row['Token-Type (log P)'])],\n",
        "        key=lambda x: x[1]\n",
        "    )\n",
        "    print(f\"   ✅ Most likely under: {best[0]}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pX1Ylq1t7mFu",
        "outputId": "cb2189f3-7fdd-4213-8385-8dcfd151bc43"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📌 Sentence: बांदे थाना में पदस्थ एएसआई शिव कुमार मंडावी को दिन दहाड़े बाजार में गोली मारी।\n",
            "   🔹 Add-One log P: -130.64\n",
            "   🔹 Add-K=0.5 log P: -124.47\n",
            "   🔹 Token-Type log P: -35.82\n",
            "   ✅ Most likely under: Token-Type\n",
            "\n",
            "📌 Sentence: डॉ .\n",
            "   🔹 Add-One log P: -5.55\n",
            "   🔹 Add-K=0.5 log P: -4.86\n",
            "   🔹 Token-Type log P: 1.50\n",
            "   ✅ Most likely under: Token-Type\n",
            "\n",
            "📌 Sentence: आॅस्ट्रेलियाई क्रिकेट टीम के पूर्व कप्तान स्टीव स्मिथ ने गेंद से छेड़छाड़ के मामले में सार्वजनिक तौर पर माफी मांगी है .\n",
            "   🔹 Add-One log P: -171.54\n",
            "   🔹 Add-K=0.5 log P: -160.88\n",
            "   🔹 Token-Type log P: -43.43\n",
            "   ✅ Most likely under: Token-Type\n",
            "\n",
            "📌 Sentence: हिंदी न्यूज़ बिहार सहरसा अभियंता दिवस पर याद किए गए डॉ .\n",
            "   🔹 Add-One log P: -89.36\n",
            "   🔹 Add-K=0.5 log P: -83.32\n",
            "   🔹 Token-Type log P: -15.07\n",
            "   ✅ Most likely under: Token-Type\n",
            "\n",
            "📌 Sentence: मड़ियाहूं कोतवाली क्षेत्र के चोरारी गांव निवासी डॉ .\n",
            "   🔹 Add-One log P: -65.69\n",
            "   🔹 Add-K=0.5 log P: -61.52\n",
            "   🔹 Token-Type log P: -9.08\n",
            "   ✅ Most likely under: Token-Type\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EU5ETKXI7rDB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}