{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y-W-wGmT34P8",
        "outputId": "7568b55a-6f23-4952-89d4-8508c2edde37"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total tokens: 13020645\n",
            "Sample: ['à¤•à¤¾', 'à¤•à¤¹à¤¨à¤¾', 'à¤¹à¥ˆ', 'à¤•à¤¿', '848', 'à¤•à¤¿à¤¯à¤¾', 'à¤œà¤¾', 'à¤°à¤¹à¤¾', 'à¤¹à¥ˆ', '557', 'à¤•à¥€', 'à¤œà¤¾', 'à¤°à¤¹à¥€', 'à¤¹à¥ˆ', '469', 'à¤¹à¥ˆ', 'à¤‰à¤¨à¥à¤¹à¥‹à¤‚à¤¨à¥‡', 'à¤•à¤¹à¤¾', 'à¤•à¤¿', '428', 'à¤œà¤¾', 'à¤°à¤¹à¤¾', 'à¤¹à¥ˆ', 'à¤•à¤¿', '373', 'à¤¨à¥‡', 'à¤•à¤¹à¤¾', 'à¤¹à¥ˆ', 'à¤•à¤¿', '347']\n"
          ]
        }
      ],
      "source": [
        "# Load your dataset\n",
        "with open(\"quadrigrams.txt\", \"r\") as f:\n",
        "    text = f.read().lower().strip().split()\n",
        "\n",
        "tokens = text\n",
        "print(\"Total tokens:\", len(tokens))\n",
        "print(\"Sample:\", tokens[:30])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dcyh5kvb4ePN",
        "outputId": "2f1023e4-434e-424a-955d-a2e1a5085091"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample 4-grams: [(('à¤•à¤¾', 'à¤•à¤¹à¤¨à¤¾', 'à¤¹à¥ˆ', 'à¤•à¤¿'), 1), (('à¤•à¤¹à¤¨à¤¾', 'à¤¹à¥ˆ', 'à¤•à¤¿', '848'), 1), (('à¤¹à¥ˆ', 'à¤•à¤¿', '848', 'à¤•à¤¿à¤¯à¤¾'), 1), (('à¤•à¤¿', '848', 'à¤•à¤¿à¤¯à¤¾', 'à¤œà¤¾'), 1), (('848', 'à¤•à¤¿à¤¯à¤¾', 'à¤œà¤¾', 'à¤°à¤¹à¤¾'), 1), (('à¤•à¤¿à¤¯à¤¾', 'à¤œà¤¾', 'à¤°à¤¹à¤¾', 'à¤¹à¥ˆ'), 1), (('à¤œà¤¾', 'à¤°à¤¹à¤¾', 'à¤¹à¥ˆ', '557'), 1), (('à¤°à¤¹à¤¾', 'à¤¹à¥ˆ', '557', 'à¤•à¥€'), 1), (('à¤¹à¥ˆ', '557', 'à¤•à¥€', 'à¤œà¤¾'), 1), (('557', 'à¤•à¥€', 'à¤œà¤¾', 'à¤°à¤¹à¥€'), 1)]\n"
          ]
        }
      ],
      "source": [
        "from nltk.util import ngrams\n",
        "from collections import Counter\n",
        "\n",
        "def build_ngram_counts(tokens, n=4):\n",
        "    counts = {i: Counter(ngrams(tokens, i)) for i in range(1, n+1)}\n",
        "    return counts\n",
        "\n",
        "counts = build_ngram_counts(tokens, 4)\n",
        "print(\"Sample 4-grams:\", list(counts[4].items())[:10])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "def compute_ngram_probs(counts_higher, counts_lower, discount=0.5):\n",
        "    probs = {}\n",
        "    for ngram, c in counts_higher.items():\n",
        "        prefix = ngram[:-1]\n",
        "        c_prefix = counts_lower.get(prefix, 0)\n",
        "        if c_prefix > 0:\n",
        "            probs[ngram] = max(c - discount, 0) / c_prefix\n",
        "    return probs\n",
        "\n",
        "def compute_backoff_weights(counts_higher, counts_lower, probs_higher, probs_lower):\n",
        "    backoff = defaultdict(float)\n",
        "    contexts = set([ngram[:-1] for ngram in counts_higher.keys()])\n",
        "    for h in contexts:\n",
        "        seen_words = [ngram[-1] for ngram in counts_higher.keys() if ngram[:-1]==h]\n",
        "        sum_disc = sum(probs_higher[(h+(w,))] for w in seen_words if (h+(w,)) in probs_higher)\n",
        "        sum_lower = sum(probs_lower.get((h[1:],w), 0) for w in seen_words)  # backoff context\n",
        "        if (1 - sum_lower) > 0:\n",
        "            backoff[h] = (1 - sum_disc) / (1 - sum_lower)\n",
        "        else:\n",
        "            backoff[h] = 1.0\n",
        "    return backoff\n",
        "\n",
        "def katz_prob(word, context, probs4, probs3, probs2, probs1, back4, back3, back2):\n",
        "    # context = tuple of previous words (up to 3)\n",
        "    if len(context) >= 3 and (context + (word,)) in probs4:\n",
        "        return probs4[context + (word,)]\n",
        "    elif len(context) >= 3 and context in back4:\n",
        "        return back4[context] * katz_prob(word, context[1:], probs3, probs2, probs1, back3, back2, {})\n",
        "    elif len(context) >= 2 and (context + (word,)) in probs3:\n",
        "        return probs3[context + (word,)]\n",
        "    elif len(context) >= 2 and context in back3:\n",
        "        return back3[context] * katz_prob(word, context[1:], probs2, probs1, {}, {}, {}, {})\n",
        "    elif len(context) >= 1 and (context + (word,)) in probs2:\n",
        "        return probs2[context + (word,)]\n",
        "    elif len(context) >= 1 and context in back2:\n",
        "        return back2[context] * probs1.get((word,), 1e-8)\n",
        "    else:\n",
        "        return probs1.get((word,), 1e-8)\n"
      ],
      "metadata": {
        "id": "gxk4A-4B0Hbo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Task: Load up to 10,000 sentences from 'sentence_tokens.txt'\n",
        "and split them into Train / Validation / Test sets.\n",
        "\n",
        " - Validation Set: 1,000 sentences\n",
        " - Test Set: 1,000 sentences\n",
        " - Training Set: remaining 8,000 sentences\n",
        "\n",
        "Outputs:\n",
        "    - train.pkl (list of token lists)\n",
        "    - val.pkl\n",
        "    - test.pkl\n",
        "    - split_summary.txt\n",
        "\"\"\"\n",
        "\n",
        "import os, pickle, random\n",
        "\n",
        "INPUT = \"sentence_tokens.txt\"\n",
        "VAL_SIZE = 1000\n",
        "TEST_SIZE = 1000\n",
        "MAX_SENTENCES = 10000\n",
        "SEED = 42\n",
        "\n",
        "def load_sentences_txt(path):\n",
        "    \"\"\"Load sentences from .txt and tokenize by whitespace.\"\"\"\n",
        "    with open(path, \"r\", encoding=\"utf8\") as fh:\n",
        "        sentences = [line.strip().split() for line in fh if line.strip()]\n",
        "    # take only first 10,000\n",
        "    return sentences[:MAX_SENTENCES]\n",
        "\n",
        "def save_pickle(obj, fn):\n",
        "    with open(fn, \"wb\") as fh:\n",
        "        pickle.dump(obj, fh)\n",
        "\n",
        "def main():\n",
        "    if not os.path.exists(INPUT):\n",
        "        raise FileNotFoundError(f\"Could not find {INPUT}\")\n",
        "\n",
        "    sentences = load_sentences_txt(INPUT)\n",
        "    total = len(sentences)\n",
        "    print(f\"Loaded {total} sentences from {INPUT}\")\n",
        "\n",
        "    if total < VAL_SIZE + TEST_SIZE:\n",
        "        raise ValueError(\"Not enough sentences to create the requested splits.\")\n",
        "\n",
        "    random.seed(SEED)\n",
        "    random.shuffle(sentences)\n",
        "\n",
        "    val = sentences[:VAL_SIZE]\n",
        "    test = sentences[VAL_SIZE:VAL_SIZE + TEST_SIZE]\n",
        "    train = sentences[VAL_SIZE + TEST_SIZE:]\n",
        "\n",
        "    save_pickle(train, \"train.pkl\")\n",
        "    save_pickle(val, \"val.pkl\")\n",
        "    save_pickle(test, \"test.pkl\")\n",
        "\n",
        "    print(f\"âœ… train.pkl â†’ {len(train)} sentences\")\n",
        "    print(f\"âœ… val.pkl   â†’ {len(val)} sentences\")\n",
        "    print(f\"âœ… test.pkl  â†’ {len(test)} sentences\")\n",
        "\n",
        "    # Write a summary text file\n",
        "    with open(\"split_summary.txt\", \"w\", encoding=\"utf8\") as fh:\n",
        "        fh.write(f\"Total sentences (used): {total}\\n\")\n",
        "        fh.write(f\"Train: {len(train)}\\nVal: {len(val)}\\nTest: {len(test)}\\n\")\n",
        "\n",
        "    print(\"ðŸ“„ Wrote split_summary.txt\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cS9c2uKJ5Hzp",
        "outputId": "fd8d72cc-1ab4-4e2a-fb72-825dca1c6f2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 10000 sentences from sentence_tokens.txt\n",
            "âœ… train.pkl â†’ 8000 sentences\n",
            "âœ… val.pkl   â†’ 1000 sentences\n",
            "âœ… test.pkl  â†’ 1000 sentences\n",
            "ðŸ“„ Wrote split_summary.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Generate unigram/bigram/trigram/quadrigram count files from train.pkl\n",
        "Outputs: counts_n1.pkl ... counts_n4.pkl\n",
        "\"\"\"\n",
        "\n",
        "import pickle\n",
        "from collections import Counter\n",
        "from pathlib import Path\n",
        "\n",
        "TRAIN_PICKLE = \"train.pkl\"\n",
        "MAX_N = 4\n",
        "SPECIAL_START = \"<s>\"\n",
        "SPECIAL_END = \"</s>\"\n",
        "\n",
        "def load_pickle(fn):\n",
        "    with open(fn, \"rb\") as fh:\n",
        "        return pickle.load(fh)\n",
        "\n",
        "def ngrams_from_sentence(tokens, n):\n",
        "    padded = [SPECIAL_START] * (n - 1) + tokens + [SPECIAL_END]\n",
        "    return [tuple(padded[i:i + n]) for i in range(len(padded) - n + 1)]\n",
        "\n",
        "def build_counts(sentences, n):\n",
        "    c = Counter()\n",
        "    for s in sentences:\n",
        "        ngs = ngrams_from_sentence(s, n)\n",
        "        c.update(ngs)\n",
        "    return c\n",
        "\n",
        "def save_pickle(obj, fn):\n",
        "    with open(fn, \"wb\") as fh:\n",
        "        pickle.dump(obj, fh)\n",
        "\n",
        "def main():\n",
        "    train = load_pickle(TRAIN_PICKLE)\n",
        "    print(f\"Loaded train ({len(train)} sentences). Building n-gram counts...\")\n",
        "    for n in range(1, MAX_N + 1):\n",
        "        counts = build_counts(train, n)\n",
        "        save_pickle(counts, f\"counts_n{n}.pkl\")\n",
        "        print(f\"Saved counts_n{n}.pkl with {len(counts)} unique n-grams\")\n",
        "    print(\"Done. Generated counts_n1.pkl to counts_n4.pkl\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sAXB9cOc5KLM",
        "outputId": "4a18b6d4-803c-4f0f-f016-d14e9d230158"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded train (8000 sentences). Building n-gram counts...\n",
            "Saved counts_n1.pkl with 18172 unique n-grams\n",
            "Saved counts_n2.pkl with 94259 unique n-grams\n",
            "Saved counts_n3.pkl with 143362 unique n-grams\n",
            "Saved counts_n4.pkl with 159153 unique n-grams\n",
            "Done. Generated counts_n1.pkl to counts_n4.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "counts_n1=[]\n",
        "counts_n3=[]\n",
        "counts_n4=[]\n",
        "counts_n2=[]\n",
        "with open(\"counts_n1.pkl\", \"rb\") as f:\n",
        "    counts_n1 = pickle.load(f)  # for unigram or bigram as per your data\n",
        "with open(\"counts_n2.pkl\", \"rb\") as f:\n",
        "    counts_n2 = pickle.load(f)\n",
        "with open(\"counts_n3.pkl\", \"rb\") as f:\n",
        "    counts_n3 = pickle.load(f)\n",
        "with open(\"counts_n4.pkl\", \"rb\") as f:\n",
        "    counts_n4 = pickle.load(f)\n",
        "\n",
        "\n",
        "\n",
        "probs4 = compute_ngram_probs(counts_n4, counts_n3)\n",
        "probs3 = compute_ngram_probs(counts_n3, counts_n2)\n",
        "probs2 = compute_ngram_probs(counts_n2, counts_n1)\n",
        "probs1 = {k: v / sum(counts_n1.values()) for k, v in counts_n1.items()}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(\"âœ… Computed discounted probabilities and backoff weights successfully.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kebwsyP55bWm",
        "outputId": "33810778-df99-4707-bdbb-a17ac0428354"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Computed discounted probabilities and backoff weights successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "back4 = compute_backoff_weights(counts_n4, counts_n3, probs4, probs3)"
      ],
      "metadata": {
        "id": "EvcyFwp3LVsc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "back3 = compute_backoff_weights(counts_n3, counts_n2, probs3, probs2)\n",
        "back2 = compute_backoff_weights(counts_n2, counts_n1, probs2, probs1)"
      ],
      "metadata": {
        "id": "LRJsUDgnLRcD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(probs1))\n",
        "print(len(probs2))\n",
        "print(len(probs3))\n",
        "print(len(probs4))\n",
        "print(len(back4))\n",
        "print(len(back3))\n",
        "print(len(back2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AbWiQDT4dKRs",
        "outputId": "0e4034db-b837-449e-ffbf-ea2628541afe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "18172\n",
            "91417\n",
            "140520\n",
            "156311\n",
            "139548\n",
            "93090\n",
            "18172\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "\n",
        "def generate_sentence_greedy(katz_prob_fn, start_tokens, max_len=20, vocab=None, top_k=10, min_len=5):\n",
        "    sentence = list(start_tokens)\n",
        "\n",
        "    for _ in range(max_len):\n",
        "        context = tuple(sentence[-(len(start_tokens)):])\n",
        "        # Get probabilities for all words\n",
        "        word_probs = np.array([katz_prob_fn(w, context) for w in vocab])\n",
        "\n",
        "        # Normalize to sum=1 (important)\n",
        "        if word_probs.sum() == 0:\n",
        "            word_probs = np.ones_like(word_probs) / len(word_probs)\n",
        "        else:\n",
        "            word_probs /= word_probs.sum()\n",
        "\n",
        "        # Get indices of top_k most probable words\n",
        "        top_indices = np.argsort(word_probs)[-top_k:]\n",
        "        top_words = [vocab[i] for i in top_indices]\n",
        "        top_probs = word_probs[top_indices]\n",
        "\n",
        "        # Re-normalize top-k probabilities\n",
        "        top_probs = top_probs / top_probs.sum()\n",
        "\n",
        "        # Randomly choose next word among top-k\n",
        "        next_word = np.random.choice(top_words, p=top_probs)\n",
        "\n",
        "        # Avoid early </s>\n",
        "        if len(sentence) < min_len and next_word == '</s>':\n",
        "            continue\n",
        "\n",
        "        sentence.append(next_word)\n",
        "\n",
        "        if next_word == '</s>':\n",
        "            break\n",
        "\n",
        "    return sentence\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import heapq\n",
        "import random\n",
        "\n",
        "def generate_sentence_beam(katz_prob_fn, start_tokens, beam_size=20, max_len=20, vocab=None, top_k=10, temperature=1.0):\n",
        "    beam = [(0.0, list(start_tokens))]  # (neg log-prob, sequence)\n",
        "\n",
        "    for _ in range(max_len):\n",
        "        candidates = []\n",
        "        for logp, seq in beam:\n",
        "            context = tuple(seq[-(len(start_tokens)):])\n",
        "\n",
        "            # Compute probabilities for all words\n",
        "            probs = np.array([katz_prob_fn(w, context) for w in vocab])\n",
        "            probs = np.maximum(probs, 1e-12)  # Avoid log(0)\n",
        "            probs = probs / probs.sum()\n",
        "\n",
        "            # Pick top-k candidates\n",
        "            top_indices = np.argsort(probs)[-top_k:]\n",
        "            top_probs = probs[top_indices]\n",
        "            top_probs = top_probs / top_probs.sum()\n",
        "\n",
        "            # Randomly sample one next word (weighted by probability)\n",
        "            sampled_idx = np.random.choice(top_indices, p=top_probs)\n",
        "            w = vocab[sampled_idx]\n",
        "            p = probs[sampled_idx]\n",
        "\n",
        "            candidates.append((logp - np.log(p), seq + [w]))\n",
        "\n",
        "        # Keep best `beam_size` beams\n",
        "        beam = heapq.nsmallest(beam_size, candidates, key=lambda x: x[0])\n",
        "\n",
        "        # Stop if all beams end with </s>\n",
        "        if all(seq[-1] == '</s>' for _, seq in beam):\n",
        "            break\n",
        "\n",
        "    # Randomly pick one of the final top beams (for more diversity)\n",
        "    return random.choice(beam)[1]\n",
        "\n",
        "\n",
        "def generate_sentences(katz_prob_fn, vocab, n_sentences=100, start_tokens=None, method=\"greedy\", beam_size=20):\n",
        "    sentences = []\n",
        "    if start_tokens is None:\n",
        "        start_tokens = ['<s>'] * (len(vocab[0].split()) if hasattr(vocab[0], \"__iter__\") else 3)\n",
        "\n",
        "    for _ in range(n_sentences):\n",
        "        if method == \"greedy\":\n",
        "            sent = generate_sentence_greedy(katz_prob_fn, start_tokens, vocab=vocab)\n",
        "        elif method == \"beam\":\n",
        "            sent = generate_sentence_beam(katz_prob_fn, start_tokens, beam_size=beam_size, vocab=vocab)\n",
        "        else:\n",
        "            raise ValueError(\"Method must be 'greedy' or 'beam'\")\n",
        "        sentences.append(\" \".join(sent))\n",
        "    return sentences\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "import heapq\n",
        "\n",
        "# Example: Katz probability function for quadrigrams (or any n-gram)\n",
        "def get_katz_prob_fn(n, probs_list, backoff_list):\n",
        "    \"\"\"\n",
        "    Returns a function f(word, context) â†’ probability\n",
        "    probs_list = [probs1, probs2, ..., probsN]\n",
        "    backoff_list = [back1, back2, ..., backN-1]\n",
        "    \"\"\"\n",
        "    def katz_prob(word, context):\n",
        "        if n == 1:\n",
        "            return probs_list[0].get((word,), 1e-8)\n",
        "        else:\n",
        "            # recursively backoff\n",
        "            context = context[-(n-1):] if len(context) >= n-1 else context\n",
        "            if tuple(context + (word,)) in probs_list[n-1]:\n",
        "                return probs_list[n-1][tuple(context + (word,))]\n",
        "            elif tuple(context) in backoff_list[n-2]:\n",
        "                lower_fn = get_katz_prob_fn(n-1, probs_list, backoff_list)\n",
        "                return backoff_list[n-2][tuple(context)] * lower_fn(word, context[1:])\n",
        "            else:\n",
        "                lower_fn = get_katz_prob_fn(n-1, probs_list, backoff_list)\n",
        "                return lower_fn(word, context[1:])\n",
        "    return katz_prob"
      ],
      "metadata": {
        "id": "LLYPWCb_1nue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# probs_list = [probs1, probs2, probs3, probs4]\n",
        "# backoff_list = [back2, back3, back4]  # backoff weights for 2-,3-,4-grams\n",
        "vocab = [w[0] for w in counts_n1.keys()]  # unigram vocabulary\n",
        "start_tokens = ['<s>', '<s>', '<s>']\n",
        "\n",
        "# Get Katz probability function\n",
        "katz_fn = get_katz_prob_fn(4, [probs1, probs2, probs3, probs4], [back2,back3, back4])\n",
        "\n",
        "# Generate 100 greedy sentences\n",
        "greedy_sentences = generate_sentences(katz_fn, vocab, n_sentences=10, start_tokens=start_tokens, method=\"greedy\")\n",
        "\n",
        "# Optionally save to files\n",
        "with open(\"quadrigram_greedy.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    for s in greedy_sentences:\n",
        "        f.write(s + \"\\n\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8zZNq9dd1qAy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate 100 beam search sentences\n",
        "beam_sentences = generate_sentences(katz_fn, vocab, n_sentences=10, start_tokens=start_tokens, method=\"beam\", beam_size=20)\n",
        "\n",
        "with open(\"quadrigram_beam.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    for s in beam_sentences:\n",
        "        f.write(s + \"\\n\")"
      ],
      "metadata": {
        "id": "k6_5OG-xiozv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "\n",
        "def generate_sentence_greedy(katz_prob_fn, start_tokens, max_len=20, vocab=None, top_k=10, min_len=5):\n",
        "    sentence = list(start_tokens)\n",
        "\n",
        "    for _ in range(max_len):\n",
        "        context = tuple(sentence[-(len(start_tokens)):])\n",
        "        # Get probabilities for all words\n",
        "        word_probs = np.array([katz_prob_fn(w, context) for w in vocab])\n",
        "\n",
        "        # Normalize to sum=1 (important)\n",
        "        if word_probs.sum() == 0:\n",
        "            word_probs = np.ones_like(word_probs) / len(word_probs)\n",
        "        else:\n",
        "            word_probs /= word_probs.sum()\n",
        "\n",
        "        # Get indices of top_k most probable words\n",
        "        top_indices = np.argsort(word_probs)[-top_k:]\n",
        "        top_words = [vocab[i] for i in top_indices]\n",
        "        top_probs = word_probs[top_indices]\n",
        "\n",
        "        # Re-normalize top-k probabilities\n",
        "        top_probs = top_probs / top_probs.sum()\n",
        "\n",
        "        # Randomly choose next word among top-k\n",
        "        next_word = np.random.choice(top_words, p=top_probs)\n",
        "\n",
        "        # Avoid early </s>\n",
        "        if len(sentence) < min_len and next_word == '</s>':\n",
        "            continue\n",
        "\n",
        "        sentence.append(next_word)\n",
        "\n",
        "        if next_word == '</s>':\n",
        "            break\n",
        "\n",
        "    return sentence\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import heapq\n",
        "import random\n",
        "\n",
        "def generate_sentence_beam(katz_prob_fn, start_tokens, beam_size=20, max_len=20, vocab=None, top_k=10, temperature=1.0):\n",
        "    beam = [(0.0, list(start_tokens))]  # (neg log-prob, sequence)\n",
        "\n",
        "    for _ in range(max_len):\n",
        "        candidates = []\n",
        "        for logp, seq in beam:\n",
        "            context = tuple(seq[-(len(start_tokens)):])\n",
        "\n",
        "            # Compute probabilities for all words\n",
        "            probs = np.array([katz_prob_fn(w, context) for w in vocab])\n",
        "            probs = np.maximum(probs, 1e-12)  # Avoid log(0)\n",
        "            probs = probs / probs.sum()\n",
        "\n",
        "            # Pick top-k candidates\n",
        "            top_indices = np.argsort(probs)[-top_k:]\n",
        "            top_probs = probs[top_indices]\n",
        "            top_probs = top_probs / top_probs.sum()\n",
        "\n",
        "            # Randomly sample one next word (weighted by probability)\n",
        "            sampled_idx = np.random.choice(top_indices, p=top_probs)\n",
        "            w = vocab[sampled_idx]\n",
        "            p = probs[sampled_idx]\n",
        "\n",
        "            candidates.append((logp - np.log(p), seq + [w]))\n",
        "\n",
        "        # Keep best `beam_size` beams\n",
        "        beam = heapq.nsmallest(beam_size, candidates, key=lambda x: x[0])\n",
        "\n",
        "        # Stop if all beams end with </s>\n",
        "        if all(seq[-1] == '</s>' for _, seq in beam):\n",
        "            break\n",
        "\n",
        "    # Randomly pick one of the final top beams (for more diversity)\n",
        "    return random.choice(beam)[1]\n",
        "\n",
        "\n",
        "def generate_sentences(katz_prob_fn, vocab, n_sentences=100, start_tokens=None, method=\"greedy\", beam_size=20):\n",
        "    sentences = []\n",
        "    if start_tokens is None:\n",
        "        start_tokens = ['<s>'] * (len(vocab[0].split()) if hasattr(vocab[0], \"__iter__\") else 3)\n",
        "\n",
        "    for _ in range(n_sentences):\n",
        "        if method == \"greedy\":\n",
        "            sent = generate_sentence_greedy(katz_prob_fn, start_tokens, vocab=vocab)\n",
        "        elif method == \"beam\":\n",
        "            sent = generate_sentence_beam(katz_prob_fn, start_tokens, beam_size=beam_size, vocab=vocab)\n",
        "        else:\n",
        "            raise ValueError(\"Method must be 'greedy' or 'beam'\")\n",
        "        sentences.append(\" \".join(sent))\n",
        "    return sentences\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "import heapq\n",
        "\n",
        "# Example: Katz probability function for quadrigrams (or any n-gram)\n",
        "def get_katz_prob_fn(n, probs_list, backoff_list):\n",
        "    \"\"\"\n",
        "    Returns a function f(word, context) â†’ probability\n",
        "    probs_list = [probs1, probs2, ..., probsN]\n",
        "    backoff_list = [back1, back2, ..., backN-1]\n",
        "    \"\"\"\n",
        "    def katz_prob(word, context):\n",
        "        if n == 1:\n",
        "            return probs_list[0].get((word,), 1e-8)\n",
        "        else:\n",
        "            # recursively backoff\n",
        "            context = context[-(n-1):] if len(context) >= n-1 else context\n",
        "            if tuple(context + (word,)) in probs_list[n-1]:\n",
        "                return probs_list[n-1][tuple(context + (word,))]\n",
        "            elif tuple(context) in backoff_list[n-2]:\n",
        "                lower_fn = get_katz_prob_fn(n-1, probs_list, backoff_list)\n",
        "                return backoff_list[n-2][tuple(context)] * lower_fn(word, context[1:])\n",
        "            else:\n",
        "                lower_fn = get_katz_prob_fn(n-1, probs_list, backoff_list)\n",
        "                return lower_fn(word, context[1:])\n",
        "    return katz_prob"
      ],
      "metadata": {
        "id": "4AumaMO2D77h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from collections import Counter\n",
        "\n",
        "# Suppose you already have a dictionary of n-gram counts:\n",
        "# e.g., counts = {('the',): 123, ('of',): 234, ('new',): 5, ...}\n",
        "# Replace with your actual counts dictionary\n",
        "counts=[]\n",
        "with open(\"counts_n1.pkl\", \"rb\") as f:\n",
        "    counts = pickle.load(f)  # for unigram or bigram as per your data\n",
        "\n",
        "# Step 1: Count frequencies of frequencies\n",
        "freq_counter = Counter(counts.values())  # Nc table\n",
        "\n",
        "# Step 2: Sort by observed count\n",
        "sorted_counts = sorted(freq_counter.items())\n",
        "\n",
        "# Step 3: Compute C* using Good-Turing formula\n",
        "rows = []\n",
        "for c, Nc in sorted_counts[:100]:  # top 100 frequency levels\n",
        "    Nc1 = freq_counter.get(c + 1, 0)\n",
        "    if Nc > 0:\n",
        "        c_star = (c + 1) * (Nc1 / Nc)\n",
        "    else:\n",
        "        c_star = 0\n",
        "    rows.append((c, Nc, round(c_star, 4)))\n",
        "\n",
        "# Step 4: Convert to DataFrame for pretty table\n",
        "gt_table = pd.DataFrame(rows, columns=[\"C (Observed Count)\", \"Nc (Frequency of Count)\", \"C* (Adjusted Count)\"])\n",
        "print(gt_table)"
      ],
      "metadata": {
        "id": "uUbdXfflD9mL",
        "outputId": "c99d71d1-c011-4121-e96a-8957bb85a99b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'counts_n1.pkl'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-279248035.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Replace with your actual counts dictionary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mcounts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"counts_n1.pkl\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mcounts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# for unigram or bigram as per your data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'counts_n1.pkl'"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}