{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6m8ItQr9swwD",
        "outputId": "3c2e22ae-59ae-4306-cf7f-a790dde5e8bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cat -> Accepted\n",
            "dog -> Accepted\n",
            "a -> Accepted\n",
            "zebra -> Accepted\n",
            "dog1 -> Not Accepted\n",
            "1dog -> Not Accepted\n",
            "DogHouse -> Not Accepted\n",
            "Dog_house -> Not Accepted\n",
            " cats -> Not Accepted\n"
          ]
        }
      ],
      "source": [
        "# DFA for simplified English words\n",
        "\n",
        "def dfa_validate(word):\n",
        "    state = 0  # start state\n",
        "    for i, ch in enumerate(word):\n",
        "        if state == 0:\n",
        "            # First character must be lowercase a-z\n",
        "            if 'a' <= ch <= 'z':\n",
        "                state = 1\n",
        "            else:\n",
        "                return False\n",
        "        elif state == 1:\n",
        "            # Following characters must also be lowercase a-z\n",
        "            if 'a' <= ch <= 'z':\n",
        "                state = 1\n",
        "            else:\n",
        "                return False\n",
        "    return state == 1  # must end in valid state\n",
        "\n",
        "\n",
        "# Test words\n",
        "test_words = [\n",
        "    \"cat\", \"dog\", \"a\", \"zebra\",   # Accepted\n",
        "    \"dog1\", \"1dog\", \"DogHouse\", \"Dog_house\", \" cats\"  # Not accepted\n",
        "]\n",
        "\n",
        "# Run DFA and print results\n",
        "for word in test_words:\n",
        "    if dfa_validate(word):\n",
        "        print(f\"{word} -> Accepted\")\n",
        "    else:\n",
        "        print(f\"{word} -> Not Accepted\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install automathon and graphviz\n",
        "!pip install automathon graphviz\n",
        "\n",
        "from automathon import DFA\n",
        "from IPython.display import Image\n",
        "\n",
        "# States\n",
        "Q = {'q0', 'q1', 'q_dead'}\n",
        "\n",
        "# Alphabet\n",
        "sigma = set([chr(c) for c in range(ord('a'), ord('z')+1)] + ['other'])\n",
        "\n",
        "# Transition function\n",
        "delta = {\n",
        "    'q0': dict(**{ch: 'q1' for ch in [chr(c) for c in range(ord('a'), ord('z')+1)]},\n",
        "               **{'other': 'q_dead'}),\n",
        "    'q1': dict(**{ch: 'q1' for ch in [chr(c) for c in range(ord('a'), ord('z')+1)]},\n",
        "               **{'other': 'q_dead'}),\n",
        "    'q_dead': {ch: 'q_dead' for ch in sigma}\n",
        "}\n",
        "\n",
        "# Start state\n",
        "q0 = 'q0'\n",
        "\n",
        "# Final states\n",
        "F = {'q1'}\n",
        "\n",
        "# Create DFA\n",
        "dfa = DFA(Q, sigma, delta, q0, F)\n",
        "\n",
        "# View DFA (this will create \"dfa.gv.png\")\n",
        "dfa.view(\"dfa\")\n",
        "\n",
        "# Display DFA diagram\n",
        "Image(filename=\"dfa.gv.png\")\n",
        "\n",
        "# Function to classify words\n",
        "def classify(word):\n",
        "    if not word:\n",
        "        return \"Not Accepted\"\n",
        "    if not ('a' <= word[0] <= 'z'):\n",
        "        return \"Not Accepted\"\n",
        "    for ch in word:\n",
        "        if not ('a' <= ch <= 'z'):\n",
        "            return \"Not Accepted\"\n",
        "    return \"Accepted\"\n",
        "\n",
        "# Test words\n",
        "test_words = [\"cat\", \"dog\", \"a\", \"zebra\", \"dog1\", \"1dog\", \"DogHouse\", \"Dog_house\", \" cats\"]\n",
        "for w in test_words:\n",
        "    print(f\"{w} -> {classify(w)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1KHjYse_tMb1",
        "outputId": "a7b1a1bb-e786-413c-a8e8-fc02c6ae89ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: automathon in /usr/local/lib/python3.11/dist-packages (0.0.15)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.11/dist-packages (0.16)\n",
            "cat -> Accepted\n",
            "dog -> Accepted\n",
            "a -> Accepted\n",
            "zebra -> Accepted\n",
            "dog1 -> Not Accepted\n",
            "1dog -> Not Accepted\n",
            "DogHouse -> Not Accepted\n",
            "Dog_house -> Not Accepted\n",
            " cats -> Not Accepted\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Colab-ready cell: read brown_nouns.pdf (or brown_nouns.txt), produce morphological outputs\n",
        "# Save this cell and run it in one go in Colab.\n",
        "\n",
        "# Install pdf extraction lib (only if PDF is used)\n",
        "!pip install -q pdfplumber\n",
        "\n",
        "import os\n",
        "import re\n",
        "import pdfplumber\n",
        "\n",
        "PDF_PATH = \"/content/brown_nouns.pdf\"   # path where your uploaded PDF appears in Colab\n",
        "TXT_PATH = \"/mnt/data/brown_nouns.txt\"   # optional: if you have a plain text file\n",
        "OUT_PATH = \"/content/output.txt\"\n",
        "\n",
        "# Helper: extract tokens (words) from PDF (if present) or from TXT\n",
        "def extract_words_from_pdf(path):\n",
        "    words = []\n",
        "    if not os.path.exists(path):\n",
        "        return words\n",
        "    with pdfplumber.open(path) as pdf:\n",
        "        for page in pdf.pages:\n",
        "            text = page.extract_text()\n",
        "            if not text:\n",
        "                continue\n",
        "            # split on whitespace and punctuation, keep simple tokens\n",
        "            toks = re.findall(r\"[A-Za-z]+(?:'[A-Za-z]+)?\", text)\n",
        "            words.extend(toks)\n",
        "    return words\n",
        "\n",
        "def extract_words_from_txt(path):\n",
        "    words = []\n",
        "    if not os.path.exists(path):\n",
        "        return words\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        text = f.read()\n",
        "    toks = re.findall(r\"[A-Za-z]+(?:'[A-Za-z]+)?\", text)\n",
        "    words.extend(toks)\n",
        "    return words\n",
        "\n",
        "# Load nouns: prefer TXT if exists, else PDF\n",
        "tokens = []\n",
        "if os.path.exists(TXT_PATH):\n",
        "    print(\"Found text file, reading\", TXT_PATH)\n",
        "    tokens = extract_words_from_txt(TXT_PATH)\n",
        "elif os.path.exists(PDF_PATH):\n",
        "    print(\"Found PDF file, reading\", PDF_PATH)\n",
        "    tokens = extract_words_from_pdf(PDF_PATH)\n",
        "else:\n",
        "    raise FileNotFoundError(\"Neither brown_nouns.txt nor brown_nouns.pdf were found at expected paths. \"\n",
        "                            \"Upload the file to /mnt/data or change the paths in the script.\")\n",
        "\n",
        "# Normalize and unique-ify while preserving order\n",
        "seen = set()\n",
        "nouns = []\n",
        "for t in tokens:\n",
        "    w = t.strip().lower()\n",
        "    if not w:\n",
        "        continue\n",
        "    if w in seen:\n",
        "        continue\n",
        "    seen.add(w)\n",
        "    nouns.append(w)\n",
        "\n",
        "print(f\"Extracted {len(nouns):,} unique candidate tokens (lowercased).\")\n",
        "\n",
        "# FST-like rule engine for plural generation\n",
        "# Rules (order matters):\n",
        "# 1) E insertion: if word ends with s, z, x, ch, sh -> add \"es\"  (examples: fox -> foxes, watch -> watches)\n",
        "# 2) Y replacement: if word ends with consonant + y -> replace y with \"ies\" (try -> tries)\n",
        "# 3) S addition (fallback): add \"s\"\n",
        "#\n",
        "# Validation: If the token contains any non-alphabetic char (except apostrophe inside which we allow above),\n",
        "# we treat as Invalid Word. Also reject empty tokens.\n",
        "#\n",
        "# Output format:\n",
        "# Accepted forms:\n",
        "#   plural_form = root+N+PL\n",
        "#   root = root+N+SG\n",
        "# If invalid: plural_form_or_root -> Invalid Word    (we will print both singular and plural lines as Invalid Word to be explicit)\n",
        "\n",
        "def is_valid_root(word):\n",
        "    # valid only if all letters a-z (we already lowered) and length >= 1\n",
        "    return bool(re.fullmatch(r\"[a-z]+\", word))\n",
        "\n",
        "def generate_plural(word):\n",
        "    # Must assume input is lowercase and alphabetic.\n",
        "    # apply rules in order\n",
        "    # check endings: 'ch' and 'sh' are two-letter endings\n",
        "    if re.search(r\"(s|z|x)$\", word) or re.search(r\"(ch|sh)$\", word):\n",
        "        return word + \"es\"\n",
        "    # consonant + y -> replace y with ies\n",
        "    if re.search(r\"[^aeiou]y$\", word):\n",
        "        return word[:-1] + \"ies\"\n",
        "    # fallback: just add s\n",
        "    return word + \"s\"\n",
        "\n",
        "# Build outputs\n",
        "lines = []\n",
        "for root in nouns:\n",
        "    if not is_valid_root(root):\n",
        "        # generate explicit Invalid Word output for both forms\n",
        "        lines.append(f\"{root} = Invalid Word\")\n",
        "        lines.append(f\"(plural of {root}) = Invalid Word\")\n",
        "        continue\n",
        "\n",
        "    plural = generate_plural(root)\n",
        "\n",
        "    # Extra safety: avoid some pathological outputs (not required, but ensures no invalid strings)\n",
        "    if not is_valid_root(plural):\n",
        "        lines.append(f\"{plural} = Invalid Word\")\n",
        "    else:\n",
        "        # Write outputs as requested: \"foxes = fox+N+PL\" and also singular mapping \"fox = fox+N+SG\"\n",
        "        lines.append(f\"{plural} = {root}+N+PL\")\n",
        "        lines.append(f\"{root} = {root}+N+SG\")\n",
        "\n",
        "# Save to file\n",
        "with open(OUT_PATH, \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(\"\\n\".join(lines))\n",
        "\n",
        "print(f\"Done — morphological outputs written to: {OUT_PATH}\")\n",
        "print(\"\\nSample (first 40 output lines):\\n\")\n",
        "with open(OUT_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "    for i, ln in enumerate(f):\n",
        "        if i >= 40:\n",
        "            break\n",
        "        print(ln.rstrip())\n",
        "\n",
        "# If you want to download / inspect the file in Colab:\n",
        "print(\"\\nYou can download the file from Colab file browser or read it with:\")\n",
        "print(f\"!sed -n '1,200p' {OUT_PATH}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TAyOMcyOtwzt",
        "outputId": "e893b689-4577-4303-dc64-79df5c93115c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found PDF file, reading /content/brown_nouns.pdf\n",
            "Extracted 17,462 unique candidate tokens (lowercased).\n",
            "Done — morphological outputs written to: /content/output.txt\n",
            "\n",
            "Sample (first 40 output lines):\n",
            "\n",
            "investigations = investigation+N+PL\n",
            "investigation = investigation+N+SG\n",
            "primaries = primary+N+PL\n",
            "primary = primary+N+SG\n",
            "elections = election+N+PL\n",
            "election = election+N+SG\n",
            "evidences = evidence+N+PL\n",
            "evidence = evidence+N+SG\n",
            "irregularitieses = irregularities+N+PL\n",
            "irregularities = irregularities+N+SG\n",
            "places = place+N+PL\n",
            "place = place+N+SG\n",
            "juries = jury+N+PL\n",
            "jury = jury+N+SG\n",
            "presentmentses = presentments+N+PL\n",
            "presentments = presentments+N+SG\n",
            "charges = charge+N+PL\n",
            "charge = charge+N+SG\n",
            "praises = praise+N+PL\n",
            "praise = praise+N+SG\n",
            "thankses = thanks+N+PL\n",
            "thanks = thanks+N+SG\n",
            "manners = manner+N+PL\n",
            "manner = manner+N+SG\n",
            "terms = term+N+PL\n",
            "term = term+N+SG\n",
            "reportses = reports+N+PL\n",
            "reports = reports+N+SG\n",
            "handfuls = handful+N+PL\n",
            "handful = handful+N+SG\n",
            "interests = interest+N+PL\n",
            "interest = interest+N+SG\n",
            "numbers = number+N+PL\n",
            "number = number+N+SG\n",
            "voterses = voters+N+PL\n",
            "voters = voters+N+SG\n",
            "sizes = size+N+PL\n",
            "size = size+N+SG\n",
            "cities = city+N+PL\n",
            "city = city+N+SG\n",
            "\n",
            "You can download the file from Colab file browser or read it with:\n",
            "!sed -n '1,200p' /content/output.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Colab-ready script with accuracy metric\n",
        "\n",
        "!pip install -q pdfplumber\n",
        "\n",
        "import os\n",
        "import re\n",
        "import pdfplumber\n",
        "\n",
        "PDF_PATH = \"/content/brown_nouns.pdf\"\n",
        "TXT_PATH = \"/mnt/data/brown_nouns.txt\"\n",
        "OUT_PATH = \"/content/output3\"\n",
        "\n",
        "# Extract from PDF\n",
        "def extract_words_from_pdf(path):\n",
        "    words = []\n",
        "    if not os.path.exists(path):\n",
        "        return words\n",
        "    with pdfplumber.open(path) as pdf:\n",
        "        for page in pdf.pages:\n",
        "            text = page.extract_text()\n",
        "            if not text:\n",
        "                continue\n",
        "            toks = re.findall(r\"[A-Za-z]+(?:'[A-Za-z]+)?\", text)\n",
        "            words.extend(toks)\n",
        "    return words\n",
        "\n",
        "# Extract from TXT\n",
        "def extract_words_from_txt(path):\n",
        "    words = []\n",
        "    if not os.path.exists(path):\n",
        "        return words\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        text = f.read()\n",
        "    toks = re.findall(r\"[A-Za-z]+(?:'[A-Za-z]+)?\", text)\n",
        "    words.extend(toks)\n",
        "    return words\n",
        "\n",
        "# Load nouns\n",
        "tokens = []\n",
        "if os.path.exists(TXT_PATH):\n",
        "    tokens = extract_words_from_txt(TXT_PATH)\n",
        "elif os.path.exists(PDF_PATH):\n",
        "    tokens = extract_words_from_pdf(PDF_PATH)\n",
        "else:\n",
        "    raise FileNotFoundError(\"Upload brown_nouns.txt or brown_nouns.pdf to /mnt/data in Colab.\")\n",
        "\n",
        "# Normalize & unique\n",
        "seen = set()\n",
        "nouns = []\n",
        "for t in tokens:\n",
        "    w = t.strip().lower()\n",
        "    if not w:\n",
        "        continue\n",
        "    if w in seen:\n",
        "        continue\n",
        "    seen.add(w)\n",
        "    nouns.append(w)\n",
        "\n",
        "print(f\"Extracted {len(nouns):,} unique candidate tokens.\")\n",
        "\n",
        "# Validation\n",
        "def is_valid_root(word):\n",
        "    return bool(re.fullmatch(r\"[a-z]+\", word))\n",
        "\n",
        "# Rule-based plural generator with classification\n",
        "def generate_plural(word):\n",
        "    if re.search(r\"(s|z|x)$\", word) or re.search(r\"(ch|sh)$\", word):\n",
        "        return word + \"es\", \"E-insertion\"\n",
        "    if re.search(r\"[^aeiou]y$\", word):\n",
        "        return word[:-1] + \"ies\", \"Y-replacement\"\n",
        "    return word + \"s\", \"S-addition\"\n",
        "\n",
        "# Counters for accuracy\n",
        "invalid_count = 0\n",
        "rule_counts = {\"E-insertion\": 0, \"Y-replacement\": 0, \"S-addition\": 0}\n",
        "\n",
        "lines = []\n",
        "for root in nouns:\n",
        "    if not is_valid_root(root):\n",
        "        invalid_count += 1\n",
        "        lines.append(f\"{root} = Invalid Word\")\n",
        "        lines.append(f\"(plural of {root}) = Invalid Word\")\n",
        "        continue\n",
        "\n",
        "    plural, rule_type = generate_plural(root)\n",
        "    rule_counts[rule_type] += 1\n",
        "\n",
        "    lines.append(f\"{plural} = {root}+N+PL\")\n",
        "    lines.append(f\"{root} = {root}+N+SG\")\n",
        "\n",
        "# Save output file\n",
        "with open(OUT_PATH, \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(\"\\n\".join(lines))\n",
        "\n",
        "# Accuracy summary\n",
        "total = len(nouns)\n",
        "valid_count = total - invalid_count\n",
        "valid_pct = (valid_count / total) * 100\n",
        "invalid_pct = (invalid_count / total) * 100\n",
        "\n",
        "print(\"\\n--- Accuracy Summary ---\")\n",
        "print(f\"Total words processed: {total}\")\n",
        "print(f\"Valid words: {valid_count} ({valid_pct:.2f}%)\")\n",
        "print(f\"Invalid words: {invalid_count} ({invalid_pct:.2f}%)\")\n",
        "\n",
        "print(\"\\n--- Valid word distribution by rule ---\")\n",
        "for rule, count in rule_counts.items():\n",
        "    pct = (count / valid_count * 100) if valid_count else 0\n",
        "    print(f\"{rule}: {count} words ({pct:.2f}%)\")\n",
        "\n",
        "print(f\"\\nDetailed output saved to: {OUT_PATH}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OEzFg-6bwBh_",
        "outputId": "05483389-97ea-439e-eb83-a5273c1a9d88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted 17,462 unique candidate tokens.\n",
            "\n",
            "--- Accuracy Summary ---\n",
            "Total words processed: 17462\n",
            "Valid words: 17429 (99.81%)\n",
            "Invalid words: 33 (0.19%)\n",
            "\n",
            "--- Valid word distribution by rule ---\n",
            "E-insertion: 6622 words (37.99%)\n",
            "Y-replacement: 1008 words (5.78%)\n",
            "S-addition: 9799 words (56.22%)\n",
            "\n",
            "Detailed output saved to: /content/output3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hgSsh3gBzk36"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}