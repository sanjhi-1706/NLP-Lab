{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CQoAcTDjm7jB",
        "outputId": "2de599f2-76b7-453e-ab80-0f001d9d91d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 144154 sentences.\n",
            "Splits written: train.pkl (142154), val.pkl (1000), test.pkl (1000)\n",
            "Done.\n"
          ]
        }
      ],
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Task 1: Create data splits (random sampling):\n",
        " - Validation Set: 1000 sentences\n",
        " - Test Set: 1000 sentences\n",
        " - Training Set: remaining sentences\n",
        "\n",
        "Reads 'sentence_tokens' (supports .json, .pkl, .txt).\n",
        "Outputs: train.pkl, val.pkl, test.pkl (pickle lists of token lists) and small summary.\n",
        "\"\"\"\n",
        "\n",
        "import os, json, pickle, random\n",
        "from pathlib import Path\n",
        "\n",
        "INPUT = \"sentence_tokens\"   # will try .json .pkl .txt or bare file\n",
        "VAL_SIZE = 1000\n",
        "TEST_SIZE = 1000\n",
        "SEED = 42\n",
        "\n",
        "def load_sentences(base):\n",
        "    candidates = [base, base + \".json\", base + \".pkl\", base + \".txt\"]\n",
        "    for f in candidates:\n",
        "        if not os.path.exists(f):\n",
        "            continue\n",
        "        if f.endswith(\".pkl\"):\n",
        "            with open(f, \"rb\") as fh:\n",
        "                data = pickle.load(fh)\n",
        "                if isinstance(data, list):\n",
        "                    return data\n",
        "        elif f.endswith(\".txt\"):\n",
        "            with open(f, \"r\", encoding=\"utf8\") as fh:\n",
        "                lines = [line.strip().split() for line in fh if line.strip()]\n",
        "                return lines\n",
        "        else:\n",
        "            # try JSON or bare\n",
        "            try:\n",
        "                with open(f, \"r\", encoding=\"utf8\") as fh:\n",
        "                    data = json.load(fh)\n",
        "                    if isinstance(data, list):\n",
        "                        return data\n",
        "            except Exception:\n",
        "                # treat as plain text fallback\n",
        "                with open(f, \"r\", encoding=\"utf8\") as fh:\n",
        "                    lines = [line.strip().split() for line in fh if line.strip()]\n",
        "                    return lines\n",
        "    raise FileNotFoundError(\"Couldn't find sentence_tokens(.json/.pkl/.txt)\")\n",
        "\n",
        "def save_pickle(obj, fn):\n",
        "    with open(fn, \"wb\") as fh:\n",
        "        pickle.dump(obj, fh)\n",
        "\n",
        "def main():\n",
        "    sentences = load_sentences(INPUT)\n",
        "    print(f\"Loaded {len(sentences)} sentences.\")\n",
        "    random.seed(SEED)\n",
        "    random.shuffle(sentences)\n",
        "    if len(sentences) < VAL_SIZE + TEST_SIZE:\n",
        "        raise ValueError(\"Not enough sentences to create the requested splits.\")\n",
        "    val = sentences[:VAL_SIZE]\n",
        "    test = sentences[VAL_SIZE:VAL_SIZE+TEST_SIZE]\n",
        "    train = sentences[VAL_SIZE+TEST_SIZE:]\n",
        "    save_pickle(train, \"train.pkl\")\n",
        "    save_pickle(val, \"val.pkl\")\n",
        "    save_pickle(test, \"test.pkl\")\n",
        "    print(\"Splits written: train.pkl ({}), val.pkl ({}), test.pkl ({})\".format(len(train), len(val), len(test)))\n",
        "    # also write simple summaries\n",
        "    with open(\"split_summary.txt\", \"w\", encoding=\"utf8\") as fh:\n",
        "        fh.write(f\"Total sentences: {len(sentences)}\\n\")\n",
        "        fh.write(f\"Train: {len(train)}\\nVal: {len(val)}\\nTest: {len(test)}\\n\")\n",
        "    print(\"Done.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Task 2: Build unigram/bigram/trigram/quadrigram counts on TRAIN and compute Good-Turing\n",
        "adjusted joint probabilities for each n (1..4).\n",
        "\n",
        "Input: train.pkl (produced by task1)\n",
        "Outputs:\n",
        " - counts_n{1..4}.pkl  (Counter of n-grams)\n",
        " - gt_joint_n{1..4}.pkl (dict: seen ngram -> probability)\n",
        " - diagnostics_n{1..4}.json (Nc, N1, N_distinct, total_tokens_observed)\n",
        "\"\"\"\n",
        "\n",
        "import pickle, math, json\n",
        "from collections import Counter, defaultdict\n",
        "from pathlib import Path\n",
        "\n",
        "TRAIN_PICKLE = \"train.pkl\"\n",
        "MAX_N = 4\n",
        "SPECIAL_START = \"<s>\"\n",
        "SPECIAL_END = \"</s>\"\n",
        "\n",
        "def load_pickle(fn):\n",
        "    with open(fn, \"rb\") as fh:\n",
        "        return pickle.load(fh)\n",
        "\n",
        "def ngrams_from_sentence(tokens, n):\n",
        "    padded = [SPECIAL_START] * (n-1) + tokens + [SPECIAL_END]\n",
        "    return [tuple(padded[i:i+n]) for i in range(len(padded)-n+1)]\n",
        "\n",
        "def build_counts(sentences, n):\n",
        "    c = Counter()\n",
        "    total = 0\n",
        "    for s in sentences:\n",
        "        ngs = ngrams_from_sentence(s, n)\n",
        "        c.update(ngs)\n",
        "        total += len(ngs)\n",
        "    return c, total\n",
        "\n",
        "def compute_Nc(counter):\n",
        "    freq_of_freq = Counter(counter.values())\n",
        "    return dict(freq_of_freq)\n",
        "\n",
        "def good_turing_counts(counter, V_power_n=None):\n",
        "    \"\"\"\n",
        "    Compute C* per seen n-gram using c* = (c+1) * N_{c+1} / N_c when possible.\n",
        "    Also compute Punseen per unseen n-gram using formula from prompt if V_power_n provided.\n",
        "    Returns: C_star_dict (ngram->c_star), Punseen_per_unseen (float or None), diagnostics\n",
        "    \"\"\"\n",
        "    Nc = compute_Nc(counter)\n",
        "    N_distinct = len(counter)\n",
        "    N1 = Nc.get(1, 0)\n",
        "    max_c = max(counter.values()) if counter else 0\n",
        "    C_star = {}\n",
        "    for ng, c in counter.items():\n",
        "        Nc_c = Nc.get(c, 0)\n",
        "        Nc_cp1 = Nc.get(c+1, 0)\n",
        "        if Nc_c > 0 and Nc_cp1 > 0:\n",
        "            c_star = (c+1) * (Nc_cp1 / Nc_c)\n",
        "        else:\n",
        "            c_star = float(c)\n",
        "        C_star[ng] = c_star\n",
        "    Punseen = None\n",
        "    if V_power_n is not None:\n",
        "        unseen_count = max(V_power_n - N_distinct, 0)\n",
        "        if unseen_count > 0 and N_distinct > 0:\n",
        "            Punseen = (N1 / N_distinct) / unseen_count\n",
        "        else:\n",
        "            Punseen = 0.0\n",
        "    diagnostics = {\n",
        "        \"Nc\": Nc,\n",
        "        \"N_distinct\": N_distinct,\n",
        "        \"N1\": N1,\n",
        "        \"max_c\": max_c,\n",
        "        \"total_seen_tokens\": sum(counter.values())\n",
        "    }\n",
        "    return C_star, Punseen, diagnostics\n",
        "\n",
        "def normalize_joint_from_Cstar(C_star, punseen, V_power_n):\n",
        "    \"\"\"\n",
        "    Convert C* to probabilities that sum to 1 by scaling seen mass and including unseen mass = punseen * unseen_count.\n",
        "    Returns: P_seen (dict ngram->prob), p_unseen_each\n",
        "    \"\"\"\n",
        "    sum_cstar = sum(C_star.values())\n",
        "    unseen_count = max(V_power_n - len(C_star), 0)\n",
        "    unseen_mass = (punseen * unseen_count) if punseen is not None else 0.0\n",
        "    if sum_cstar == 0:\n",
        "        scale = 0.0\n",
        "    else:\n",
        "        scale = max(0.0, (1.0 - unseen_mass) / sum_cstar)\n",
        "    P_seen = {ng: cstar * scale for ng, cstar in C_star.items()}\n",
        "    # final numeric adjust (small)\n",
        "    total = sum(P_seen.values()) + (punseen or 0.0) * unseen_count\n",
        "    if total > 0:\n",
        "        # renormalize to exactly 1\n",
        "        factor = 1.0 / total\n",
        "        for ng in list(P_seen.keys()):\n",
        "            P_seen[ng] *= factor\n",
        "        if punseen is not None:\n",
        "            punseen *= factor\n",
        "    return P_seen, punseen\n",
        "\n",
        "def save_pickle(obj, fn):\n",
        "    with open(fn, \"wb\") as fh:\n",
        "        pickle.dump(obj, fh)\n",
        "\n",
        "def main():\n",
        "    train = load_pickle(TRAIN_PICKLE)\n",
        "    print(f\"Loaded train ({len(train)} sentences). Building counts and Good-Turing models...\")\n",
        "    # compute vocab size from train tokens\n",
        "    vocab = set()\n",
        "    for s in train:\n",
        "        vocab.update(s)\n",
        "    vocab.update([SPECIAL_START, SPECIAL_END])\n",
        "    V = len(vocab)\n",
        "    for n in range(1, MAX_N+1):\n",
        "        counts, total = build_counts(train, n)\n",
        "        save_pickle(counts, f\"counts_n{n}.pkl\")\n",
        "        Vpow = V ** n\n",
        "        C_star, punseen, diag = good_turing_counts(counts, V_power_n=Vpow)\n",
        "        P_seen, punseen = normalize_joint_from_Cstar(C_star, punseen, Vpow)\n",
        "        save_pickle(P_seen, f\"gt_joint_n{n}.pkl\")\n",
        "        with open(f\"diagnostics_n{n}.json\", \"w\", encoding=\"utf8\") as fh:\n",
        "            # JSON-friendly: reduce large Nc dict by converting keys to str\n",
        "            dd = dict(diag)\n",
        "            dd[\"Nc\"] = {str(k): v for k, v in diag[\"Nc\"].items()}\n",
        "            dd[\"vocab_size\"] = V\n",
        "            fh.write(json.dumps(dd, indent=2))\n",
        "        print(f\"n={n}: distinct_seen={diag['N_distinct']} N1={diag['N1']} total_tokens={diag['total_seen_tokens']}\")\n",
        "    print(\"Done. Files produced: counts_n*.pkl, gt_joint_n*.pkl, diagnostics_n*.json\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zy_qF6lTrWOO",
        "outputId": "f57ca364-c09f-4884-81b5-6c02457d52d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded train (142154 sentences). Building counts and Good-Turing models...\n",
            "n=1: distinct_seen=90639 N1=45265 total_tokens=3151811\n",
            "n=2: distinct_seen=893373 N1=635954 total_tokens=3151811\n",
            "n=3: distinct_seen=1998504 N1=1725751 total_tokens=3151811\n",
            "n=4: distinct_seen=2581780 N1=2415729 total_tokens=3151811\n",
            "Done. Files produced: counts_n*.pkl, gt_joint_n*.pkl, diagnostics_n*.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Task 3: Compute sentence log-probabilities for validation and test sets using\n",
        "the Good-Turing-smoothed joint models produced in task2.\n",
        "\n",
        "Inputs:\n",
        " - val.pkl, test.pkl (from task1)\n",
        " - gt_joint_n{1..4}.pkl (from task2)\n",
        "Outputs:\n",
        " - logprobs_n{1..4}_val.txt\n",
        " - logprobs_n{1..4}_test.txt\n",
        "\"\"\"\n",
        "\n",
        "import pickle, math\n",
        "from collections import Counter\n",
        "SPECIAL_START = \"<s>\"\n",
        "SPECIAL_END = \"</s>\"\n",
        "MAX_N = 4\n",
        "\n",
        "def load_pickle(fn):\n",
        "    with open(fn, \"rb\") as fh:\n",
        "        return pickle.load(fh)\n",
        "\n",
        "def sentence_ngrams(tokens, n):\n",
        "    padded = [SPECIAL_START] * (n-1) + tokens + [SPECIAL_END]\n",
        "    ngrams = [tuple(padded[i:i+n]) for i in range(len(padded)-n+1)]\n",
        "    return ngrams\n",
        "\n",
        "def conditional_prob(ngram, joint_n, joint_n_minus1, punseen_n, punseen_n_minus1):\n",
        "    # joint_n: dict of joint probs for n-grams\n",
        "    # joint_n_minus1: dict for (n-1)-grams\n",
        "    if len(ngram) == 1:\n",
        "        return joint_n.get(ngram, punseen_n if punseen_n is not None else 1e-16)\n",
        "    joint = joint_n.get(ngram, punseen_n if punseen_n is not None else 1e-16)\n",
        "    history = ngram[:-1]\n",
        "    denom = joint_n_minus1.get(history, punseen_n_minus1 if punseen_n_minus1 is not None else 1e-12)\n",
        "    if denom <= 0:\n",
        "        denom = 1e-12\n",
        "    p = joint / denom\n",
        "    return max(p, 1e-16)\n",
        "\n",
        "def sentence_logprob(tokens, order, joint_models):\n",
        "    \"\"\"\n",
        "    joint_models: dict n -> (joint_probs_dict, punseen_each)\n",
        "    \"\"\"\n",
        "    logp = 0.0\n",
        "    n = order\n",
        "    padded = [SPECIAL_START]*(n-1) + tokens + [SPECIAL_END]\n",
        "    for i in range(n-1, len(padded)):\n",
        "        ngram = tuple(padded[i-(n-1):i+1])\n",
        "        if n == 1:\n",
        "            joint_n, punseen_n = joint_models[1]\n",
        "            p = joint_n.get(ngram, punseen_n if punseen_n is not None else 1e-16)\n",
        "        else:\n",
        "            joint_n, punseen_n = joint_models[n]\n",
        "            joint_nm1, punseen_nm1 = joint_models[n-1]\n",
        "            p = conditional_prob(ngram, joint_n, joint_nm1, punseen_n, punseen_nm1)\n",
        "        logp += math.log(p)\n",
        "    return logp\n",
        "\n",
        "def main():\n",
        "    val = load_pickle(\"val.pkl\")\n",
        "    test = load_pickle(\"test.pkl\")\n",
        "    joint_models = {}\n",
        "    for n in range(1, MAX_N+1):\n",
        "        jp = load_pickle(f\"gt_joint_n{n}.pkl\")  # dict ngram->prob\n",
        "        # We didn't explicitly save punseen in task2's pickle. We'll approximate punseen by tiny value for safety.\n",
        "        # If you saved punseen, load it; otherwise set None so code uses fallback.\n",
        "        # Here: punseen unknown -> None\n",
        "        joint_models[n] = (jp, None)\n",
        "\n",
        "    for n in range(1, MAX_N+1):\n",
        "        out_val = f\"logprobs_n{n}_val.txt\"\n",
        "        out_test = f\"logprobs_n{n}_test.txt\"\n",
        "        with open(out_val, \"w\", encoding=\"utf8\") as fv:\n",
        "            for sent in val:\n",
        "                lp = sentence_logprob(sent, n, joint_models)\n",
        "                fv.write(f\"{lp}\\n\")\n",
        "        with open(out_test, \"w\", encoding=\"utf8\") as ft:\n",
        "            for sent in test:\n",
        "                lp = sentence_logprob(sent, n, joint_models)\n",
        "                ft.write(f\"{lp}\\n\")\n",
        "        print(f\"Wrote {out_val}, {out_test}\")\n",
        "    print(\"Done.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EGSMa5nDrbuy",
        "outputId": "08c659fd-fcce-4d7f-dbe8-bddfc8bc7e30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote logprobs_n1_val.txt, logprobs_n1_test.txt\n",
            "Wrote logprobs_n2_val.txt, logprobs_n2_test.txt\n",
            "Wrote logprobs_n3_val.txt, logprobs_n3_test.txt\n",
            "Wrote logprobs_n4_val.txt, logprobs_n4_test.txt\n",
            "Done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Task 4: Deleted-interpolated smoothing for quadrigram model.\n",
        "We use the validation set to find interpolation weights λ1..λ4 (sum to 1)\n",
        "to maximize log-likelihood of val data:\n",
        " P_interp(w | history) = sum_{k=1..4} λ_k * P_k(w | history_k)\n",
        "\n",
        "Approach: coarse grid search on the 4-simplex (lambda1,lambda2,lambda3,lambda4).\n",
        "Inputs required:\n",
        " - gt_joint_n1..n4 pickles (produced in task2)\n",
        " - val.pkl (produced in task1)\n",
        "Outputs:\n",
        " - best_lambdas.txt\n",
        "\"\"\"\n",
        "\n",
        "import pickle, math\n",
        "from itertools import product\n",
        "SPECIAL_START = \"<s>\"\n",
        "SPECIAL_END = \"</s>\"\n",
        "MAX_N = 4\n",
        "\n",
        "GRID_STEP = 0.1   # default coarse; lower -> finer but slower\n",
        "\n",
        "def load_pickle(fn):\n",
        "    with open(fn, \"rb\") as fh:\n",
        "        return pickle.load(fh)\n",
        "\n",
        "def conditional_from_joint(ngram, joint_n, joint_nm1, punseen_n=None, punseen_nm1=None):\n",
        "    if len(ngram) == 1:\n",
        "        return joint_n.get(ngram, punseen_n if punseen_n is not None else 1e-16)\n",
        "    joint = joint_n.get(ngram, punseen_n if punseen_n is not None else 1e-16)\n",
        "    history = ngram[:-1]\n",
        "    denom = joint_nm1.get(history, punseen_nm1 if punseen_nm1 is not None else 1e-12)\n",
        "    if denom <= 0:\n",
        "        denom = 1e-12\n",
        "    return max(joint / denom, 1e-16)\n",
        "\n",
        "def sentence_ngrams(tokens, n):\n",
        "    padded = [SPECIAL_START]*(n-1) + tokens + [SPECIAL_END]\n",
        "    for i in range(n-1, len(padded)):\n",
        "        yield tuple(padded[i-(n-1):i+1])\n",
        "\n",
        "def load_joint_models():\n",
        "    jm = {}\n",
        "    for n in range(1, MAX_N+1):\n",
        "        jp = load_pickle(f\"gt_joint_n{n}.pkl\")\n",
        "        jm[n] = (jp, None)   # None punseen; if you have it, include\n",
        "    return jm\n",
        "\n",
        "def compute_val_loglik(lambdas, val_sents, joint_models):\n",
        "    total_loglik = 0.0\n",
        "    for s in val_sents:\n",
        "        padded = [SPECIAL_START]*(MAX_N-1) + s + [SPECIAL_END]\n",
        "        for i in range(MAX_N-1, len(padded)):\n",
        "            w = padded[i]\n",
        "            history = padded[i-(MAX_N-1):i]\n",
        "            # gather P_k for k=1..4\n",
        "            probs = []\n",
        "            for k in range(1, MAX_N+1):\n",
        "                if k == 1:\n",
        "                    ngram = (w,)\n",
        "                else:\n",
        "                    hist_k = history[-(k-1):] if (k-1) > 0 else []\n",
        "                    ngram = tuple(hist_k + [w])\n",
        "                joint_k, punseen_k = joint_models[k]\n",
        "                if k == 1:\n",
        "                    p_k = joint_k.get(ngram, punseen_k if punseen_k is not None else 1e-16)\n",
        "                else:\n",
        "                    joint_km1, punseen_km1 = joint_models[k-1]\n",
        "                    p_k = conditional_from_joint(ngram, joint_k, joint_km1, punseen_k, punseen_km1)\n",
        "                probs.append(p_k)\n",
        "            p_interp = sum(l * p for l, p in zip(lambdas, probs))\n",
        "            if p_interp <= 0:\n",
        "                p_interp = 1e-16\n",
        "            total_loglik += math.log(p_interp)\n",
        "    return total_loglik\n",
        "\n",
        "def grid_simplex(step=GRID_STEP):\n",
        "    steps = int(1.0/step) + 1\n",
        "    lambdas = []\n",
        "    for i1 in range(steps):\n",
        "        l1 = i1*step\n",
        "        for i2 in range(steps):\n",
        "            l2 = i2*step\n",
        "            for i3 in range(steps):\n",
        "                l3 = i3*step\n",
        "                s = l1 + l2 + l3\n",
        "                if s > 1.0 + 1e-12:\n",
        "                    continue\n",
        "                l4 = 1.0 - s\n",
        "                if l4 < -1e-12:\n",
        "                    continue\n",
        "                lambdas.append((l1, l2, l3, l4))\n",
        "    return lambdas\n",
        "\n",
        "def main():\n",
        "    val = load_pickle(\"val.pkl\")\n",
        "    joint_models = load_joint_models()\n",
        "    best = (None, -1e300)\n",
        "    grid = grid_simplex(GRID_STEP)\n",
        "    print(f\"Evaluating {len(grid)} lambda candidates (step={GRID_STEP}) on {len(val)} validation sentences...\")\n",
        "    for lamb in grid:\n",
        "        ll = compute_val_loglik(lamb, val, joint_models)\n",
        "        if ll > best[1]:\n",
        "            best = (lamb, ll)\n",
        "    print(\"Best lambdas:\", best[0], \"loglik:\", best[1])\n",
        "    with open(\"best_quadrigram_lambdas.txt\", \"w\", encoding=\"utf8\") as fh:\n",
        "        fh.write(f\"{best[0]}\\nloglik={best[1]}\\n\")\n",
        "    print(\"Saved best_quadrigram_lambdas.txt\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "snpF0-D7rotE",
        "outputId": "a89d26bb-8a21-4713-f5f8-34d501babcf8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating 286 lambda candidates (step=0.1) on 1000 validation sentences...\n",
            "Best lambdas: (0.2, 0.5, 0.2, 0.10000000000000009) loglik: -113867.8890612377\n",
            "Saved best_quadrigram_lambdas.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "o813TBmgrv6D"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}